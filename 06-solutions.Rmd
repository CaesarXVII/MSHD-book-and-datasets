---
output:
  html_document: default
  pdf_document: default
---
# Solutions

## Chapter 1

### Data on Malnutrition in Zambia{#zam}

>- Load the dataset and build the variables so that they can be used for a regression analysis.


```{r, results='hide', warning=FALSE}

require(foreign)  # install foreign package if you do not have it yet

# See section 1.6.2 e-book for information on the dataset.

# dat = read.spss("Zambia.SAV", add.undeclared.levels = "no")

dat = read.spss("Zambia.SAV")

# Construct system matrix

# The idea behind this exercise is to be aware that data cleaning is most of the times the real issue 
# with a real problem. It is sensitive to say that 80% of the work is cleaning and only 20% is modeling.

# Extract response variable i.e. HW70 Height for age standard deviation (according to WHO)
y = dat$HW70
y[y == 9996] = NA
y[y == 9997] = NA
y[y == 9998] = NA
y[y == 9999] = NA

# Revert tranformation (i.e. z-score)
y = y/100

# Variable 1: The calculated months of breastfeeding gives the duration of breastfeeding
x1 = dat$M5
x1[x1 == 94] = 0
x1[x1 == 97] = NA
x1[x1 == 98] = NA
x1[x1 == 99] = NA
x1[x1 > 40] = NA

# Variable 2: Age in months of the child
x2 = dat$HW1

# Variable 3: Age of the mother at birth
x3 = dat$V012 - dat$B8
x3[x3>45] = NA

# Variable 4: Body mass index (BMI) of the mother
x4 = dat$V445

x4 = x4/100  # no sense without this division

# Variable 5: Height of the mother in meters
x5 = dat$V438
x5[x5 == 9998] = NA
x5[x5 == 9999] = NA
x5[x5 < 1300] = NA
x5[x5 > 1900] = NA

x5 = x5/1000  # it was in mm, we need to transform from original

# Variable 6: Weight of the mother in kilograms
x6 = dat$V437

x6=x6/10 # we need to go back to Kg

# Variable 7: De facto region of residence

# Creating dummies (i.e. indicator functions) for each level of an existing factor enables
# to check the coefficients of each level in a possible future model estimation

x7 = as.factor(dat$V101)


x7 = model.matrix(~x7-1)

dim(x7)

# Variable 8: Mother highest education level attended
x8 = as.factor(dat$V106)
x8 = model.matrix(~x8-1)

dim(x8)

# Variable 9: Wealth index factor score
x9 = dat$V191

# Variable 10: Weight of child at birth given in kilograms with three implied decimal places
x10 = dat$M19
x10[x10 == 9996] = NA
x10[x10 == 9997] = NA
x10[x10 == 9998] = NA
x10[x10 == 9999] = NA
x10 = x10/1000

# Variable 11: Child Sex
x11 = dat$B4

# Variable 12: Preceding birth interval is calculated as the difference in months between the current birth and the previous birth
x12 = dat$B11
x12[x12 > 125] = NA

# Variable 13: Drinking Water
x13 = dat$V113
x13 = model.matrix(~x13-1)
x13 = x13[,c(2,3,4,8,9,13,17,18)]

dim(x13)

levels(x13)

mat.sys = na.omit(cbind(y,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13))
dim(mat.sys)[2]


# Number of regressor
p = dim(mat.sys)[2]

# Construct X and Y
y = mat.sys[,1]
X = mat.sys[,2:p]

# Create a dataframe

data_zambia = cbind(y,X)

data_zambia = data.frame(data_zambia)

```


>- Associate proper names to each variable (hint: look at the previous comments in the r chunk).


```{r, results='hide'}

colnames(data_zambia) = c("Height for age sd", "Breastfeeding duration (months)","Age of the child (months)", "Age of the mother (years)", "BMI mother", "Heigth mother (meter)", "Weight mother (kg)", "Region:Central", "Region:Copperbelt", "Region:Eastern", "Region:Luapula", "Region:Lusaka", "Region:Northern", "Region:Northwestern", "Region:Southern", "Region:Western", "Ed:No education", "Ed:Primary", "Ed:Secondary", "Ed:Higher", "Wealth index factor score", "Child weight at birth (kg)", "Child sex", "Interval between births","Water:Piped into dwelling", "Water:Piped to yard/plot", "Water:Public tap/standpipe", "Water:Protected well", "Water:Unprotected well", "Water:River/dam/lake/ponds/stream/canal/irrigation channel", "Water:Bottled water", "Water:Other")
                    

```


>- Perform a linear regression on all the available variables.


```{r, eval=FALSE}

attach(data_zambia)

lm_zambia = lm(`Height for age sd` ~ . -`Region:Central`- `Ed:No education`, data = data_zambia)

# We take off two levels to avoid multicollinearity. This should always be done when you create dummies.
summary(lm_zambia) # read the output understand the benchmark of the factor

lm_zambia_full = lm(`Height for age sd` ~ . , data = data_zambia)

summary(lm_zambia_full) #here it is R who choses the benchmark for the factors (i.e. NA variables)

detach(data_zambia)

```


>- Reduce the number of covariates (e.g. using the *t*-test) and add some interactions. Perform a linear regression on the new dataset.


```{r, results='hide'}

attach(data_zambia)

# Eliminate variables with t-test in a stepwise manner (fixed alfa = 0.05 in this case)

model_zambia_reduced = lm(`Height for age sd` ~ ., data = data_zambia[,c(1:2,4,9:16,21:23)])


summary(model_zambia_reduced) # notice what is happening to the age of the mother variable 


# Introduce one interaction in the reduced model. We start with the childsex factor.

model_zambia_int = lm(`Height for age sd` ~ . + `Breastfeeding duration (months)`*`Child sex`, data = data_zambia[,c(1:2,4,9:16,21:23)])


summary(model_zambia_int) #We take out the interaction from the model as it is not significant

#### Remember: the hierarchical effect states that anytime you add an interaction also the marginal effects

#### should be part of your model

detach(data_zambia)

```


>- Other available procedures for a first model selection in this specific case:


```{r, eval=FALSE}

# (1) VIF (variance inflation factor) for avoiding multicollinearity, 

# (2) Automatic Stepwise procedures (e.g. forward and backward) 

# (3) Exhaustive search (See practical 3 exercises)

# Example with an automatic stepwise procedure

help("step")

stepwise_procedue = step(lm_zambia_full,direction = "backward") #or forward

# This procedure evaluates, given a criterion, a sequence of variables stopping when
# the criterion is increasing

```


>- Analyse your chosen estimated model with a residual analysis (e.g. residuals vs fitted plot, normal QQ plot etc.). 

```{r}

# Validate your model looking at residuals vs fitted plot and normal QQ plot

plot(model_zambia_reduced, which = 1)  # Residuals vs fitted: no particular structure

plot(model_zambia_reduced, which = 2) 

# Normal QQ plot: We observe right tail which is not compatible with a normal assumption

```


### Prognostic Factors in Childhood Leukemia{#leuk}

> Exercises 

```{r, message=F, echo=F}
show_solution <- T # set to T to show the solutions on these exercises.
solution_header <- ifelse(show_solution, "<div>", "<div style='display:none'>")
```


- Load the data from the URL <http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv>

```{r results="asis", echo=F}
cat(solution_header)
```
```{r, eval=FALSE}

leukemia_big <- read.csv("http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv")

```
</div>


- Create the response variable y according to the number of ALL and AML patients. In the same fashion create the matrix X of independent variables. 

See  <https://web.stanford.edu/~hastie/CASI_files/DATA/leukemia.html> for further details.


```{r results="asis", echo=F}
cat(solution_header)
```
```{r, eval=FALSE}

leukemia_mat = as.matrix(leukemia_big)

dim(leukemia_mat)

leukemia_mat = t(leukemia_mat) #this is the design matrix for the analysis

# Generate the 0 and 1 values for the two different categories: there are 20 ALL, 14 AML, 27 ALL and

# 11 AML for a total of 47 ALL and 25 AML.

# Given the above excerpt from the cancer society, I have decided to code ALL as 1 and AML as 0 since

# doctors are interested in knowing the characteristics which differentiate ALL from AML in order to

# understand if we can use standard treatment or a more aggressive one.

y = c(rep(1,20),rep(0,14), rep(1,27), rep(0,11))  #the response vector

length(y)

X = leukemia_mat

dim(X)

```
</div>


- Choose the correct exponential family for this situation and perform a GLM on the data. Comment on the results that you obtain. 

<!-- Since p>>n there are problems -->

```{r results="asis", echo=F}
cat(solution_header)
```
```{r, eval=FALSE}

model_glm = glm(formula = y ~ X,family = "binomial")


summary(model_glm)  #singularity issues in the IWLS algorithm of GLM. It is impossible to invert the matrix.

# The binary Lasso is a possible way to solve the issue and have an actual estimate. See glmnet package.

```
</div>

## Chapter 2 

### Cross-validation{#cv}

(solutions provided by **Alexander Maslev**, **Hanxiong Wang** and **Minyoung Lee**).\ 

Program k-fold Cross-Validation (with k=2) and do model selection in a specific simulation setting with an exhaustive search. Follow these steps:

> (a) Generate from a MVN (multivariate normal) a matrix $\mathbf{X_{n*p}}$ with $n = 1000$ and $p = 5$. You can choose the location vector as you wish but set the scale matrix as the identity.\

```{r include=FALSE,echo=FALSE,message = FALSE}
library(MASS)
require(mvtnorm)
```

We have chosen the location vector [2,4,6,8,10] and the scale matrix as the identity.

```{r}
n<-1000
p<-5
Mu<-c(2,4,6,8,10) # location vector
sigma<-diag(5) # scale matrix as the identity
X<-mvrnorm(n , Mu, sigma)
```


> (b) Choose the generating vector $\boldsymbol{\beta }= [3 \; 1.5 \; 0 \; 2 \; 0]$ and retrieve the signal to noise ratio of this setting.\

We found in Wikipedia that the statistical definition of SNR is the reciprocal of the coefficient of variation (i.e. the ratio of mean to standard deviation of a signal or measurement) :
$$SNR = \frac{\mu}{\sigma}$$
where $\mu$  is the signal mean or expected value and $\sigma$ is the standard deviation of the noise.

```{r}
beta<-c(3,1.5,0,2,0)
e<-rnorm(n, mean = 0, sd = 1)

SNR<-mean(X%*%beta)/sqrt(var(e))

SNR

```

In the engineering literature, there is an alternative definition: 

$$SNR_{eng} = \frac{Var(f(x))}{Var(\epsilon)}$$
where $f(x)$ is the chosen prediction rule (e.g. linear function in the OLS case) and $\epsilon$ is the noise. You can fix in advance your SNR with the following code and generate the data accordingly:

```{r, eval=FALSE}

signal_to_noise_ratio = #number to fix as you wish#

data = X%*%beta

noise = rnorm(n,0,1)

k = sqrt(var(data)/(signal_to_noise_ratio*var(noise)))


y_hat_eng = data + k*noise # how you generate data to retrieve your fixed signal to noise ratio

```

> (c) Generate $\hat{\mathbf{y}}$ thanks to the relation $\mathbf{y} =  \mathbf{X_{n*p}} \; \boldsymbol{\beta} + \boldsymbol{\epsilon}$ where $\epsilon_{i}$ is a standard normal, $n = 1000$ and $p = 5$. Suppose for simplicity that the errors are uncorrelated. \


```{r}

Y_hat<-X%*%beta+e

```


> (d) Split the data randomly in two halves (k=2) and use the training set to determine $\hat{\boldsymbol{\beta}}$. Then, compute the squared loss function as prediction error measure for each possible model. Observe which model is the best model.\


```{r}

index <- sample(1:n, size=0.5*n)

# Split data
y_train<- Y_hat[-index,]
x_train<-X[-index,]
y_test<- Y_hat[index,]
x_test<-X[index,]

index_sub_choose<-c(1:p)
sub_matrix <- matrix(data = NA,ncol = p,nrow = 2^p-1)
t=0
for(i in 1:5)
{
  index_matrix <- combn(index_sub_choose,i)
  for(j in 1:ncol(index_matrix))
  {
    t <- t+1
    index_sub <- index_matrix[,j]
    sub_matrix[t,c(index_sub)]  <-  1
  }
}

k<-nrow(sub_matrix)
cv <- matrix(data=NA,nrow = k,ncol = 1)
for(j in 1:k){
  Xsub    <-x_train[,which(sub_matrix[j,]==1)]
  betaMLE <-solve(t(Xsub)%*%Xsub)%*%t(Xsub)%*%y_train
  new_Y   <-x_test[,which(sub_matrix[j,]==1)]%*%betaMLE
  cv[j,]      <- t(y_test-new_Y)%*%(y_test-new_Y)
  
}

BEST_cv<-which(sub_matrix[which.min(cv),]==1)
BEST_cv
Xsub_cv<-x_train[,BEST_cv]
betaMLE_cv<-solve(t(Xsub_cv)%*%Xsub_cv)%*%t(Xsub_cv)%*%y_train
betaMLE_cv
```

Each time it changes but most of the times we obtain the best model when p=5. The beta sometimes is not close to generating vector $\beta$ = [3 1.5 0 2 0]. 

> (e) Suppose now that we increase the size of $\boldsymbol{\beta}$ to 100 (i.e. $p = 100$ ). Calculate the number of possible models to evaluate together with an estimate of the time needed for an exhaustive search (*hint: use previous results*). Conclude on the feasibility of this task.    


```{r,echo=FALSE}
vec=rep(0,100)
for (i in 1:100) {
  vec[i]=choose(100,i)
}
sum(vec)
sum(vec)/31*0.1691079

```

When we run a CV process with p=5, it takes 0.1691079 seconds. For p = 5, we can have 31 different models thanks to Newton's binomial theorem (i.e. $2^p - 1$). Unfortunately when we increase p to 100, we have 1.26e+30 different models. Thus it will take approximately 1.9e+24 hours. This is the case when we do k=2 cross validation. Moreover if we increase the number of k, we will drastically increase the time needed. This task is not feasible. 

### Akaike Information Criterion{#aic}

(solutions provided by **Alexander Maslev**, **Hanxiong Wang** and **Minyoung Lee**).\ 

> - Program AIC and do model selection in a specific simulation setting with an exhaustive search (follow the passages listed in the CV exercise section).\


```{r}
n<-1000
p<-5
Mu<-c(2,4,6,8,10) # location vector
sigma<-diag(5) # scale matrix as the identity
X<-mvrnorm(n , Mu, sigma)

beta<-c(3,1.5,0,2,0)
e<-rnorm(n, mean = 0, sd = 1)
SNR<-mean(X%*%beta)/var(e)

Y_hat<-X%*%beta+e
```


```{r}
index_sub_choose<-c(1,2,3,4,5)
sub_matrix <- matrix(data = NA,ncol = 5,nrow = 31)
t=0
for(i in 1:5)
{
  index_matrix <- combn(index_sub_choose,i)
  for(j in 1:ncol(index_matrix))
  {
    t <- t+1
    index_sub <- index_matrix[,j]
    sub_matrix[t,c(index_sub)]  <-  1
  }
}

# AIC
RSS<-rep(0,k)
AIC<-rep(0,k)
k<-nrow(sub_matrix)
for(j in 1:k){
Xsub<-as.matrix(X[,which(sub_matrix[j,]==1)])
betaMLE<-solve(t(Xsub)%*%Xsub)%*%t(Xsub)%*%Y_hat
new_Y<-Xsub%*%betaMLE
for(i in 1:(n/2)){
  RSS[j]<-RSS[j]+(new_Y[i]-Y_hat[i])^2
}
AIC[j]<-RSS[j]/var(e)+2*ncol(Xsub)
}
BEST<-which(sub_matrix[which.min(AIC),]==1)
BEST
Xsub<-as.matrix(X[,BEST])
betaMLE<-solve(t(Xsub)%*%Xsub)%*%t(Xsub)%*%Y_hat
betaMLE
```

Each time it changes but most of the times we obtain the best model when p=4 in position[1 2 4 5]. The estimated beta is very close to generating vector $\beta$ = [3 1.5 0 2 0].

The above results hold not considering $\sigma$ and the intercept as parameters. In the general formulation of the Akaike's information criterion, $p = dim(\Theta)$ where $\Theta$ is the parameters space.  

As we computed in point (e) in CV exercise, when p = 100, we have 1.26e+30 different model. The time needed to run the AIC process with p = 5 takes 0.2441621 seconds. When we calculate the time that we need to run when p = 100, we obtain approximately 2.7e+24 hours. Thus we still think that the task to do all the combinations when p = 100 is not feasible. 

```{r,echo=FALSE}
vec=rep(0,100)
for (i in 1:100) {
  vec[i]=choose(100,i)
}
sum(vec)
sum(vec)/31*0.2441621
9.984266e+27/60/60

```

> - Compare the performance of your programmed CV and AIC by replicating 100 times the tasks. In particular you should evaluate three specific criteria: the proportion of times the correct model is selected (*Exact*), the proportion of times the selected model contains the correct one (*Correct*) and the average number of selected regressors (*Average $\sharp$*)\


```{r}

## Rep 100 times AIC

cv <- matrix(data=NA,nrow = k,ncol = 1)
Exact<-data.frame(t(c(0,0)))
colnames(Exact)<-c("AIC","CV")
AverageN<-matrix(NA,nrow=100,ncol = 2)
colnames(AverageN)<-c("AIC","CV")
Correct<-data.frame(t(c(0,0)))
colnames(Correct)<-c("AIC","CV")

for(l in 1:100){
  
### SETTING ###
  n<-1000
  p<-5
  Mu<-c(2,4,6,8,10) # location vector
  sigma<-diag(5) # scale matrix as the identity
  X<-mvrnorm(n , Mu, sigma)
  beta<-c(3,1.5,0,2,0)
  e<-rnorm(n, mean = 0, sd = 1)
  Y_hat<-X%*%beta+e

### AIC ###    
  RSS<-rep(0,k)
  AIC<-rep(0,k)

  for(j in 1:k){
    Xsub<-as.matrix(X[,which(sub_matrix[j,]==1)])
    betaMLE<-solve(t(Xsub)%*%Xsub)%*%t(Xsub)%*%Y_hat
    new_Y<-Xsub%*%betaMLE
    for(i in 1:(n/2)){
      RSS[j]<-RSS[j]+(new_Y[i]-Y_hat[i])^2
    }
    AIC[j]<-RSS[j]/var(e)+2*ncol(Xsub)
  }
  BEST<-sub_matrix[which.min(AIC),]
  BEST[is.na(BEST)] <-0
  if(sum(BEST-c(1,1,0,1,0))==0){
    
    Exact[1]<-Exact[1]+1
    
  }
  if(sum((BEST[c(1,2,4)]-c(1,1,1)))==0){
    Correct[1]<-Correct[1]+1
  }
    AverageN[l,1]<-sum(BEST)
  
  
### CV ###
  index <- sample(1:n, size=0.5*n)
  y_train<- Y_hat[-index,]
  x_train<-X[-index,]
  y_test<- Y_hat[index,]
  x_test<-X[index,]
  
  for(j in 1:k){
    Xsub    <-x_train[,which(sub_matrix[j,]==1)]
    betaMLE <-solve(t(Xsub)%*%Xsub)%*%t(Xsub)%*%y_train
    new_Y   <-x_test[,which(sub_matrix[j,]==1)]%*%betaMLE
    cv[j,]      <- t(y_test-new_Y)%*%(y_test-new_Y)
    
  }
  
  BEST_cv<-sub_matrix[which.min(cv),]
  BEST_cv[is.na(BEST_cv)] <-0
  if(sum(BEST_cv-c(1,1,0,1,0))==0){
    
    Exact[2]<-Exact[2]+1
    
  }
  if(sum((BEST_cv[c(1,2,4)]-c(1,1,1)))==0){
    Correct[2]<-Correct[2]+1
  }
    AverageN[l,2]<-sum(BEST_cv)
   
}

Exact/100
Correct/100
colMeans(AverageN)

```

In this part, we have simulated the AIC process and the CV process for 100 times and evaluated the three criteria. We have found that AIC gives better results than CV in terms of Exact models and it has a lower number of selected regressors on average. Both methods achieves model selection consistency in this simple setting as the proportion of correct models reaches the value of 1. 

This result is expected since AIC is derived from the likelihood which inherits all the nice properties (e.g. Cramer-Rao bound etc.) when the model is correct. On the other hand, CV is a non parametric method thus inferior by definition to the AIC in this setting. However in a real application our conjectured model maybe far from the truth and CV could be a better choice.

> - In the same simulation setting outlined in the CV exercise section, generate from a MVN (multivariate normal) a matrix $\mathbf{X_{n*p}}$ with $n = 1000$ and $p = 5$ but now fix the scale matrix with an autoregressive form $\boldsymbol{\Sigma}=[\sigma_{lm}]_{l,m=1,\ldots,p}$  with $\sigma_{lm} = \rho^{\mid l - m\mid}$. Compare the performance of CV and AIC for $\boldsymbol{\rho} = [0.2 \; 0.5\; 0.7]$ ($\rho = 0$ corresponds to the identity case that you have already treated).\

There are different sources of randomness: X matrix, epsilon (error), y (induced) and the split for CV.We need to choose what to preserve: we replicate every time only the randomness of epsilon and of course the split of the CV thus we need to set a seed for the X matrix. All this discussion on randomness is pivotal for a significant and controlled simulation setting. 

```{r}

require(MASS)

set.seed(5) # For the X matrix

n = 1000

p = 5

rho = 0.5 # change rho as you please to inspect other correlations among the predictors

mu = rep(1,p)

sigma = rep(0,p^2)

sigma = matrix(data = sigma, ncol = p,nrow = p)


# Autoregressive structure

for (i in 1:p) {
  
  for (j in 1:p) {
    
    
    sigma[i,j] = rho^(abs(i-j))
    
    
  }
  
  
}

X = mvrnorm(n,mu,sigma)


# beta = c(0,0,1,0,0) alternative beta to check bias properties

beta = c(3,1.5,0,2,0)

ind = 1:1000

pos_CV = rep(0,100)

pos_AIC = rep(0,100)


for(z in 1:100) {
  
  
  y_hat = X%*%beta + rnorm(n,0,1) 
  
  data = data.frame(cbind(y_hat,X))
  
  colnames(data) = c("y","x1","x2","x3","x4","x5")
  
  index = sample(x = ind,size = 500,replace = F)
  
  train_set = data[index,]
  
  test_set = data[-index,]
  
  cv_error = rep(0,31)
  
  aic_values = rep(0,31)
  
  # One regressor only models
  
  for(i in 1:5) {
    
    
    m_1 = lm(train_set$y ~ train_set[,i+1], data = train_set)
    
    cv_error[i] = mean((test_set$y - (cbind(rep(1,500),test_set[,i+1])%*% m_1$coefficients))^2)
    
    m_1 = lm(data$y ~ data[,i+1], data)
    
    aic_values[i] = -2*logLik(m_1) + 2*3
    
  }
  
  # Two regressors models
  
  x = c(1,2,3,4,5)
  
  M = combn(x,2)
  
  
  for(i in 1:10) {
    
    
    m_2 = lm(train_set$y ~ train_set[,M[1,i]+1] + train_set[,M[2,i]+1], data = train_set)
    
    cv_error[i+5] = mean((test_set$y - cbind(rep(1,500),test_set[,M[1,i]+1], test_set[, M[2,i] +1])%*%m_2$coefficients)^2)
    
    m_2 = lm(data$y ~ data[,M[1,i]+1] + data[,M[2,i]+1], data)
    
    aic_values[i+5] = -2*logLik(m_2) + 2*4
  }
  
  
  # Three regressors models
  
  M = combn(x,3)
  
  
  for(i in 1:10) {
    
    
    m_3 = lm(train_set$y ~ train_set[,M[1,i]+1] + train_set[,M[2,i]+1] + train_set[,M[3,i]+1], data = train_set)
    
    cv_error[i+ 15] = mean((test_set$y - cbind(rep(1,500),test_set[,M[1,i]+1], test_set[, M[2,i] +1],test_set[, M[3,i]+1])%*%m_3$coefficients)^2)
    
    m_3 = lm(data$y ~ data[,M[1,i]+1] + data[,M[2,i]+1] + data[,M[3,i]+1], data)
    
    aic_values[i+15] = -2*logLik(m_3) + 2*5
    
  }
  
  
  # Four regressors models
  
  M = combn(x,4)
  
  
  for(i in 1:5) {
    
    
    m_4 = lm(train_set$y ~ train_set[,M[1,i]+1] + train_set[,M[2,i]+1] + train_set[,M[3,i]+1] + train_set[,M[4,i]+1], data = train_set)
    
    cv_error[i+25] = mean((test_set$y - cbind(rep(1,500),test_set[,M[1,i]+1], test_set[, M[2,i] +1],test_set[, M[3,i]+1],test_set[, M[3,i]+1])%*% m_4$coefficients)^2)
    
    m_4 = lm(data$y ~ data[,M[1,i]+1] + data[,M[2,i]+1] + data[,M[3,i]+1] + data[,M[4,i]+1], data)
    
    aic_values[i+25] = -2*logLik(m_4) + 2*6
    
  }
  
  # Full model 
  
  full_model = lm(train_set$y ~ ., data = train_set)
  
  full_model$coefficients
  
  cv_error_full = mean((test_set$y - cbind(rep(1,500), test_set$x1,test_set$x2,test_set$x3,test_set$x4,test_set$x5)%*% full_model$coefficients)^2)
  
  cv_error[31] = cv_error_full
  
  full_model = lm(data$y ~ ., data)
  
  aic_values[31] = -2*logLik(full_model) + 2*7
  
  pos_CV[z] = which.min(cv_error)
  
  pos_AIC[z] = which.min(aic_values)
  
}

# We know that the right position should be 17 which is the exact model.

# Exact model proportion

sum(pos_CV == "17")/100 

sum(pos_AIC == "17")/100 

# Correct model proportion (linked to the consistency of the model selection procedure)

sum(pos_CV > 16)/100 

sum(pos_AIC > 16)/100


# Average number of regressors

counted = c(rep(1,5),rep(2,10), rep(3,10), rep(4,5),5)

result_cv = rep(0,100)

result_aic = rep(0,100)


for (i in 1:100) {
  
  
  result_cv[i] = counted[pos_CV[i]]

  result_aic[i] = counted[pos_AIC[i]]  
  
}


# CV CASE

mean(result_cv)

# AIC case

mean(result_aic)


```

CONCLUSIONS: correlation does not play a role here because it biases our estimates but not that much that is necessary to make AIC (less so CV) fails to recognize the significant regressors. This holds especially in the presence of strong signal so for a beta with important components. The transition chain is: we create correlated X, we transmit correlation to the ys, the likelihood theory is not optimal anymore (y should be independent, X fixed) and I get biased estimates. However I need a really important bias to make AIC fails to recognize significant variables in the linear case. The GLM case is different: correlation is a real issue because of the link function. Even a small bias in that direction can alter the order of variables and thus AIC results. 

> - Upload the [Zambia dataset](https://github.com/CaesarXVII/MSHD-book-and-datasets/blob/master/datasets/malnutrion_zambia_cleaned.Rda) and perform an exhaustive search on the continuous covariates (i.e. avoiding factors) based on CV and AIC in order to find the best model. You can either employ your codes derived in previous exercises or make use of the existing R packages: *leaps*, *glmulti*, *MuMIn* and *caret*. \


```{r,eval=FALSE}

# Load Zambia dataset 

load(file = "your_directory/data_zambia.Rda"")

data_zambia = data_zambia[,c(1:7,21,22,24)] #exclude the factors from the analysis


### Exhaustive search with leaps (AIC case) ###

require(leaps)

regsubsets.out <- regsubsets(data_zambia$`Height for age sd` ~ .,data = data_zambia,nbest = 1,
                             nvmax = NULL,    # NULL for no limit on number of variables
                             force.in = NULL, force.out = NULL,
                             method = "exhaustive")

summary(regsubsets.out)


plot(regsubsets.out) # BIC is default

plot(regsubsets.out,scale = "Cp") #C_p case which is equal to AIC with a linear model


### Exhaustive search with glmulti (AIC case) ### 


glmulti.lm.out <- glmulti::glmulti(data_zambia$`Height for age sd` ~ .,data = data_zambia,
          level = 1,               # No interaction considered
          method = "h",            # Exhaustive approach
          crit = "aic",            # AIC as criteria
          confsetsize = 5,         # Keep 5 best models
          plotty = F, report = F,  # No plot or interim reports
          fitfunction = "lm")      # lm function



### Exhaustive search with MumIn (AIC case) ###

require(MuMIn)


data_model <- lm(data_zambia$`Height for age sd` ~ .,data = data_zambia)


combinations <- dredge(data_model)

print(combinations)


### Exhaustive search with caret (CV case and AIC case) ###

require(caret)

attach(data_zambia)

# Unfortunately there is no exhaustive search based on CV in Caret, it is just stepwise.

#setting up 10-fold cross-validation

control <- trainControl(method="cv", number=10) 

#finding the ideal model by AIC criterion

model_AIC = train(`Height for age sd`~.,data=data_zambia,method="lmStepAIC",trControl=control)

#finding the ideal model by mean square error

modelCV = train(`Height for age sd`~.,data=data_zambia,method="lm",trControl=control)

detach(data_zambia)

# In order to do a full exhaustive search with CV, we should exploit the codes produced in CV exercise. Of course 
# as the number of variables increase, the task becomes impossible.

```

### ROC curves {#roc}

Read the information on the [Leukemia Dataset](https://caesarxvii.github.io/MSHD-book-and-datasets/index.html#prognostic-factors-in-childhood-leukemia) in the first chapter of the book. Then load the [Leukemia dataset reduced](https://github.com/CaesarXVII/MSHD-book-and-datasets/blob/master/datasets/data_leukemia_reduced.Rda) which contains a subset of 11 eleven predictors among the 3571 present in the *leukemia_small.csv*. These variables have been selected, because of their importance, by the binary lasso which is a shrinkage method that will be discussed later on in the course. Now perform the following steps:

> Fit the appropriate GLM for the situation using only one of the available predictors (e.g. V457)\

```{r}

load("data_leukemia_reduced.Rda")

## Part (a)

mod_1 = glm(y ~ V457, data = data_leukemia_reduced,family = "binomial")

```

> Read the [ROC curve](https://caesarxvii.github.io/MSHD-book-and-datasets/assessing-the-validity-of-a-model.html#the-roc-curve) section of the e-book. Then find the TPR (i.e. true positive rate), FPR (i.e. false positive rate), TNR (i.e. true negative rate), FNR (i.e. false negative rate) of the fitted values found at point (a) with a cut-off value $c = 0.5$.\

```{r}
# Confusion matrix

c0 = 0.5

mod_1 = glm(y ~ V457, data = data_leukemia_reduced,family = "binomial")


conf_mat = table(data_leukemia_reduced$y, mod_1$fitted.values > c0) #unbalanced sample 47 ones and 25 0s

conf_mat

prop_confmat = prop.table(conf_mat, 1) # For ROC curve is on the line

prop_confmat

TNR = conf_mat[1,1]/(conf_mat[1,1] + conf_mat[1,2]) # TRUE NEGATIVE (TNR)

FPR = conf_mat[1,2]/(conf_mat[1,1] + conf_mat[1,2])  # FALSE POSITIVE (FPR)

FNR = conf_mat[2,1]/(conf_mat[2,1] + conf_mat[2,2])  # FALSE NEGATIVE (FNR)
  
TPR = conf_mat[2,2]/(conf_mat[2,1] + conf_mat[2,2])  # TRUE POSITIVE (TPR)
                               
# Also possible with caret but with train and test arguments. Not full dataset inquiry as in this case.

# require(caret)

# confusionMatrix()

```


> For a given cut-off grid of values, that you can choose as you wish, plot the ROC curve relative to the estimated model.\

```{r}

c0 = seq(0.1,0.9,0.1)

TP = rep(0,length(c0))

FP = rep(0,length(c0))

mod_1 = glm(y ~ V457, data = data_leukemia_reduced,family = "binomial")

for (i in 1:length(c0)) {
  
  conf_mat = table(data_leukemia_reduced$y, mod_1$fitted.values > c0[i]) #unbalanced sample 47 ones and 25 0s
  
  prop_confmat = prop.table(conf_mat, 1) # For ROC curve is on the columns
  
  TP[i] = prop_confmat[2,2]
  
  FP[i] = prop_confmat[1,2]
  
}

FP = sort(FP) # for a correct line

TP = sort(TP) # for a correct line

# Every ROC curve passes towards (0,0) and (1,1)

TP = c(0,TP,1)


FP = c(0,FP,1)


plot(FP,TP, col="green",main = "ROC Curve",xlab = "1 - Specificity",ylab = "Sensitivity",xlim=c(0, 1), ylim=c(0,1),type = "l")
lines(c(0, 1), c(0, 1), type='l',col="red")


```

> Check the quality of your result at point (c) with the R package *pROC*.\

```{r,warning=FALSE}

# install.packages("pROC")

require(pROC)

roc_1 = roc(response=data_leukemia_reduced$y, predictor = mod_1$fitted.values)

plot(roc_1)


```

## Chapter 3

### Selection by Hypothesis Testing {#HT}

> First of all we retrieve the simulation setting used in *Practical 3*.\

```{r, warning=FALSE}

# install.packages("selectiveInference")

require(selectiveInference)

require(MASS)

# Create the simulation setting

set.seed(11)

n = 1000

p = 10

# change rho as you please to inspect other correlations among the predictors

rho = 0 

mu = rep(1,p)

sigma = rep(0,p^2)

sigma = matrix(data = sigma, ncol = p,nrow = p)


# Autoregressive structure

for (i in 1:p) {
  
  for (j in 1:p) {
    
    
    sigma[i,j] = rho^(abs(i-j))
    
    
  }
  
  
}

X = mvrnorm(n,mu,sigma)


beta = c(3,1.5,0,2,rep(0,6))


y = X%*%beta + rnorm(n,0,1) #for us sigma is 1


```

> Now, after having read the documentation of the R package *selectiveInference* and installed it, perform the following steps:\
a) Use the functions *fs()*, *fsInf()* and *forwardStop()* to do a stepwise regression based on partial correlations and a model selection phase with the ForwardStop rule on your generated data. Try different values for the type one error: how does the choice of $\alpha$ impact the model selection technique?\

```{r}

## Part a ##


fsfit = fs(X,y) #partial correlations like OMP or stagewise

fsfit$action #order of regressors

fsfit$beta #the beta estimation at each step


# compute sequential p-values and confidence intervals  (sigma estimated from full model) 

out = fsInf(fsfit,alpha = 0.1) #default

out # the value of forward stop is the last regressor which is active

# estimate optimal stopping point 

last = forwardStop(out$pv, alpha=0.4) # you want to reject more often fixing higher alfa

stop_point = forwardStop(out$pv,alpha = 0.85)

fsfit$action[1:last] # to catch right components

# Conclusions: fixing a higher alfa will let you include on average more regressors as you will reject more often H0 the null hypothesis. 

```

> b) Given the order of variables produced by *fs()*, use AIC and BIC criteria for model selection to retrieve your final model (*Hint: you do not need to program them, use an existing function of the selectiveInference package*).\

```{r}

## Part b ##

# We use the same fit, the k step is decided by AIC penalty

out_2 = fsInf(fsfit,type = "aic",alpha = 0.05) #akaike case, the fixed value of alfa is 0.1

out_2 #be careful check with classic AIC evaluation (sometimes is not correct)


# We have already an idea that AIC tends to overfit


### YOU CAN PROGRAM AIC USING ORDER IN fsfit$action ###


# We use the same fit, the k step is decided by BIC penalty


out_3 = fsInf(fsfit, type = "aic", mult = log(n)) #bic case

out_3

# Another nice evidence, BIC has harsh penalty than AIC.

# Other possibilities of fsInf() function

out_4 = fsInf(fsfit,type = "all",k = 4,mult = log(n)) 

# this is just to stop at a fixed number of steps

out_4

```

> c) Calculate how many models are needed for an exhaustive search in this simulation setting. Use your previous results obtained in *Practical 3* to understand the computational time gained by stepwise regression with respect to exhaustive search. Use the package *tictoc* for this comparison.\

```{r, eval=FALSE}

## Part c ##


# Look at Practical 3, see your exhaustive search time for p=10. 

# In my case around 17 minutes for 1023 models (2^10 -1) since I used 1sec per model

require(tictoc)

tic()

fsfit = fs(X,y)

out = fsInf(fsfit) #default

last = forwardStop(out$pv, alpha=0.05) # you want to reject more often fixing higher alfa

Sys.sleep(1)
toc()

# 1.81 against 17 minutes... Exhaustive vs stepwise, a huge gain indeed!

```

> d) (*Optional*) Change the simulation setting outlined above to an high dimensional one: generate from a MVN (multivariate normal) a matrix $\mathbf{X_{nxp}}$ with $n = 100$ and $p = 150$. Evaluate the performance of the ForwardStop rule in this high dimensional setting (i.e. by replicating the model selection task 100 times) thanks to the usual three specific criteria: the proportion of times the correct model is selected (*Exact*), the proportion of times the selected model contains the correct one (*Correct*) and the average number of selected regressors (*Average* $\sharp$}). What do you observe? What is the role of $\alpha$ in this case?\

```{r, eval=FALSE}

## Part d ##

# Create the simulation setting

set.seed(11)

n = 100

p = 100  

#p = 150 --> It does not work as we can not invert this kind of matrix thus we need n > or = than p

# change rho as you please to inspect other correlations among the predictors

rho = 0 

mu = rep(1,p)

sigma = rep(0,p^2)

sigma = matrix(data = sigma, ncol = p,nrow = p)


# Autoregressive structure

for (i in 1:p) {
  
  for (j in 1:p) {
    
    
    sigma[i,j] = rho^(abs(i-j))
    
    
  }
  
  
}

X = mvrnorm(n,mu,sigma)


beta = c(3,1.5,0,2,rep(0,96))


y = X%*%beta + rnorm(n,0,1) #for us sigma is 1


fsfit = fs(X,y) #partial correlations like OMP or stagewise


# You need to have p < n to work with this kind of package. Or you can do a first screening based on

# marginal correlations (like in SIS): order them and take the first n. 


sigma_hat = estimateSigma(X,y) 


# You need to estimate sigma in this big models. If not there is bad estimation by default since sigma

# is estimated on the full model.


# compute sequential p-values and confidence intervals

out = fsInf(fsfit,sigma = 0.977) 


out # the value of forward stop is the last regressor which is active

# estimate optimal stopping point 


last = forwardStop(out$pv, alpha=0.05) # you want to reject more often fixing higher alfa

fsfit$action[1:last] # to catch right components


# Evaluation with model selection criteria

order = list()

for (z in 1:100) {
  
  y = X%*%beta + rnorm(n,0,1) 
  
  
  fsfit = fs(X,y) 
  
  
  sigma_hat = estimateSigma(X,y) 
  
  a = sigma_hat$sigmahat
  
  out = fsInf(fsfit,sigma = a) 
  
  last = forwardStop(out$pv, alpha=0.05) # you want to reject more often fixing higher alfa
  
  order[[z]] = fsfit$action[1:last] 
  
}

### Exact models ###

exact = rep(0,100)

for (i in 1:100) {
  
  if (identical(order[[i]],c(1,4,2)) == TRUE){
    
    exact[i] = 1
    
    
  }
  
  
}

sum(exact)/100  

### Correct (related to consistency in model selection) ###

prop_cor = rep(0,100)

for (i in 1:100) {
  
  if (length(order[[i]]) >= 3) {
    
    
    prop_cor[i] = 1
    
  } 
}

sum(prop_cor)/100  #Due to the low number of observations n=100

  
### Average number of regressors ###


# Find the number of elements in each order[[z]]

num.el = sapply(order, length)

sum(num.el)/100

# The role of alfa is the same as before, as you increase it you get overfitting while if you lower it

# you will reject more often thus leading to a sparser model. 


```

> Consider the Malnutrition in Zambia dataset. For simplicity work only on the continuous covariates (i.e. avoiding factors) and order them according to their partial correlations using the R function *fs* of the *Selective Inference* R Package (<https://cran.r-project.org/web/packages/selectiveInference/index.html>). Compare the selected models when using: \ 
  
  > a) the ForwardStop \
  b) the $C_p$ or AIC (equal in linear case) \
  c) the BIC

```{r}

load("malnutrion_zambia_cleaned.Rda")


data_zambia = data_zambia[,c(1:7,21,22,24)] #exclude the factors from the analysis

### selective inference package ###

## ForwardStop rule ##

X = as.matrix(data_zambia[,2:10])

y = data_zambia[,1]

fsfit = fs(X,y) 

fsfit$action #order by partial correlations

# compute sequential p-values and confidence intervals # (sigma estimated from full model)

out = fsInf(fsfit) 

# The above function is using forward stop and pvalue evaluated at each step with forwardstop 

# It is a formula to control FDR.

out 

forwardStop(out$pv, alpha=.10) #only breastfeeding and height of the mother matters


## AIC and BIC penalty to decide step k ##

out_2 = fsInf(fsfit,type = "aic") #akaike case, the fixed value of alfa is 0.1

out_2 #estimated stopping point is correct. 

# You notice that AIC tends to overfit in this framework. Remember that we were doing 

# exhaustive search and not stepwise with AIC in the other Practicals.

out_3 = fsInf(fsfit, type = "aic", mult = log(n)) #bic case

out_3 # BIC has a harsh penalty that correct AIC tendency to overfit (in this applied context)

# Here we are in a reality case so AIC is not optimal anymore as in our simulation setting, we expect

# something different while forward stop control the FRD. BIC has a harsh penalty that is why it 

# selects less variables. Moreover we know that, from previous practicals, our residuals inspection

# told us that we had a right tail with respect to the Normal QQ plot (see ebook corrections). 


```
  
### Sure Independence Screening {#SIS}  

> a) Load the *Leukemia* dataset.\
> b) Split the dataset randomly and create a *train* and *test* sample.\

```{r, warning=FALSE}

require(SIS) #install it if you do not have it

# Leukemia dataset taken from SIS package.

train_set = data(leukemia.train)

test_set = data("leukemia.test")

train_set = as.data.frame(train_set)

test_set = as.data.frame(test_set)


X_train = as.matrix(leukemia.train[,1:7129])

y_train = leukemia.train[,7130]


X_test = as.matrix(leukemia.test[,1:7129])

y_test = leukemia.test[,7130]

```

> c) The functions SIS() performs first a screening procedure based on marginal correlations and then applies a penalized method (*Chapter 4* of the e-book) to obtain the final model. Choose among all the available options (i.e. in terms both of penalized methods and tuning constants) three candidates and evaluate the predictions of the selected models on the *test* sample. Which penalized method performs best in this specific example after the SIS?

```{r, warning=FALSE}

# Model Phase

model_leuk_scad=SIS(X_train, y_train, family='binomial') #SCAD is default

model_leuk_lasso = SIS(X_train, y_train, family='binomial',penalty = 'lasso')

model_leuk_mcp = SIS(X_train, y_train, family='binomial',penalty = 'MCP')

# Prediction Phase

pred_scad=predict(model_leuk_scad,X_test,type="class")

sum(pred_scad == y_test)/34 #0.7941176 

pred_lasso=predict(model_leuk_lasso,X_test,type="class")

sum(pred_lasso == y_test)/34 #  0.7941176

pred_mcp=predict(model_leuk_mcp,X_test,type="class")

sum(pred_mcp == y_test)/34 # 0.7941176

# We got the same predictions for every method. 

# SCAD and MCP are near in terms of coefficient estimates but Lasso has one regressor more. 

```

### PC-simple algorithm{#PC}

> First of all build a simulation setting as explained below:\
- Generate from a MVN (multivariate normal) a matrix $\mathbf{X_{n*p}}$ with $n = 1000$ and $p = 10$. Choose the location but set the scale matrix with an autoregressive form $\boldsymbol{\Sigma}=[\sigma_{lm}]_{l,m=1,\ldots,p}$  with $\sigma_{lm} = \rho^{\mid l - m\mid}$.\ 
- Fix $\rho = 0.5$ and set the seed equal to 11 (i.e. set.seed(11)).\
- Choose the generating vector $\boldsymbol{\beta }= [3 \; 1.5 \; 0 \; 2 \; rep(0,6)]$.\
- Generate $\mathbf{\hat{y}}$ thanks to the relation $\mathbf{y} =  \mathbf{X_{n*p}} \; \boldsymbol{\beta} + \boldsymbol{\epsilon}$ where $\epsilon_{i}$ is a standard normal. Suppose for simplicity that the errors are uncorrelated.\

```{r}

## Simulation Setting

require(MASS)

set.seed(11)

n = 100

p = 10 

# change rho as you please to inspect other correlations among the predictors

rho = 0.5 

mu = rep(1,p)

sigma = rep(0,p^2)

sigma = matrix(data = sigma, ncol = p,nrow = p)


# Autoregressive structure

for (i in 1:p) {
  
  for (j in 1:p) {
    
    
    sigma[i,j] = rho^(abs(i-j))
    
    
  }
  
  
}

X = mvrnorm(n,mu,sigma)

dim(X)

beta = c(3,1.5,0,2,rep(0,6))


y = X%*%beta + rnorm(n,0,1) #for us sigma is 1

```

> Now perform the following passages on your simulated data: \
a) Find the active set $M_{1}$ using the Fisher's Z transformation and the associated correlation coefficient test (fix $\alpha = 0.05$ for the rest of the exercise).\

```{r, eval=FALSE}

## Part a

# Fix alfa at 5% for all the exercise

v_cor = cor(x = X,y = y)

q=rep(0,10)


for (i in 1:10) {
  
  if(sqrt(n-3)*abs(1/2*log((1 + v_cor[i])/(1-v_cor[i]))) > 1.96 ) {
    
    q[i]=1
    
  }
  
}

A_0 = X #Active set zero

# With the first step we select the first 5 variables, we reject H0 so that we have evidence that the

# correlation is different from 0. 

A_1 = X[,1:5] #new active set

```

> b) Calculate all the partial correlations of order 1 (i.e. one variable at the time) of the active set $M_{1}$, test them and retrieve $M_{2} \subseteq M_1$ which is the new active set.\

```{r, eval=FALSE}

## Part b

# We can now create the 20x1 vector of first order partial correlation and then apply previous formula


P = rep(0,25)

P = matrix(P,nrow = 5,ncol = 5)



for (z in 1:5) {
  
  for (i in 1:5) {
    
    if (i != z ) {
      
      
      mod_1 = lm(A_1[,z] ~ A_1[,i])
      
      e_1 = A_1[,z] - cbind(rep(1,n),A_1[,i])%*%mod_1$coefficients
      
      mod_2 = lm(y ~ A_1[,i])
      
      e_2 =  y - cbind(rep(1,n),A_1[,i])%*%mod_2$coefficients
      
      
      P[i,z] = cor(e_1,e_2) 
      
      
    }
    
  }
  
  
}

P = P + diag(5)

v_cor = c(P) #vectorization

# fix alfa at 5% and test the sample correlation.

q=rep(0,25)


# Cardinality of the active set is now 1 because we are evaluating partial correaltions of order 1

for (i in 1:25) {
  
  if(sqrt(n-4)*abs(1/2*log((1 + v_cor[i])/(1-v_cor[i]))) > 1.96 ) {
    
    q[i]=1
    
  }
  
}

q # variables 3 and 5 are eliminated from the active set

A_2 = X[,c(1,2,4)]

```

> c) Find the partial correlations of higher order and test them until your reach the condition $M_{m-1} = M_{m}$ which implies the convergence of the PC-simple algorithm. Do you obtain the exact model?

```{r, eval=FALSE}

## Part c

# Now we need to check 2nd order partial correlation to discriminate.

# Note: we could also update estimation for partial correlation in order to speed up the computations

# but in this easy example is not necessary. 

# VARIABLE 1 ACTIVE SET A_2

mod_1 = lm(A_2[,1] ~ A_2[,2] + A_2[,3])

e_1 = A_2[,1] - cbind(rep(1,n),A_2[,2],A_2[,3])%*%mod_1$coefficients

mod_2 = lm(y ~ A_2[,2] + A_2[,3])

e_2 =  y - cbind(rep(1,n),A_2[,2],A_2[,3])%*%mod_2$coefficients

# Cardinality of active set now is two!

sqrt(n-5)*abs(1/2*log((1 + cor(e_1,e_2))/(1-cor(e_1,e_2)))) > 1.96

# VARIABLE 2 ACTIVE SET A_2

mod_1 = lm(A_2[,2] ~ A_2[,1] + A_2[,3])

e_1 = A_2[,2] - cbind(rep(1,n),A_2[,1],A_2[,3])%*%mod_1$coefficients

mod_2 = lm(y ~ A_2[,1] + A_2[,3])

e_2 =  y - cbind(rep(1,n),A_2[,1],A_2[,3])%*%mod_2$coefficients

# Cardinality of active set now is two!

sqrt(n-5)*abs(1/2*log((1 + cor(e_1,e_2))/(1-cor(e_1,e_2)))) > 1.96

# VARIABLE 3 ACTIVE SET A_2

mod_1 = lm(A_2[,3] ~ A_2[,1] + A_2[,2])

e_1 = A_2[,3] - cbind(rep(1,n),A_2[,1],A_2[,2])%*%mod_1$coefficients

mod_2 = lm(y ~ A_2[,1] + A_2[,2])

e_2 =  y - cbind(rep(1,n),A_2[,1],A_2[,2])%*%mod_2$coefficients

# Cardinality of active set now is two!

sqrt(n-5)*abs(1/2*log((1 + cor(e_1,e_2))/(1-cor(e_1,e_2)))) > 1.96

# Conclusions: all conditions are true, then the algorithm will not move anymore and we have 

# our final model with the original variables c(1,2,4) which is also the exact one. 

```

### Regression Tree {#RT}

> a) Load the *Zambia* dataset, split it randomly in a *train* and *test* sample (common choice is $\frac{2}{3}$ train and $\frac{1}{3}$ test). For simplicity, you can consider only the continuous variables. \

```{r, warning=FALSE}

require(rpart)

load("malnutrion_zambia_cleaned.Rda")


data_zambia = data_zambia[,c(1:7,21,22,24)] #exclude the factors from the analysis

X = as.matrix(data_zambia[,2:10])

y = data_zambia[,1]


## Part a

ind = 1:1927

index = sample(x = ind,size = 1284,replace = F)

data_zambia_train= data_zambia[index,]

data_zambia_test= data_zambia[-index,]

```


> b) Fit a regression tree with the function rpart() and plot the tree. Have a look at *rpart.plot* package if you want to improve the appearance of the fitted tree. \ 

```{r, warning=FALSE}

## Part b

# grow tree 

attach(data_zambia_train)

fit <- rpart(`Height for age sd`~ 
               `Breastfeeding duration (months)` + `Age of the child (months)`+`Age of the mother (years)`+`BMI mother`+`Heigth mother (meter)`+`Weight mother (kg)`+`Wealth index factor score`+`Child weight at birth (kg)`+`Interval between births`, 
             method="anova", data=data_zambia_train)

printcp(fit) # display the results 
plotcp(fit) # visualize cross-validation results 
# summary(fit) detailed summary of splits

library(rattle)
library(rpart.plot)
library(RColorBrewer)

fancyRpartPlot(fit)

detach(data_zambia_train)

```



> c) After having pruned the tree, evaluate its prediction on the *test* sample (i.e. use predict() on a tree object.

```{r, warning=FALSE}

## Part c

attach(data_zambia_train)

# prune the tree

fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"]

fit$cptable

pfit<- prune(fit, cp=0.02406722) # from cptable 

fancyRpartPlot(pfit) 

# Notice the different tree after pruning: Age of the child and Weight of the mother stays relevant

# while other variables disappear. 

Prediction <- predict(pfit, data_zambia_test, type = "vector") 

# We can use the mean squared error to evaluate the predictions

mse = sum((Prediction - data_zambia_test$`Height for age sd`)^2)/length(Prediction)

detach(data_zambia_train)

# Now we should find a benchmark to understand how well is doing the Regression Tree

# In this case we will use linear regression plus model selection thanks to FDR rule.

X_train = as.matrix(data_zambia_train[,2:10])


mod_zambia = lm(data_zambia_train[,1] ~ X_train, data = data_zambia_train)

object = summary(mod_zambia)

# Fix a level q = 0.05 of FDR (see Hochberg&Benjamini), order the p-values and we reject three H0 thus

# selecting variables: Breastfeeding duration, Weight at birth and wealth index factor scores.

mod_zambia_final = lm(data_zambia_train[,1] ~ X_train[,c(1,7,8)], data = data_zambia_train)

X_test = as.matrix(data_zambia_test[,c(2,8,9)])

predictions_linear = cbind(rep(1,643),X_test)%*%mod_zambia_final$coefficients

mse_linear = sum((predictions_linear - data_zambia_test$`Height for age sd`)^2)/length(Prediction)

# Similar to the one of the Regression Tree so there is not a big improvement.

mse

mse_linear

```


### Classification Tree {#CT}

> a) Load the *Iris* dataset already present in R, split it randomly in a *train* and *test* sample (common choice is $\frac{2}{3}$ train and $\frac{1}{3}$ test).\ 

```{r, warning=FALSE}

## Part a

data_iris = iris

ind = 1:150

index = sample(x = ind,size = 100,replace = F)

train= data_iris[index,]

test= data_iris[-index,]

```


> b) Fit a classification tree with the function rpart() and plot the tree. Have a look at *rpart.plot* package if you want to improve the appearance of the fitted tree.\ 

```{r, warning=FALSE}

## Part b

attach(train) 

fit_iris = rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width,
            data=train,
            method="class")


printcp(fit_iris) # display the results 
plotcp(fit_iris) # visualize cross-validation results 

fancyRpartPlot(fit_iris)

detach(train)

```


> c) After having pruned the tree, evaluate its prediction on the *test* sample (i.e. use predict() on a tree object).\ 

```{r, warning=FALSE}

## Part c

# prune the tree 

pfit_iris<- prune(fit_iris, cp=   fit$cptable[which.min(fit$cptable[,"xerror"]),"CP"])

fancyRpartPlot(pfit_iris)

Prediction <- predict(fit_iris, test, type = "class") #pruned or not pruned trees are equal in this example

t = Prediction == test$Species

sum(t)/length(test$Species)

```


