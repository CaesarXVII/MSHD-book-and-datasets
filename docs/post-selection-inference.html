<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Model Selection in High Dimensions</title>
  <meta name="description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Model Selection in High Dimensions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Model Selection in High Dimensions" />
  
  <meta name="twitter:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  

<meta name="author" content="Maria-Pia Victoria-Feser (professor), Cesare Miglioli and Guillaume Blanc (teaching assistants)">


<meta name="date" content="2019-02-23">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="properties-of-model-selection-criteria.html">
<link rel="next" href="solutions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#read-this-part-first"><i class="fa fa-check"></i><b>1.1</b> Read this part first</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#content-choice-and-structure"><i class="fa fa-check"></i><b>1.2</b> Content choice and structure</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#bibliography"><i class="fa fa-check"></i><b>1.2.1</b> Bibliography</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#useful-links"><i class="fa fa-check"></i><b>1.2.2</b> Useful links</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#using-r"><i class="fa fa-check"></i><b>1.3</b> Using R</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#useful-r-packages"><i class="fa fa-check"></i><b>1.3.1</b> Useful R packages</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#managing-data"><i class="fa fa-check"></i><b>1.3.2</b> Managing Data</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#loading-data-from-an-r-package"><i class="fa fa-check"></i><b>1.3.3</b> Loading data from an R package</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#loading-data-from-a-local-file"><i class="fa fa-check"></i><b>1.3.4</b> Loading data from a local file</a></li>
<li class="chapter" data-level="1.3.5" data-path="index.html"><a href="index.html#loading-data-from-an-online-file"><i class="fa fa-check"></i><b>1.3.5</b> Loading data from an online file</a></li>
<li class="chapter" data-level="1.3.6" data-path="index.html"><a href="index.html#loading-data-from-an-online-database-using-a-mysql-query-optional"><i class="fa fa-check"></i><b>1.3.6</b> Loading data from an online database using a mySQL query (Optional)</a></li>
<li class="chapter" data-level="1.3.7" data-path="index.html"><a href="index.html#data-wrangling"><i class="fa fa-check"></i><b>1.3.7</b> Data Wrangling</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#writing-reports"><i class="fa fa-check"></i><b>1.4</b> Writing reports</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#r-markdown"><i class="fa fa-check"></i><b>1.4.1</b> R Markdown</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4.2</b> GitHub</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.5</b> Examples</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#data-on-malnutrition-in-zambia"><i class="fa fa-check"></i><b>1.5.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#prognostic-factors-in-childhood-leukemia"><i class="fa fa-check"></i><b>1.5.2</b> Prognostic Factors in Childhood Leukemia</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#gene-expression-in-prostate-cancer"><i class="fa fa-check"></i><b>1.5.3</b> Gene Expression in Prostate Cancer</a></li>
<li class="chapter" data-level="1.5.4" data-path="index.html"><a href="index.html#gene-expression-ratios-in-lung-cancer-and-mesothelioma"><i class="fa fa-check"></i><b>1.5.4</b> Gene Expression Ratios in Lung Cancer and Mesothelioma</a></li>
<li class="chapter" data-level="1.5.5" data-path="index.html"><a href="index.html#r-package-quantmod"><i class="fa fa-check"></i><b>1.5.5</b> R package quantmod</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#fundamental-statistical-concepts"><i class="fa fa-check"></i><b>1.6</b> Fundamental statistical concepts</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#sample-and-population"><i class="fa fa-check"></i><b>1.6.1</b> Sample and population</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#models-and-risk"><i class="fa fa-check"></i><b>1.6.2</b> Models and risk</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#estimators-and-associated-variability"><i class="fa fa-check"></i><b>1.6.3</b> Estimators and associated variability</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#simulating-the-population-using-resampling-techniques"><i class="fa fa-check"></i><b>1.6.4</b> Simulating the population using resampling techniques</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#model-selection"><i class="fa fa-check"></i><b>1.6.5</b> Model Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html"><i class="fa fa-check"></i><b>2</b> Assessing the validity of a model</a><ul>
<li class="chapter" data-level="2.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#cross-validation"><i class="fa fa-check"></i><b>2.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="2.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#covariance-penalties-criteria"><i class="fa fa-check"></i><b>2.3</b> Covariance Penalties Criteria</a><ul>
<li class="chapter" data-level="2.3.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-2"><i class="fa fa-check"></i><b>2.3.1</b> Introduction</a></li>
<li class="chapter" data-level="2.3.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mallows-c_p"><i class="fa fa-check"></i><b>2.3.2</b> Mallows <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="2.3.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#efrons-q-class"><i class="fa fa-check"></i><b>2.3.3</b> Efron’s <span class="math inline">\(q\)</span>-class</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#information-theory-and-bayesian-criteria"><i class="fa fa-check"></i><b>2.4</b> Information Theory and Bayesian Criteria</a><ul>
<li class="chapter" data-level="2.4.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#aic-akaike-information-criterion"><i class="fa fa-check"></i><b>2.4.1</b> AIC: Akaike Information Criterion</a></li>
<li class="chapter" data-level="2.4.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#bic-bayesian-information-criterion"><i class="fa fa-check"></i><b>2.4.2</b> BIC: Bayesian Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mean-squared-error-based-criteria"><i class="fa fa-check"></i><b>2.5</b> Mean Squared Error Based Criteria</a><ul>
<li class="chapter" data-level="2.5.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#steins-unbiased-risk-estimator-sure"><i class="fa fa-check"></i><b>2.5.1</b> Stein’s unbiased risk estimator (SURE)</a></li>
<li class="chapter" data-level="2.5.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-focused-information-criterion-fic"><i class="fa fa-check"></i><b>2.5.2</b> The Focused Information Criterion (FIC)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-measures"><i class="fa fa-check"></i><b>2.6</b> Classification measures</a><ul>
<li class="chapter" data-level="2.6.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-logistic-model"><i class="fa fa-check"></i><b>2.6.1</b> The logistic model</a></li>
<li class="chapter" data-level="2.6.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#prediction-error-measures-for-binary-classification"><i class="fa fa-check"></i><b>2.6.2</b> Prediction error measures for Binary classification</a></li>
<li class="chapter" data-level="2.6.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-error-estimation"><i class="fa fa-check"></i><b>2.6.3</b> Classification error estimation</a></li>
<li class="chapter" data-level="2.6.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-roc-curve"><i class="fa fa-check"></i><b>2.6.4</b> The ROC curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html"><i class="fa fa-check"></i><b>3</b> Ordering the variables</a><ul>
<li class="chapter" data-level="3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#stepwise-forward-regression"><i class="fa fa-check"></i><b>3.2</b> Stepwise forward regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#partial-correlations"><i class="fa fa-check"></i><b>3.2.1</b> Partial correlations</a></li>
<li class="chapter" data-level="3.2.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#selection-by-hypothesis-testing"><i class="fa fa-check"></i><b>3.2.2</b> Selection by hypothesis testing</a></li>
<li class="chapter" data-level="3.2.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#orthogonal-matching-pursuit"><i class="fa fa-check"></i><b>3.2.3</b> Orthogonal matching pursuit</a></li>
<li class="chapter" data-level="3.2.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#forward-stagewise-regression"><i class="fa fa-check"></i><b>3.2.4</b> Forward stagewise regression</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#streamwise-regression"><i class="fa fa-check"></i><b>3.3</b> Streamwise regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-4"><i class="fa fa-check"></i><b>3.3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#sure-independence-screening"><i class="fa fa-check"></i><b>3.3.2</b> Sure Independence Screening</a></li>
<li class="chapter" data-level="3.3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#pc-simple-algorithm"><i class="fa fa-check"></i><b>3.3.3</b> PC-simple algorithm</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-and-regression-tree-cart"><i class="fa fa-check"></i><b>3.4</b> Classification And Regression Tree (CART)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#regression-tree"><i class="fa fa-check"></i><b>3.4.1</b> Regression tree</a></li>
<li class="chapter" data-level="3.4.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-trees"><i class="fa fa-check"></i><b>3.4.2</b> Classification Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html"><i class="fa fa-check"></i><b>4</b> Shrinkage Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#ridge-regression"><i class="fa fa-check"></i><b>4.2</b> Ridge regression</a></li>
<li class="chapter" data-level="4.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-lasso-estimator"><i class="fa fa-check"></i><b>4.3</b> The lasso estimator</a></li>
<li class="chapter" data-level="4.4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#alternative-penalized-regression-methods"><i class="fa fa-check"></i><b>4.4</b> Alternative penalized regression methods</a><ul>
<li class="chapter" data-level="4.4.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-adaptive-and-relaxed-lasso"><i class="fa fa-check"></i><b>4.4.1</b> The adaptive and relaxed lasso</a></li>
<li class="chapter" data-level="4.4.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-elastic-net"><i class="fa fa-check"></i><b>4.4.2</b> The elastic net</a></li>
<li class="chapter" data-level="4.4.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-nonnegative-garotte"><i class="fa fa-check"></i><b>4.4.3</b> The nonnegative garotte</a></li>
<li class="chapter" data-level="4.4.4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#non-convex-penalties"><i class="fa fa-check"></i><b>4.4.4</b> Non convex penalties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html"><i class="fa fa-check"></i><b>5</b> Properties of model selection criteria</a><ul>
<li class="chapter" data-level="5.1" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#introduction-6"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#selection-consistency"><i class="fa fa-check"></i><b>5.2</b> Selection consistency</a></li>
<li class="chapter" data-level="5.3" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#selection-efficiency"><i class="fa fa-check"></i><b>5.3</b> Selection efficiency</a></li>
<li class="chapter" data-level="5.4" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#the-oracle-property"><i class="fa fa-check"></i><b>5.4</b> The oracle property</a></li>
<li class="chapter" data-level="5.5" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#probability-of-overfitting"><i class="fa fa-check"></i><b>5.5</b> Probability of overfitting</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="post-selection-inference.html"><a href="post-selection-inference.html"><i class="fa fa-check"></i><b>6</b> Post-Selection Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="post-selection-inference.html"><a href="post-selection-inference.html#introduction-7"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="post-selection-inference.html"><a href="post-selection-inference.html#inference-via-the-nonparametric-bootstrap"><i class="fa fa-check"></i><b>6.2</b> Inference via the nonparametric Bootstrap</a></li>
<li class="chapter" data-level="6.3" data-path="post-selection-inference.html"><a href="post-selection-inference.html#improving-on-the-bootstrap-smoothed-bootstrap-or-bagging"><i class="fa fa-check"></i><b>6.3</b> Improving on the Bootstrap: Smoothed Bootstrap or Bagging</a></li>
<li class="chapter" data-level="6.4" data-path="post-selection-inference.html"><a href="post-selection-inference.html#post-selection-significance-testing"><i class="fa fa-check"></i><b>6.4</b> Post selection significance testing</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i><b>7</b> Solutions</a><ul>
<li class="chapter" data-level="7.1" data-path="solutions.html"><a href="solutions.html#chapter-1"><i class="fa fa-check"></i><b>7.1</b> Chapter 1</a><ul>
<li class="chapter" data-level="7.1.1" data-path="solutions.html"><a href="solutions.html#zam"><i class="fa fa-check"></i><b>7.1.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="7.1.2" data-path="solutions.html"><a href="solutions.html#leuk"><i class="fa fa-check"></i><b>7.1.2</b> Prognostic Factors in Childhood Leukemia</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="solutions.html"><a href="solutions.html#chapter-2"><i class="fa fa-check"></i><b>7.2</b> Chapter 2</a><ul>
<li class="chapter" data-level="7.2.1" data-path="solutions.html"><a href="solutions.html#cv"><i class="fa fa-check"></i><b>7.2.1</b> Cross-validation</a></li>
<li class="chapter" data-level="7.2.2" data-path="solutions.html"><a href="solutions.html#aic"><i class="fa fa-check"></i><b>7.2.2</b> Akaike Information Criterion</a></li>
<li class="chapter" data-level="7.2.3" data-path="solutions.html"><a href="solutions.html#roc"><i class="fa fa-check"></i><b>7.2.3</b> ROC curves</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="solutions.html"><a href="solutions.html#chapter-3"><i class="fa fa-check"></i><b>7.3</b> Chapter 3</a><ul>
<li class="chapter" data-level="7.3.1" data-path="solutions.html"><a href="solutions.html#HT"><i class="fa fa-check"></i><b>7.3.1</b> Selection by Hypothesis Testing</a></li>
<li class="chapter" data-level="7.3.2" data-path="solutions.html"><a href="solutions.html#SIS"><i class="fa fa-check"></i><b>7.3.2</b> Sure Independence Screening</a></li>
<li class="chapter" data-level="7.3.3" data-path="solutions.html"><a href="solutions.html#PC"><i class="fa fa-check"></i><b>7.3.3</b> PC-simple algorithm</a></li>
<li class="chapter" data-level="7.3.4" data-path="solutions.html"><a href="solutions.html#RT"><i class="fa fa-check"></i><b>7.3.4</b> Regression Tree</a></li>
<li class="chapter" data-level="7.3.5" data-path="solutions.html"><a href="solutions.html#CT"><i class="fa fa-check"></i><b>7.3.5</b> Classification Tree</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="solutions.html"><a href="solutions.html#chapter-4"><i class="fa fa-check"></i><b>7.4</b> Chapter 4</a><ul>
<li class="chapter" data-level="7.4.1" data-path="solutions.html"><a href="solutions.html#Ridge"><i class="fa fa-check"></i><b>7.4.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="solutions.html"><a href="solutions.html#lasso"><i class="fa fa-check"></i><b>7.4.2</b> Lasso</a></li>
<li class="chapter" data-level="7.4.3" data-path="solutions.html"><a href="solutions.html#nonconvex"><i class="fa fa-check"></i><b>7.4.3</b> Non Convex Penalties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Model Selection in High Dimensions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="post-selection-inference" class="section level1">
<h1><span class="header-section-number">6</span> Post-Selection Inference</h1>
<div id="introduction-7" class="section level2">
<h2><span class="header-section-number">6.1</span> Introduction</h2>
<p>When model selection is performed with the intention to use the selected model for inference (not only prediction), i.e. for a better understanding of the phenomenon under study, one needs inferential methods (e.g. building confidence intervals, testing) that take account the variability induced by data-based model selection methods. In other words, by ignoring the variability introduced by the model selection step, one introduces biases when computing for example confidence intervals for slope coefficients in the linear regression model. The bias always lowers the risk measure (variance) estimator, resulting in smaller confidence intervals and possibly also not centered around the true values. This can have important impacts when for example evaluating the strength of a drug (dosage) on an health-related outcome.</p>
As an illustrative example, consider the data set <em>cholesterol</em> analysed in <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span> which concerns the response of 164 men to a treatment (over a period of 7 years) for lowering cholesterol levels in the body. The single predictor is a (standardized) fraction of intended dose (of the cholesterol lowering drug <em>cholestyramine</em>) actually taken (compliance). The response is the decrease in cholesterol level over the course of the experiment. A potential suitable model (or family of models) for the relationship between the decrease of the cholesterol level and the amount of cholestryramine (compliance) is a polynomial regression model
<span class="math display">\[\begin{equation}
\boldsymbol{Y}=\sum_{j=0}^J\beta_jX^j+\boldsymbol{\varepsilon}=\beta_0+ \beta_1X+\beta_2X^2+\ldots +\beta_JX^J+\boldsymbol{\varepsilon}
\end{equation}\]</span>
<p>A model selection method is typically applied here to select <span class="math inline">\(J\)</span>. For the cholesterol data, using the <span class="math inline">\(C_p\)</span> as model selection criterion (note that the sequence of ordered models is given), provides <span class="math inline">\(J=3\)</span> as the <em>best</em> model.</p>
<blockquote>
<p>Polynomial fit (<span class="math inline">\(J=3\)</span>) for the cholesterol data <img src="Figures/Rplot_cholesterol_20_1.png" alt="Top" /></p>
</blockquote>
<p>With this example, one might be interested in building a confidence interval for the response to treatment, at a given treatment level <span class="math inline">\(X\)</span>. Letting <span class="math inline">\(\mu=\sum_{j=0}^J\beta_jX^j\)</span>, one actually seeks a confidence interval <span class="math inline">\(CI_{1-\alpha}\left(\mu\vert \hat{\boldsymbol{\beta}},\hat{J}\right)\)</span> constructed from the sample via <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> and <span class="math inline">\(\hat{J}\)</span>. Ignoring the randomness of <span class="math inline">\(\hat{J}\)</span> (i.e. by fixing <span class="math inline">\(J=\hat{J}\)</span>) and performing standard inference using <span class="math inline">\(\hat{\mu}=\sum_{j=0}^J\hat{\beta}_j\mathbf{x}^j\)</span> will provide misleading confidence intervals.</p>
</div>
<div id="inference-via-the-nonparametric-bootstrap" class="section level2">
<h2><span class="header-section-number">6.2</span> Inference via the nonparametric Bootstrap</h2>
<p>One way to infer (computing confidence intervals) is by reproducing simultaneously the selection and estimation mechanism by means of the nonparametric bootstrap: <span class="math inline">\(B\)</span> samples of size <span class="math inline">\(n\)</span>, <span class="math inline">\(\left(y_i^b,x_i^b\right), i=1,\ldots,n\)</span>, are created by drawing with replacement from the original sample. On each of the <span class="math inline">\(B\)</span> samples, selection (using e.g. the <span class="math inline">\(C_p\)</span>) is performed to obtain <span class="math inline">\(\hat{J}^b\)</span>, estimates <span class="math inline">\(\hat{\beta}_j^b\)</span> used in computing <span class="math inline">\(\hat{\mu}^b=\sum_{j=0}^{\hat{J}^b}\hat{\beta}_j^b\mathbf{x}^j\)</span>. The Figure below provides the bootstrap distribution of <span class="math inline">\(\hat{\mu}^b\)</span>, for a compliance <span class="math inline">\(x=-2\)</span>, and for <span class="math inline">\(\widetilde{\mu}^b=\sum_{j=0}^{J}\hat{\beta}_j^b\mathbf{x}^j\)</span> with <span class="math inline">\(J=3\)</span>, the value found in the original sample.</p>
<blockquote>
<p>Bootstrap distribution of <span class="math inline">\(\tilde{\mu}^b\)</span> (fixed) and <span class="math inline">\(\hat{\mu}^b\)</span> (adaptive) on the cholesterol data <img src="Figures/Rplot_cholesterol_20_4.png" alt="Top" /></p>
</blockquote>
<p>As expected, the length of the confidence intervals are underestimated when one ignores the randomness introduced by the selection procedure (i.e. the randomness of <span class="math inline">\(\hat{J}\)</span>). Actually, the confidence interval for <span class="math inline">\(\mu\)</span> at <span class="math inline">\(x=-2\)</span> (estimated via the nonparametric bootstrap) ignoring the randomness of the selection procedure is <span class="math inline">\(CI_{1-\alpha}\left(\mu\vert J=3\right)=1.05 \pm 8.09\)</span> while not ignoring the randomness of the model selection procedure it is <span class="math inline">\(CI_{1-\alpha}\left(\mu\vert \hat{J}=3\right)=1.40 \pm 16.17\)</span>.</p>
<p>What actually happens with (hard thresholding) model selection criteria such as the <span class="math inline">\(C_p\)</span> is that it is a discontinuous process, creating <em>jumps</em> in the estimates. In the cholesterol data example, with the nonparametric bootstrap, for about <span class="math inline">\(20\)</span>% of the bootstrap samples, the <span class="math inline">\(C_p\)</span> selected <span class="math inline">\(\hat{J}=1\)</span>, i.e. the linear model. In these cases, <span class="math inline">\(\hat{\mu}^b\vert \hat{J}=1\)</span> are smaller. This is illustrated in the Figure below.</p>
<blockquote>
<p>Bootstrap distribution of <span class="math inline">\(\tilde{\mu}^b\)</span> (Fixed: <span class="math inline">\(J=3\)</span>), <span class="math inline">\(\hat{\mu}^b\vert \hat{J}=1\)</span> (Adaptive: <span class="math inline">\(J=1\)</span>) and <span class="math inline">\(\hat{\mu}^b\vert \hat{J}&gt;1\)</span> (Adaptive: <span class="math inline">\(J&gt;1\)</span>) on the cholesterol data <img src="Figures/Rplot_cholesterol_20_6.png" alt="Top" /></p>
</blockquote>
<p>The distribution of the <span class="math inline">\(\hat{\mu}^b\)</span> when <span class="math inline">\(\hat{J}=1\)</span> is clearly different (different location) that the distribution of the <span class="math inline">\(\hat{\mu}^b\)</span> when <span class="math inline">\(\hat{J}&gt;1\)</span>. <span class="math inline">\(\hat{J}=1\)</span> happens in about 20% of the bootstrap samples, which creates, from one sample to the other, important <em>jumps</em>.</p>
</div>
<div id="improving-on-the-bootstrap-smoothed-bootstrap-or-bagging" class="section level2">
<h2><span class="header-section-number">6.3</span> Improving on the Bootstrap: Smoothed Bootstrap or Bagging</h2>
<em>Bagging</em> stands for bootstrap averaging and leads to “improvements for unstable procedures” <span class="citation">(Breiman <a href="#ref-Brei:96">1996</a>)</span>, which include subset selection in linear regression <span class="citation">(Breiman <a href="#ref-Brei:94">1994</a>)</span>. Bagging is also known as bootstrap smoothing <span class="citation">(Efron and Tibshirani <a href="#ref-EfTi:96">1996</a>)</span>. Given bootstrap replicates of a bootstrapped statistic <span class="math inline">\(T^b, b=1,\ldots,B\)</span>, the bootstrap standard errors are computed using the bootstrap distribution as
<span class="math display">\[\begin{equation}
\widehat{\mbox{sd}}_B=\sqrt{\frac{1}{B-1}\sum_{b=1}^B\left(T^b-\overline{T}_B\right)^2}
\end{equation}\]</span>
with
<span class="math display">\[\begin{equation}
\overline{T}_B = \frac{1}{B}\sum_{b=1}^BT^b
\end{equation}\]</span>
<p>Since after a bootstrap sampling, <span class="math inline">\(\overline{T}_B\)</span> is available, the bagging method proposes to compute the standard errors associated to <span class="math inline">\(\overline{T}_B\)</span>, which are lower. A brute force method would bootstrap several values for <span class="math inline">\(\overline{T}_B\)</span> which ends up in large computational times and is prohibitive in high dimensions. Actually, there is no need to resample further.</p>
Let <span class="math inline">\(N_{bi}\)</span> denote the number of times observation <span class="math inline">\(i\)</span> occurs in the bootstrap sample <span class="math inline">\(b\)</span>. The vector <span class="math inline">\(\mathbf{N}_b=\left(N_{1b},\ldots,N_{nb}\right)\)</span> has a multinomial distribution with <span class="math inline">\(n\)</span> draws on <span class="math inline">\(n\)</span> categories each with associated probability <span class="math inline">\(1/n\)</span>, and has mean vector and covariance matrix
<span class="math display">\[\begin{equation}
\mathbf{N}_b \sim \mbox{Mult}\left(\boldsymbol{1}_n, \mathbf{I}_n-\frac{1}{n}\boldsymbol{1}_n\boldsymbol{1}_n\right)
\end{equation}\]</span>
with <span class="math inline">\(\boldsymbol{1}_n\)</span> the <span class="math inline">\(n\times 1\)</span> vector ones. Using this setting, <span class="citation">Efron (<a href="#ref-Efro:14">2014</a>)</span> shows that the <em>infinitesimal jackknife estimate</em> of the standard deviation for <span class="math inline">\(\overline{T}_B\)</span> is
<span class="math display">\[\begin{equation}
\sqrt{\sum_{i=1}^n\mbox{cov}^2\left(N_{bi},T^b\right)}
\end{equation}\]</span>
A suitable estimator is given by
<span class="math display">\[\begin{equation}
\widetilde{\mbox{sd}}_B=\sqrt{\sum_{i=1}^n\widehat{\mbox{cov}}^2\left(N_{bi},T^b\right)}
\end{equation}\]</span>
with
<span class="math display">\[\begin{equation}
\widehat{\mbox{cov}}^2\left(N_{bi},T^b\right)=\frac{1}{B}\sum_{b=1}^B\left(N_{bi}-N_{\cdot i}\right)\left(T^b-\overline{T}_B\right)
\end{equation}\]</span>
and <span class="math inline">\(N_{\cdot i}=(1/B)\sum_{b=1}^BN_{bi}\)</span>. Moreover, it is always true that
<span class="math display">\[\begin{equation}
\frac{\widetilde{\mbox{sd}}_B}{\widehat{\mbox{sd}}_B}\leq 1
\end{equation}\]</span>
<p>Bagging with the infinitesimal jackknife allows to be more precise than with the bootstrap without the need to simulate (bootstrap) more replicates, which makes it a computationally convenient inferential method. For the cholesterol data example, the standard deviations (across values for <span class="math inline">\(x\)</span>) decrease of about 12%. In general, the savings due to bagging increase with the nonlinearity of <span class="math inline">\(T^b\)</span> and is therefore a suitable approach for inference after selection. <span class="citation">Wager, Hastie, and Efron (<a href="#ref-WaHaEf:14">2014</a>)</span> have used this approach to derive confidence intervals for <em>Random Forests</em>. Bagging is actually a form of <em>model averaging</em>.</p>
</div>
<div id="post-selection-significance-testing" class="section level2">
<h2><span class="header-section-number">6.4</span> Post selection significance testing</h2>
<p>Very recently, test statistics and their associated distribution have been proposed in the linear regression case, to test hypothesis about (linear combinations of) slope parameters, that take into account the randomness of the selection part of the analysis. It is actually possible to formalize the conditional distribution of (a linear combination of) the slope estimators (typically the LS), conditionally on the distribution of the outcome of the selection method. For the later, an important result is provided by the <em>polyhedral lemma</em> which allows to formalize the model selection selection operation in a more tractable way. References include <span class="citation">Lee et al. (<a href="#ref-LeSuSuTa:16">2016</a>)</span>, <span class="citation">Tibshirani et al. (<a href="#ref-TiTaLoTi:16">2016</a>)</span>, <span class="citation">Hastie, Tibshirani, and Wainwright (<a href="#ref-HaTiWa:16">2016</a>)</span>, Section 6.3, and <span class="citation">Tian and Taylor (<a href="#ref-TiTa:18">2018</a>)</span>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-EfHa:16">
<p>Efron, B., and T. Hastie. 2016. <em>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</em>. Cambridge University Press.</p>
</div>
<div id="ref-Brei:96">
<p>Breiman, L. 1996. “Bagging Predictors.” <em>Machine Learning</em> 24: 123–40.</p>
</div>
<div id="ref-Brei:94">
<p>Breiman, L. 1994. “Bagging Predictors.” Technical Report 421. Department of Statistics, University of California, Berkeley.</p>
</div>
<div id="ref-EfTi:96">
<p>Efron, B., and R. Tibshirani. 1996. “Using Specially Designed Exponential Families for Density Estimation.” <em>Annals of Statististics</em> 24: 2431–61.</p>
</div>
<div id="ref-Efro:14">
<p>Efron, B. 2014. “Estimation and Accuracy After Model Selection.” <em>Journal of the American Statistical Association</em> 109: 991–1007.</p>
</div>
<div id="ref-WaHaEf:14">
<p>Wager, S., T. Hastie, and B. Efron. 2014. “Confidence Intervals for Random Forests: The Jackknife and the Infinitesimal Jackknife.” <em>Journal of Machine Learning Research</em> 15: 1625–51.</p>
</div>
<div id="ref-LeSuSuTa:16">
<p>Lee, J. D., D. L. Sun, Y. Sun, and J. E. Taylor. 2016. “Exact Post-Selection Inference, with Application to the Lasso.” <em>The Annals of Statistics</em> 44: 907–27.</p>
</div>
<div id="ref-TiTaLoTi:16">
<p>Tibshirani, R. J., J. E. Taylor, R. Lockhart, and R. Tibshirani. 2016. “Exact Post-Selection Inference for Sequential Regression Procedures.” <em>Journal of the American Statistical Association</em> 111: 600–620.</p>
</div>
<div id="ref-HaTiWa:16">
<p>Hastie, T., R. Tibshirani, and M. Wainwright. 2016. <em>Statistical Learning with Sparsity: The Lasso and Generalizations</em>. Monographs on Statistics &amp; Applied Probability 143. Chapman &amp; Hall.</p>
</div>
<div id="ref-TiTa:18">
<p>Tian, X., and J. E. Taylor. 2018. “Selective Inference with a Randomized Response.” <em>Annals of Statististics</em> 46: 679–710.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="properties-of-model-selection-criteria.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="solutions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
