<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Model Selection in High Dimensions</title>
  <meta name="description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Model Selection in High Dimensions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Model Selection in High Dimensions" />
  
  <meta name="twitter:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  

<meta name="author" content="Maria-Pia Victoria-Feser (professor), Cesare Miglioli and Guillaume Blanc (teaching assistants)">


<meta name="date" content="2018-04-26">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="index.html">
<link rel="next" href="ordering-the-variables.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#read-this-part-first"><i class="fa fa-check"></i><b>1.1</b> Read this part first</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#content-choice-and-structure"><i class="fa fa-check"></i><b>1.2</b> Content choice and structure</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#bibliography"><i class="fa fa-check"></i><b>1.2.1</b> Bibliography</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#useful-links"><i class="fa fa-check"></i><b>1.2.2</b> Useful links</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#using-r"><i class="fa fa-check"></i><b>1.3</b> Using R</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#useful-r-packages"><i class="fa fa-check"></i><b>1.3.1</b> Useful R packages</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#managing-data"><i class="fa fa-check"></i><b>1.3.2</b> Managing Data</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#loading-data-from-an-r-package"><i class="fa fa-check"></i><b>1.3.3</b> Loading data from an R package</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#loading-data-from-a-local-file"><i class="fa fa-check"></i><b>1.3.4</b> Loading data from a local file</a></li>
<li class="chapter" data-level="1.3.5" data-path="index.html"><a href="index.html#loading-data-from-an-online-file"><i class="fa fa-check"></i><b>1.3.5</b> Loading data from an online file</a></li>
<li class="chapter" data-level="1.3.6" data-path="index.html"><a href="index.html#loading-data-from-an-online-database-using-a-mysql-query-optional"><i class="fa fa-check"></i><b>1.3.6</b> Loading data from an online database using a mySQL query (Optional)</a></li>
<li class="chapter" data-level="1.3.7" data-path="index.html"><a href="index.html#data-wrangling"><i class="fa fa-check"></i><b>1.3.7</b> Data Wrangling</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#writing-reports"><i class="fa fa-check"></i><b>1.4</b> Writing reports</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#r-markdown"><i class="fa fa-check"></i><b>1.4.1</b> R Markdown</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4.2</b> GitHub</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.5</b> Examples</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#data-on-malnutrition-in-zambia"><i class="fa fa-check"></i><b>1.5.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#prognostic-factors-in-childhood-leukemia"><i class="fa fa-check"></i><b>1.5.2</b> Prognostic Factors in Childhood Leukemia</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#r-package-quantmod"><i class="fa fa-check"></i><b>1.5.3</b> R package quantmod</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#fundamental-statistical-concepts"><i class="fa fa-check"></i><b>1.6</b> Fundamental statistical concepts</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#sample-and-population"><i class="fa fa-check"></i><b>1.6.1</b> Sample and population</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#models-and-risk"><i class="fa fa-check"></i><b>1.6.2</b> Models and risk</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#estimators-and-associated-variability"><i class="fa fa-check"></i><b>1.6.3</b> Estimators and associated variability</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#simulating-the-population-using-resampling-techniques"><i class="fa fa-check"></i><b>1.6.4</b> Simulating the population using resampling techniques</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#model-selection"><i class="fa fa-check"></i><b>1.6.5</b> Model Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html"><i class="fa fa-check"></i><b>2</b> Assessing the validity of a model</a><ul>
<li class="chapter" data-level="2.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#cross-validation"><i class="fa fa-check"></i><b>2.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="2.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#covariance-penalties-criteria"><i class="fa fa-check"></i><b>2.3</b> Covariance Penalties Criteria</a><ul>
<li class="chapter" data-level="2.3.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-2"><i class="fa fa-check"></i><b>2.3.1</b> Introduction</a></li>
<li class="chapter" data-level="2.3.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mallows-c_p"><i class="fa fa-check"></i><b>2.3.2</b> Mallows <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="2.3.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#efrons-q-class"><i class="fa fa-check"></i><b>2.3.3</b> Efron’s <span class="math inline">\(q\)</span>-class</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#information-theory-and-bayesian-criteria"><i class="fa fa-check"></i><b>2.4</b> Information Theory and Bayesian Criteria</a><ul>
<li class="chapter" data-level="2.4.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#aic-akaike-information-criterion"><i class="fa fa-check"></i><b>2.4.1</b> AIC: Akaike Information Criterion</a></li>
<li class="chapter" data-level="2.4.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#bic-bayesian-information-criterion"><i class="fa fa-check"></i><b>2.4.2</b> BIC: Bayesian Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mean-squared-error-based-criteria"><i class="fa fa-check"></i><b>2.5</b> Mean Squared Error Based Criteria</a><ul>
<li class="chapter" data-level="2.5.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#steins-unbiased-risk-estimator-sure"><i class="fa fa-check"></i><b>2.5.1</b> Stein’s unbiased risk estimator (SURE)</a></li>
<li class="chapter" data-level="2.5.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-focused-information-criterion-fic"><i class="fa fa-check"></i><b>2.5.2</b> The Focused Information Criterion (FIC)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-measures"><i class="fa fa-check"></i><b>2.6</b> Classification measures</a><ul>
<li class="chapter" data-level="2.6.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-logistic-model"><i class="fa fa-check"></i><b>2.6.1</b> The logistic model</a></li>
<li class="chapter" data-level="2.6.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#prediction-error-measures-for-binary-classification"><i class="fa fa-check"></i><b>2.6.2</b> Prediction error measures for Binary classification</a></li>
<li class="chapter" data-level="2.6.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-error-estimation"><i class="fa fa-check"></i><b>2.6.3</b> Classification error estimation</a></li>
<li class="chapter" data-level="2.6.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-roc-curve"><i class="fa fa-check"></i><b>2.6.4</b> The ROC curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html"><i class="fa fa-check"></i><b>3</b> Ordering the variables</a><ul>
<li class="chapter" data-level="3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#stepwise-forward-regression"><i class="fa fa-check"></i><b>3.2</b> Stepwise forward regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#partial-correlations"><i class="fa fa-check"></i><b>3.2.1</b> Partial correlations</a></li>
<li class="chapter" data-level="3.2.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#selection-by-hypothesis-testing"><i class="fa fa-check"></i><b>3.2.2</b> Selection by hypothesis testing</a></li>
<li class="chapter" data-level="3.2.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#orthogonal-matching-pursuit"><i class="fa fa-check"></i><b>3.2.3</b> Orthogonal matching pursuit</a></li>
<li class="chapter" data-level="3.2.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#forward-stagewise-regression"><i class="fa fa-check"></i><b>3.2.4</b> Forward stagewise regression</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#streamwise-regression"><i class="fa fa-check"></i><b>3.3</b> Streamwise regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-4"><i class="fa fa-check"></i><b>3.3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#sure-independence-screening"><i class="fa fa-check"></i><b>3.3.2</b> Sure Independence Screening</a></li>
<li class="chapter" data-level="3.3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#pc-simple-algorithm"><i class="fa fa-check"></i><b>3.3.3</b> PC-simple algorithm</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-and-regression-tree-cart"><i class="fa fa-check"></i><b>3.4</b> Classification And Regression Tree (CART)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#regression-tree"><i class="fa fa-check"></i><b>3.4.1</b> Regression tree</a></li>
<li class="chapter" data-level="3.4.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-trees"><i class="fa fa-check"></i><b>3.4.2</b> Classification Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html"><i class="fa fa-check"></i><b>4</b> Shrinkage Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#ridge-regression"><i class="fa fa-check"></i><b>4.2</b> Ridge regression</a></li>
<li class="chapter" data-level="4.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-lasso-estimator"><i class="fa fa-check"></i><b>4.3</b> The lasso estimator</a></li>
<li class="chapter" data-level="4.4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#alternative-penalized-regression-methods"><i class="fa fa-check"></i><b>4.4</b> Alternative penalized regression methods</a><ul>
<li class="chapter" data-level="4.4.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-adaptive-and-relaxed-lasso"><i class="fa fa-check"></i><b>4.4.1</b> The adaptive and relaxed lasso</a></li>
<li class="chapter" data-level="4.4.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-elastic-net"><i class="fa fa-check"></i><b>4.4.2</b> The elastic net</a></li>
<li class="chapter" data-level="4.4.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-nonnegative-garotte"><i class="fa fa-check"></i><b>4.4.3</b> The nonnegative garotte</a></li>
<li class="chapter" data-level="4.4.4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#non-convex-penalties"><i class="fa fa-check"></i><b>4.4.4</b> Non convex penalties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i><b>5</b> Solutions</a><ul>
<li class="chapter" data-level="5.1" data-path="solutions.html"><a href="solutions.html#chapter-1"><i class="fa fa-check"></i><b>5.1</b> Chapter 1</a><ul>
<li class="chapter" data-level="5.1.1" data-path="solutions.html"><a href="solutions.html#zam"><i class="fa fa-check"></i><b>5.1.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="5.1.2" data-path="solutions.html"><a href="solutions.html#leuk"><i class="fa fa-check"></i><b>5.1.2</b> Prognostic Factors in Childhood Leukemia</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="solutions.html"><a href="solutions.html#chapter-2"><i class="fa fa-check"></i><b>5.2</b> Chapter 2</a><ul>
<li class="chapter" data-level="5.2.1" data-path="solutions.html"><a href="solutions.html#cv"><i class="fa fa-check"></i><b>5.2.1</b> Cross-validation</a></li>
<li class="chapter" data-level="5.2.2" data-path="solutions.html"><a href="solutions.html#aic"><i class="fa fa-check"></i><b>5.2.2</b> Akaike Information Criterion</a></li>
<li class="chapter" data-level="5.2.3" data-path="solutions.html"><a href="solutions.html#roc"><i class="fa fa-check"></i><b>5.2.3</b> ROC curves</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="solutions.html"><a href="solutions.html#chapter-3"><i class="fa fa-check"></i><b>5.3</b> Chapter 3</a><ul>
<li class="chapter" data-level="5.3.1" data-path="solutions.html"><a href="solutions.html#HT"><i class="fa fa-check"></i><b>5.3.1</b> Selection by Hypothesis Testing</a></li>
<li class="chapter" data-level="5.3.2" data-path="solutions.html"><a href="solutions.html#SIS"><i class="fa fa-check"></i><b>5.3.2</b> Sure Independence Screening</a></li>
<li class="chapter" data-level="5.3.3" data-path="solutions.html"><a href="solutions.html#PC"><i class="fa fa-check"></i><b>5.3.3</b> PC-simple algorithm</a></li>
<li class="chapter" data-level="5.3.4" data-path="solutions.html"><a href="solutions.html#RT"><i class="fa fa-check"></i><b>5.3.4</b> Regression Tree</a></li>
<li class="chapter" data-level="5.3.5" data-path="solutions.html"><a href="solutions.html#CT"><i class="fa fa-check"></i><b>5.3.5</b> Classification Tree</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="solutions.html"><a href="solutions.html#chapter-4"><i class="fa fa-check"></i><b>5.4</b> Chapter 4</a><ul>
<li class="chapter" data-level="5.4.1" data-path="solutions.html"><a href="solutions.html#Ridge"><i class="fa fa-check"></i><b>5.4.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="5.4.2" data-path="solutions.html"><a href="solutions.html#lasso"><i class="fa fa-check"></i><b>5.4.2</b> Lasso</a></li>
<li class="chapter" data-level="5.4.3" data-path="solutions.html"><a href="solutions.html#nonconvex"><i class="fa fa-check"></i><b>5.4.3</b> Non Convex Penalties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Model Selection in High Dimensions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="assessing-the-validity-of-a-model" class="section level1">
<h1><span class="header-section-number">2</span> Assessing the validity of a model</h1>
<div id="introduction-1" class="section level2">
<h2><span class="header-section-number">2.1</span> Introduction</h2>
<p>Broadly speaking, model selection is about choosing a model that best <em>fits</em> (on some sense) the available sample so that it can be used to understand the phenomenon under investigation. This also includes being able to predict future outcomes and to assess research hypotheses within the population.</p>
<p>Therefore, to reach these objectives, the available information should be used for two important tasks:</p>
<ul>
<li>building the model (learning phase)</li>
<li>assess its <em>out-of-sample validity</em></li>
</ul>
<p>The two tasks are interdependent since building the model, i.e. finding the most suitable one, is made by optimizing, in some sense, its out-of-sample validity.</p>
<p>Out-of-sample validity is a crucial concept in model selection. Indeed, it allows to assess how the results of a statistical analysis (e.g. the selection of a model) will generalize to other outcomes, equivalently, to the population. Without out-of-sample validation, a selection procedure will necessarily tend to choose models that <em>overfit</em> the data, since they would be the best within the sample. In other words, when overfitting, a model describes random error or noise instead of the underlying relationship, leading to an excessively complex model (too many parameters relative to the number of observations) and consequently a poor predictive performance and a wrong representation of what happens in the population.</p>
<blockquote>
Example of overfitting situation<br />

</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Taken from https://en.wikipedia.org/wiki/Overfitting.</span>
<span class="co"># Noisy (roughly linear) data is fitted to a linear function and a polynomial function. </span>
<span class="co"># Although the polynomial function is a perfect fit, the linear function can be expected </span>
<span class="co"># to generalize better: if the two functions were used to extrapolate beyond the fit data, </span>
<span class="co"># the linear function would make better predictions. This is because the polynomial</span>
<span class="co"># function it is too dependent on the data which contains sampling error. </span>
<span class="co">#</span>
knitr<span class="op">::</span><span class="kw">include_graphics</span>(<span class="st">&quot;Figures/OverfitEx1.png&quot;</span>)</code></pre></div>
<p><img src="Figures/OverfitEx1.png" style="display: block; margin: auto;" /></p>
<p>More formally, consider a random variable <span class="math inline">\(Y\)</span> distributed according to model <span class="math inline">\(F_{\boldsymbol{\theta}}\)</span>, possibly conditionally on a set of fixed covariates <span class="math inline">\(\mathbf{X}=[x_1 \ldots \, x_p]\)</span>. We observe a random sample <span class="math inline">\(\mathbf{y} = (Y_i)_{i = 1, \ldots , n}\)</span> supposedly generated from <span class="math inline">\(F_{\boldsymbol{\theta}}\)</span> together with a non-random <span class="math inline">\(n\times p\)</span> full rank matrix of predictors <span class="math inline">\(\mathbf{X}\)</span>. Define the prediction function <span class="math inline">\(\hat{\mathbf{y}}\)</span> that depends on the considered model, for example <span class="math inline">\(\hat{\mathbf{y}}=\hat{\boldsymbol{\beta}}\mathbf{x}\)</span> for the linear regression model. The inferential task is to assess the accuracy of the prediction function, so that it can be compared to alternative prediction functions.</p>
Quantifying the prediction error of a prediction function requires specification of a discrepancy <span class="math inline">\(D\)</span> between a prediction <span class="math inline">\(\hat{\mathbf{y}}\)</span> and the actual response <span class="math inline">\(\mathbf{y}\)</span>. A natural choice is the mean squared error (mean residual squared error)
<span class="math display">\[\begin{equation}  
D\left(\hat{\mathbf{y}},\mathbf{y}\right)=\frac{1}{n}\sum_{i=1}^nd\left(\hat{y}_i,y_i\right)=\frac{1}{n}\sum_{i=1}^n\left(\hat{y}_i-y_i\right)^2
\end{equation}\]</span>
However, in order to avoid overfitting situations for out-of-sample validity, what is actually sought is the <em>true prediction error</em>, i.e. the (mathematically) expected discrepancy between prediction function out-of-sample <span class="math inline">\(\hat{y}_0\)</span> and the corresponding out-of-sample realization <span class="math inline">\(y_0\)</span>, namely
<span class="math display">\[\begin{equation}  
\text{Err}=\mathbb{E}_{0}\left[d\left(\hat{Y}_0,Y_0\right)\right]
\end{equation}\]</span>
<p>where <span class="math inline">\(\mathbb{E}_{0}\)</span> denotes the expectation at the <em>out-of-sample</em> distribution. This quantity needs to be estimated in some manner with the sample at hand, hence the real challenge in model selection is<br />
- the specification of an out-of-sample validity measure, called a model selection criterion<br />
- estimators for model selection criteria<br />
- performance measures to compare different model selection criteria<br />
</p>
<p>In the following sections we present several methods for model selection and measures for out-of-sample validity.</p>
<blockquote>
<p>Exercice (optional):<br />
- Consider the Malnutrition in Zambia dataset (without interections). As model fit assesment consider a) the residual variance and b) the R^2. Starting from the full model (without interactions), increase progressively model size by including polynomials (on the continuous variables) and observe the behavior of the model fit criteria (using e.g. a plot).  </p>
</blockquote>
<blockquote>
Additional material:<br />
Controlling for out-of-sample validity follows the lines of the scientific approach, in particular Occam’s razor problem-solving principle (<a href="https://en.wikipedia.org/wiki/Occam&#39;s_razor" class="uri">https://en.wikipedia.org/wiki/Occam's_razor</a>).<br />
One can intuitively understand overfitting from the fact that information from all past experience can be divided into two groups: information that is relevant for the future <em>(“signal”)</em> and irrelevant information <em>(“noise”)</em>.<br />
Everything else being equal, the more difficult a criterion is to predict (i.e., the higher its uncertainty), the more noise existing in past information needs to be ignored. The problem is to determine which part to ignore. See <a href="https://en.wikipedia.org/wiki/Overfitting" class="uri">https://en.wikipedia.org/wiki/Overfitting</a>.<br />

</blockquote>
</div>
<div id="cross-validation" class="section level2">
<h2><span class="header-section-number">2.2</span> Cross-Validation</h2>
The challenge is now to find a suitable estimator for <span class="math inline">\(\text{Err}\)</span> with the help of the sample and the (assumed) model <span class="math inline">\(F_{\boldsymbol{\theta}}\)</span>. A first guess is the apparent error
<span class="math display">\[\begin{equation}  
\text{err}= \frac{1}{n}\sum_{i=1}^nd\left(\hat{y}_i,y_i\right)
\end{equation}\]</span>
<p>Unfortunately err usually underestimates Err since <span class="math inline">\(\hat{y}_i\)</span> has been adjusted to fit the observed responses <span class="math inline">\(y_i\)</span> (<em>trainig set</em>).</p>
Ideally, one should have an independent <em>validation set</em> (or <em>test set</em>) of say <span class="math inline">\(n^{\star}\)</span> additional observations <span class="math inline">\(y_{0i}, i=1,\ldots,n^{\star}\)</span>, so that one could estimate Err using
<span class="math display">\[\begin{equation}  
\widehat{\text{Err}}_{\text{val}}= \frac{1}{n^{\star}}\sum_{i=1}^{n^{\star}}d\left(\hat{y}_{0i},y_{0i}\right)
\end{equation}\]</span>
Cross-validation attempts to mimic <span class="math inline">\(\widehat{\text{Err}}_{\text{val}}\)</span> without the need for a validation set. Define <span class="math inline">\(\hat{y}_{(i)}\)</span> to be the predicted value computed on the reduced training set in which the <span class="math inline">\(i\)</span>th observation has been omitted. The the <em>leave one out</em> cross-validation estimate of prediction error is
<span class="math display">\[\begin{equation}  
\widehat{\text{Err}}_{\text{cv1}}= \frac{1}{n}\sum_{i=1}^{n}d\left(\hat{y}_{(i)},y_{i}\right)
\end{equation}\]</span>
<p>A more common practice is to leave out several observations at a time: the sample is randomly partitioned into <span class="math inline">\(J\)</span> groups of size about <span class="math inline">\(\lfloor n/J\rfloor=n_J\)</span> each, then for each <span class="math inline">\(j=1,\ldots,J\)</span> groups, the training set is the sample without the <span class="math inline">\(j\)</span>th group on which the prediction <span class="math inline">\(\hat{y}_{(j)}\)</span> is computed and then compared to the observations in the <span class="math inline">\(j\)</span>th group <span class="math inline">\(y_j\)</span> using the chosen discrepancy measure <span class="math inline">\(D\)</span>. A common choice is <span class="math inline">\(J=10\)</span> leading to the <em>ten-fold cross-validation</em> procedure. Reducing from <span class="math inline">\(n\)</span> to <span class="math inline">\(J\)</span> the number of training and validation sets reduces the necessary number of prediction rules constructions (estimation). The optimal value for <span class="math inline">\(J\)</span> is however not known.</p>
<p><br></p>
<blockquote>
<p>Exercise: (solutions (<a href="solutions.html#cv">5.2.1</a>) provided by <strong>Alexander Maslev</strong>, <strong>Hanxiong Wang</strong> and <strong>Minyoung Lee</strong>). </p>
</blockquote>
<blockquote>
Program k-fold Cross-Validation (with k=2) and do model selection in a specific simulation setting with an exhaustive search. Follow these steps:<br />

</blockquote>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Generate from a MVN (multivariate normal) a matrix <span class="math inline">\(\mathbf{X_{n*p}}\)</span> with <span class="math inline">\(n = 1000\)</span> and <span class="math inline">\(p = 5\)</span>. You can choose the location vector as you wish but set the scale matrix as the identity.<br />
</li>
<li>Choose the generating vector <span class="math inline">\(\boldsymbol{\beta }= [3 \; 1.5 \; 0 \; 2 \; 0]\)</span> and retrieve the signal to noise ratio of this setting.<br />
</li>
<li>Generate <span class="math inline">\(\hat{\mathbf{y}}\)</span> thanks to the relation <span class="math inline">\(\mathbf{y} = \mathbf{X_{n*p}} \; \boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span> where <span class="math inline">\(\epsilon_{i}\)</span> is a standard normal, <span class="math inline">\(n = 1000\)</span> and <span class="math inline">\(p = 5\)</span>. Suppose for simplicity that the errors are uncorrelated.<br />
</li>
<li>Split the data randomly in two halves (k=2) and use the training set to determine <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>. Then, compute the squared loss function as prediction error measure for each possible model. Observe which model is the best model.<br />
</li>
<li>Suppose now that we increase the size of <span class="math inline">\(\boldsymbol{\beta}\)</span> to 100 (i.e. <span class="math inline">\(p = 100\)</span> ). Calculate the number of possible models to evaluate together with an estimate of the time needed for an exhaustive search (<em>hint: use previous results</em>). Conclude on the feasibility of this task.</li>
</ol>
</blockquote>
<p><br></p>
</div>
<div id="covariance-penalties-criteria" class="section level2">
<h2><span class="header-section-number">2.3</span> Covariance Penalties Criteria</h2>
<div id="introduction-2" class="section level3">
<h3><span class="header-section-number">2.3.1</span> Introduction</h3>
Originally, the covariance penalty approach treats prediction error estimation in a regression framework, with the predictors <span class="math inline">\(\mathbf{X}\)</span> considered as fixed. Moreover, supposing for the moment that the discrepancy measure is the squared difference (or <span class="math inline">\(L_2\)</span> norm), the <em>true</em> prediction error (conditionally on <span class="math inline">\(\mathbf{x}_i\)</span>) is defined as
<span class="math display">\[\begin{equation}  
\text{Err}_i=\mathbb{E}_{0}\left[\left(\hat{Y}_i-Y_0\right)^2\right]
\end{equation}\]</span>
<p>The overall prediction error is <span class="math inline">\(\text{Err}=1/n\sum_{i=1}^n\text{Err}_i\)</span>.</p>
The question is how to estimate this quantity, given a sample and a data generating model? <span class="citation">(Efron <a href="#ref-efron2004estimation">2004</a>)</span> uses <span class="math inline">\(\mathbb{E}\left[\text{Err}_i\right]\)</span> (where <span class="math inline">\(\mathbb{E}\)</span> denotes the expectation at the <em>insample</em> distribution) and shows that
<span class="math display">\[\begin{equation}  
\mathbb{E}\left[\text{Err}_i\right]=
\mathbb{E}\left[\text{err}_i\right]+2\text{cov}\left(\hat{Y}_i;Y_i\right)
\end{equation}\]</span>
<p>with <span class="math inline">\(\text{err}_i= (Y_i-\hat{Y}_i)^2\)</span>, the apparent (or in-sample) error and <span class="math inline">\(\text{cov}\left(\hat{Y}_i;Y_i\right)=\mathbb{E}\left[(Y_i-\mathbb{E}[Y_i])(\hat{Y}_i-\mathbb{E}[Y_i])\right]\)</span>.</p>
<blockquote>
<p>Proof (see <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span>, p. 220): </p>
</blockquote>
<blockquote>
Let <span class="math inline">\(\delta_{1i}=Y_i-\mathbb{E}[Y_i]\)</span> and <span class="math inline">\(\delta_{2i}=\hat{Y}_i-\mathbb{E}[Y_i]\)</span>, we can write<br />

<span class="math display">\[\begin{eqnarray}
(Y_i-\hat{Y}_i)^2&amp;=&amp;(\delta_{1i}-\delta_{2i})^2=\delta^2_{1i}-2\delta_{1i}\delta_{2i}+\delta_{2i}^2 \\
&amp;=&amp;(Y_i-\mathbb{E}[Y_i])^2-2(Y_i-\mathbb{E}[Y_i])(\hat{Y}_i-\mathbb{E}[Y_i])+(\hat{Y}_i-\mathbb{E}[Y_i])^2
\end{eqnarray}\]</span>
We then have
<span class="math display">\[\begin{eqnarray}
\mathbb{E}\left[(Y_i-\hat{Y}_i)^2\right]&amp;=&amp;\mathbb{E}\left[(Y_i-\mathbb{E}[Y_i])^2\right]-2\mathbb{E}\left[(Y_i-\mathbb{E}[Y_i])(\hat{Y}_i-\mathbb{E}[Y_i])\right]+\mathbb{E}\left[(\hat{Y}_i-\mathbb{E}[Y_i])^2\right] \\
&amp;=&amp; \sigma^2 -2\mbox{cov}\left(Y_i,\hat{Y}_i\right)+\mathbb{E}\left[(\hat{Y}_i-\mathbb{E}[Y_i])^2\right]
\end{eqnarray}\]</span>
from which we deduce that <span class="math inline">\(\mathbb{E}\left[(\hat{Y}_i-\mathbb{E}[Y_i])^2\right]=\mathbb{E}[\text{err}_i]+2\mbox{cov}\left(Y_i,\hat{Y}_i\right)-\sigma^2\)</span>. Likewise
<span class="math display">\[\begin{eqnarray}
\mathbb{E}\left[(Y_{0i}-\hat{Y}_i)^2\right]&amp;=&amp;\mathbb{E}\left[(Y_{0i}-\mathbb{E}[Y_i])^2\right]-2\mathbb{E}\left[(Y_{0i}-\mathbb{E}[Y_i])(\hat{Y}_i-\mathbb{E}[Y_i])\right]+\mathbb{E}\left[(\hat{Y}_i-\mathbb{E}[Y_i])^2\right] \\
\end{eqnarray}\]</span>
and because <span class="math inline">\(\hat{Y}_i\)</span> and <span class="math inline">\(Y_{0i}\)</span> are independent, we get
<span class="math display">\[\begin{eqnarray}
\mathbb{E}\left[(Y_{0i}-\hat{Y}_i)^2\right]&amp;=&amp; \sigma^2 +\mathbb{E}\left[(\hat{Y}_i-\mathbb{E}[Y_i])^2\right]
\end{eqnarray}\]</span>
Note that <span class="math inline">\(\mathbb{E}\left[\mathbb{E}_{0}\left[(Y_{0i}-\hat{Y}_i)^2\right]\right]=\mathbb{E}_0\left[\mathbb{E}\left[(Y_{0i}-\hat{Y}_i)^2\right]\right]\)</span>, so that
<span class="math display">\[\begin{eqnarray}
\mathbb{E}\left[\text{Err}_i\right]&amp;=&amp;\sigma^2 +\mathbb{E}\left[(\hat{Y}_i-\mathbb{E}[Y_i])^2\right] \\
&amp;=&amp; \mathbb{E}[\text{err}_i]+2\mbox{cov}\left(Y_i,\hat{Y}_i\right)
\end{eqnarray}\]</span>
</blockquote>
This result says that, on average, the apparent error <span class="math inline">\(\text{err}_i\)</span> underestimates the true prediction error <span class="math inline">\(\text{Err}_i\)</span> by the covariance penalty <span class="math inline">\(2\text{cov}\left(\hat{Y}_i;Y_i\right)\)</span>. This makes intuitive sense since <span class="math inline">\(\text{cov}\left(\hat{Y}_i;Y_i\right)\)</span> measures the amount by which <span class="math inline">\(Y_i\)</span> influences its own prediction <span class="math inline">\(\hat{Y}_i\)</span>. An natural estimator for the overall prediction error is then given by
<span class="math display" id="eq:err">\[\begin{equation}  
\widehat{\text{Err}}=
\frac{1}{n}\sum_{i=1}^n\left(\hat{y}_i-y_i\right)^2+\frac{2}{n}\sum_{i=1}^n\widehat{\text{cov}}\left(\hat{Y}_i;Y_i\right) \tag{2.1}
\end{equation}\]</span>
<p>Depending on the assumed model, <span class="math inline">\(\widehat{\text{cov}}\left(\hat{Y}_i;Y_i\right)\)</span> is obtained analytically up to a value of <span class="math inline">\(\boldsymbol{\theta}\)</span>, the model’s parameters, which is then replaced by <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span> (plug-in method), or by resampling methods such as the parametric bootstrap. For the later, considering the model <span class="math inline">\(F_{\boldsymbol{\theta}}\)</span>, one uses the following steps:</p>
<ol style="list-style-type: decimal">
<li>Estimate <span class="math inline">\(\boldsymbol{\theta}\)</span> from <span class="math inline">\(F^{(n)}\)</span> to get <span class="math inline">\(\hat{\boldsymbol{\theta}}\)</span>.<br />
</li>
<li>Set the seed<br />
</li>
<li>For <span class="math inline">\(j=1,\ldots,B\)</span>, do<br />
</li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Simulate <span class="math inline">\(n\)</span> values <span class="math inline">\(y_i^{(j)}, i=1,\ldots,n\)</span> from <span class="math inline">\(F_{\hat{\boldsymbol{\theta}}}\)</span>,<br />
</li>
<li>Compute <span class="math inline">\(\hat{y}_i^{(j)}, i=1,\ldots,n\)</span>, possibly conditionally on the matrix of predictors <span class="math inline">\(\mathbf{X}\)</span><br />
</li>
</ol>
<ol start="4" style="list-style-type: decimal">
<li>Compute <span class="math inline">\(\forall i\)</span>
<span class="math display">\[\begin{equation}  
\widehat{\text{cov}}\left(\hat{Y}_i;Y_i\right)=1/B\sum_{j=1}^B\left(\hat{y}_i^{(j)}-\hat{y}_i^{(j\cdot)}\right)\left(y_i^{(j)}-y_i^{(j\cdot)}\right)
\end{equation}\]</span>
with <span class="math inline">\(\hat{y}_i^{(j\cdot)}=1/n\sum_{j=1}^B\hat{y}_i^{(j)}\)</span> and <span class="math inline">\(y_i^{(j\cdot)}=1/n\sum_{j=1}^By_i^{(j)}\)</span></li>
</ol>
<blockquote>
<p>Exercise (optional):<br />
Consider the simulation setting of the exercise in the previous Section.<br />
- Instead of splitting the sample in two halves, compute the covariance prenalized predition error, with covariance penalty estimated via simulations (using the proposed algorithm).<br />
- Compare the analysis (decison) with the one obtained by means of cross-validation (in the previous exercise).</p>
</blockquote>
</div>
<div id="mallows-c_p" class="section level3">
<h3><span class="header-section-number">2.3.2</span> Mallows <span class="math inline">\(C_p\)</span></h3>
Consider the linear regression model <span class="math inline">\(Y_i|\mathbf{x}_i \sim \mathcal{N}(\boldsymbol{\mu}\left(\mathbf{x}_i\right),\sigma^2), 0&lt;\sigma^2&lt;\infty\)</span>, with
<span class="math display">\[\begin{equation}  
\boldsymbol{\mu}\left(\mathbf{x}_i\right)=\mathbf{x}_i^T \boldsymbol{\beta}, 
\end{equation}\]</span>
where <span class="math inline">\(\boldsymbol{\beta} \in \mathbb{R}^p\)</span> and <span class="math inline">\(\mathbf{x}_i^T\)</span> is the <em>i</em>th row of <span class="math inline">\(\mathbf{X}\)</span> (that includes a column of ones for the intercept). One notable result (see exercise below) that can be deduced from the covariance penalty formula, for the linear regression model using the least squares estimator (LSE) <span class="math inline">\(\hat{\beta}=\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}\)</span>, <span class="math inline">\(\mathbf{y}=[y_1,\ldots,y_n]^T\)</span>, is Mallow’s <span class="math inline">\(C_p\)</span><a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> <span class="citation">(Mallows <a href="#ref-Mall:73">1973</a>)</span>:
<span class="math display" id="eq:mallow">\[\begin{equation}  
C_p=\frac{1}{n}\sum_{i=1}^n\left(\hat{y}_i-y_i\right)^2+\frac{2}{n}p\sigma^2
\tag{2.2}
\end{equation}\]</span>
<blockquote>
<p>Exercise: derive Mallow’s <span class="math inline">\(C_p\)</span> from <span class="math inline">\(\text{Err}\)</span>.</p>
</blockquote>
<blockquote>
<a href="assessing-the-validity-of-a-model.html#eq:mallow">(2.2)</a> is a special case of <a href="assessing-the-validity-of-a-model.html#eq:err">(2.1)</a> if <span class="math inline">\(\sum_{i=1}^n\text{cov}\left(\hat{Y}_i;Y_i\right)=p\sigma^2\)</span>. We can make use of the trace operator (see <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)">trace</a>) to write
<span class="math display">\[\begin{equation}
\sum_{i=1}^n\text{cov}\left(\hat{Y}_i;Y_i\right) = \mbox{tr}\left(\mbox{cov}(\hat{\mathbf{Y}},\mathbf{Y})\right)
\end{equation}\]</span>
Since <span class="math inline">\(\hat{\mathbf{Y}} = \mathbf{H}\boldsymbol{Y}\)</span>, with <span class="math inline">\(\mathbf{H} = \mathbf{X}(\mathbf{X}^{T}\mathbf{X})^{-1}\mathbf{X}^{T}\)</span> is the least squares projection matrix (or <em>hat matrix</em>) of trace <span class="math inline">\(p\)</span>, we can then write
<span class="math display">\[\begin{eqnarray}
\mbox{tr}\left(\mbox{cov}(\hat{\mathbf{Y}},\mathbf{Y})\right) &amp;=&amp; \mbox{tr}\left( \mbox{cov}(\mathbf{HY},\mathbf{Y})\right)\\
&amp;=&amp; \mbox{tr}\left( \mathbf{H}\mbox{cov}(\mathbf{Y},\mathbf{Y})\right) = 
\mbox{tr}\left(\mathbf{H} \sigma^2 \mathbf{I}\right) \\
&amp;=&amp; \sigma^2 \mbox{tr}\left(\mathbf{H}\right) = \sigma^2 p
\end{eqnarray}\]</span>
<p>q.e.d.</p>
</blockquote>
<p>Note that <a href="assessing-the-validity-of-a-model.html#eq:mallow">(2.2)</a> cannot be used in a real situation since the value of <span class="math inline">\(\sigma^2\)</span> is unknown. In practice, <span class="math inline">\(\sigma^2\)</span> is replaced by <span class="math inline">\(\hat{\sigma}^2\)</span> suitabily chosen. For that, there is no <em>optimal</em> choice and what is usually preferred is the residual variance estimator at the full or maximal model. When <span class="math inline">\(n&lt;p\)</span>, the choice is still an open question…</p>
<blockquote>
Exercise (optional):<br />
Consider a Linear Mixed Model (LMM), for example the electrode resistance data, estimated using the generalized least squares (GLS) estimator.<br />
- Derive and/or estimate <span class="math inline">\(\widehat{\text{Err}}\)</span>. Hint: write the model with a vector of stacked responses and, consequently, a non-diagonal residual error (co)variance.<br />

</blockquote>
</div>
<div id="efrons-q-class" class="section level3">
<h3><span class="header-section-number">2.3.3</span> Efron’s <span class="math inline">\(q\)</span>-class</h3>
Covariance penalties can be applied to measures of prediction error other than squared error, like the Kullback - Leibler divergence. Then, to derive <span class="math inline">\(\widehat{\text{Err}}\)</span>, one needs a more general expression for <span class="math inline">\(\mathbb{E}\left[\text{Err}_i\right]\)</span>. <span class="citation">B. Efron (<a href="#ref-efron1986biased">1986</a><a href="#ref-efron1986biased">a</a>)</span> uses a function <span class="math inline">\(Q(\cdot,\cdot)\)</span> based on the <span class="math inline">\(q\)</span>-class error measure between two scalar functions <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>, given by
<span class="math display" id="eq:Quv">\[\begin{equation}
    Q(u,v) = q(v) + \dot{q}(v) (u - v) - q(u)
    \tag{2.3}
\end{equation}\]</span>
where <span class="math inline">\(\dot{q}(v)\)</span> is the derivative of <span class="math inline">\(q( \cdot )\)</span> evaluated at <span class="math inline">\(v\)</span>. For example <span class="math inline">\(q(v) = v(1-v)\)</span> gives the squared loss function <span class="math inline">\(Q(u,v) = (u - v)^2\)</span> and <span class="math inline">\(q(v)=\min\{v,(1-v)\}\)</span> leads to the missclassification loss <span class="math inline">\(Q(u,v)=I\{u\neq I(u&gt;1/2)\}\)</span>, where <span class="math inline">\(I(\cdot)\)</span> denotes the indicator function. Efron’s optimism Theorem <span class="citation">(Efron <a href="#ref-efron2004estimation">2004</a>)</span> demonstrates that
<span class="math display">\[\begin{equation*} 
    \text{Err}_i = \mathbb{E} \left[ \mathbb{E}_0 \left[ Q(Y^0_i,\hat{Y}_i) | \mathbf{y}\right] \right] =\mathbb{E} \left[ Q(Y_i,\hat{Y}_i) + \Omega_i \right] 
    \label{eq:optimismTHM}
\end{equation*}\]</span>
with <span class="math inline">\(\Omega_i = \text{cov} \left( \dot{q}(\hat{Y}_i),Y_i \right)\)</span>. Hence, an estimator of <span class="math inline">\(\text{Err}\)</span> is obtained as
<span class="math display">\[\begin{equation} 
        \widehat{\text{Err}} = \frac{1}{n} \sum_{i = 1}^{n}\left( Q(y_i,\hat{y}_i) + \widehat{\text{cov}} \left( \dot{q}(\hat{Y}_i),Y_i \right)\right)
\end{equation}\]</span>
<blockquote>
<p>Exercise:<br />
- Verify that by setting <span class="math inline">\(q(v)=\min\{v,(1-v)\}\)</span> one gets <span class="math inline">\(Q(u,v)=I\{u\neq I(u&gt;1/2)\}\)</span>.</p>
</blockquote>
</div>
</div>
<div id="information-theory-and-bayesian-criteria" class="section level2">
<h2><span class="header-section-number">2.4</span> Information Theory and Bayesian Criteria</h2>
<div id="aic-akaike-information-criterion" class="section level3">
<h3><span class="header-section-number">2.4.1</span> AIC: Akaike Information Criterion</h3>
<p>The AIC is derived from <em>Information Theory</em> which concerns the quantification, storage, and communication of information (<span class="citation">Shannon (<a href="#ref-Shan:48a">1948</a><a href="#ref-Shan:48a">a</a>)</span>,<span class="citation">Shannon (<a href="#ref-Shan:48b">1948</a><a href="#ref-Shan:48b">b</a>)</span>). Associated measures are applied to distributions of random variables and include the <em>entropy</em> measure for a single random variable. A derived measure for two random variables is the <em>Kullback-Leibler divergence</em> (or information divergence, information gain, or relative entropy).</p>
Consider two densities <span class="math inline">\(f_0\)</span> and <span class="math inline">\(f_1\)</span>, the Kullback–Leibler divergence is
<span class="math display">\[\begin{equation}  
D\left(f_0,f_1\right)=2\int f_0(y) \log\left(\frac{f_0(y)}{f_1(y)}\right)dy =
2\mathbb{E}_0\left[\log\left(\frac{f_0(y)}{f_1(y)}\right)\right]
\end{equation}\]</span>
The Kullback-Leibler divergence can be used to evaluate the adequacy of a model, by considering e.g. <span class="math inline">\(f_1:=f(y;\hat{\boldsymbol{\theta}})\)</span>, the fitted density corresponding to model <span class="math inline">\(F_{\boldsymbol{\theta}}\)</span>. In that case, the <em>true</em> prediction error becomes a <em>deviance</em> given by
<span class="math display">\[\begin{equation}  
\text{Err}_i=2\mathbb{E}_0\left[\log\left(\frac{f_0(y)}{f(y_i;\hat{\boldsymbol{\theta}})}\right)\right]
\end{equation}\]</span>
with total deviance <span class="math inline">\(1/n\sum_{i=1}^n \text{Err}_i\)</span>. Akaike <span class="citation">(Akaike <a href="#ref-Akai:74">1974</a>)</span> proposed to consider, as a model adequacy measure, an estimator of
<span class="math display">\[\begin{equation}  
2\mathbb{E}_0\left[\mathbb{E}\left[\log\left(f_0(y)\right)-\log\left(f(y;\hat{\boldsymbol{\theta}})\right)\right]\right]
\end{equation}\]</span>
where <span class="math inline">\(\mathbb{E}\)</span> denotes the expectation at the <em>insample</em> distribution. Akaike derived the estimator
<span class="math display">\[\begin{equation} 

-2\sum_{i=1}^n \log f(\hat{y}_i;\hat{\boldsymbol{\theta}})+2p + \text{const.}
\end{equation}\]</span>
where <span class="math inline">\(\text{const.}\)</span> does not depend on the model and hence can be omitted when comparing models. For the linear regression model with <span class="math inline">\(\hat{\boldsymbol{\mu}}=\mathbf{x}^T \hat{\boldsymbol{\beta}}\)</span>, supposing <span class="math inline">\(\sigma^2\)</span> known (and omitting the constant term), we have
<span class="math display">\[\begin{equation} 

\frac{1}{\sigma^2}\sum_{i=1}^n \left(y_i-\mathbf{x}_i^T \hat{\boldsymbol{\beta}}\right)^2+2p 
\end{equation}\]</span>
There exist several expressions for the AIC for the linear regression model, one of them being
<span class="math display">\[\begin{equation} 

\text{AIC}=  \frac{1}{n\sigma^2} \text{RSS}+\frac{2}{n}p 
\end{equation}\]</span>
<p>where <span class="math inline">\(\text{RSS}=\sum_{i=1}^n\left(y_i-\mathbf{x}_i^T \hat{\boldsymbol{\beta}}\right)^2\)</span> is the residual sum-of-squares. We can see that <span class="math inline">\(C_p=\sigma^2\text{AIC}\)</span>.</p>
<p><br></p>
<blockquote>
<p>Exercise (solutions (<a href="solutions.html#aic">5.2.2</a>) provided by <strong>Alexander Maslev</strong>, <strong>Hanxiong Wang</strong> and <strong>Minyoung Lee</strong>). </p>
</blockquote>
<blockquote>
Derive the AIC for the regression model from its more general definition. (optional)<br />

</blockquote>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Program AIC and do model selection in a specific simulation setting with an exhaustive search (follow the passages listed in the CV exercise section).<br />
</li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Compare the performance of your programmed CV and AIC by replicating 100 times the tasks. In particular you should evaluate three specific criteria: the proportion of times the correct model is selected (<em>Exact</em>), the proportion of times the selected model contains the correct one (<em>Correct</em>) and the average number of selected regressors (<em>Average <span class="math inline">\(\sharp\)</span></em>)<br />
</li>
<li>In the same simulation setting outlined in the CV exercise section, generate from a MVN (multivariate normal) a matrix <span class="math inline">\(\mathbf{X_{n*p}}\)</span> with <span class="math inline">\(n = 1000\)</span> and <span class="math inline">\(p = 5\)</span> but now fix the scale matrix with an autoregressive form <span class="math inline">\(\boldsymbol{\Sigma}=[\sigma_{lm}]_{l,m=1,\ldots,p}\)</span> with <span class="math inline">\(\sigma_{lm} = \rho^{\mid l - m\mid}\)</span>. Compare the performance of CV and AIC for <span class="math inline">\(\boldsymbol{\rho} = [0.2 \; 0.5\; 0.7]\)</span> (<span class="math inline">\(\rho = 0\)</span> corresponds to the identity case that you have already treated).<br />
</li>
<li>Upload the <a href="https://github.com/CaesarXVII/MSHD-book-and-datasets/blob/master/datasets/malnutrion_zambia_cleaned.Rda">Zambia dataset</a> and perform an exhaustive search on the continuous covariates (i.e. avoiding factors) based on CV and AIC in order to find the best model. You can either employ your codes derived in previous exercises or make use of the existing R packages: <em>leaps</em>, <em>glmulti</em>, <em>MuMIn</em> and <em>caret</em>.<br />
</li>
</ol>
</blockquote>
<blockquote>
<p>Exercise (optional):<br />
Using the general result on covariance penalty measures based on Efron’s <span class="math inline">\(q\)</span>-class, show that the AIC is a suitable estimator of the prediction error.</p>
</blockquote>
<p><br></p>
</div>
<div id="bic-bayesian-information-criterion" class="section level3">
<h3><span class="header-section-number">2.4.2</span> BIC: Bayesian Information Criterion</h3>
<span class="citation">(Schwarz <a href="#ref-Schw:78">1978</a>)</span> derived the Bayesian information criterion as
<span class="math display">\[\begin{equation} 
\text{BIC} = -\sum_{i=1}^n \log f(\hat{y}_i;\hat{\boldsymbol{\mu}},\hat{\sigma}^2)+p\log(n) + \text{const.}
\end{equation}\]</span>
<p>where <span class="math inline">\(\log(n)\)</span> is the natural logarithm of n and <span class="math inline">\(\text{const.}\)</span> does not depend on the model; hence can be omitted when comparing models. The BIC is derived from Bayesian inference arguments, but is not related to information theory. Compared to the AIC (or indeed the <span class="math inline">\(C_p\)</span>), the BIC uses <span class="math inline">\(p\log(n)\)</span> instead of <span class="math inline">\(2p\)</span> as an estimated penalty and since <span class="math inline">\(\log(n)&gt; 2\)</span> for any <span class="math inline">\(n &gt; 7\)</span>, the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than AIC.</p>
</div>
</div>
<div id="mean-squared-error-based-criteria" class="section level2">
<h2><span class="header-section-number">2.5</span> Mean Squared Error Based Criteria</h2>
<div id="steins-unbiased-risk-estimator-sure" class="section level3">
<h3><span class="header-section-number">2.5.1</span> Stein’s unbiased risk estimator (SURE)</h3>
<span class="citation">Stein (<a href="#ref-Stei:81">1981</a>)</span> derived an unbiased estimator of the mean-squared error of “a nearly arbitrary, nonlinear biased estimator”, hence providing an indication of the accuracy of a given estimator. Namely, consider the normal setting with <span class="math inline">\(Y\vert \mathbf{X} \sim \cal{N}\left(\boldsymbol{\mu}(\mathbf{X}),\sigma^2\right)\)</span> with a differentiable estimator <span class="math inline">\(\hat{\boldsymbol{\mu}}:=\hat{\boldsymbol{\mu}}(Y,\mathbf{X})\)</span>, <span class="citation">Stein (<a href="#ref-Stei:81">1981</a>)</span> proposed an unbiased estimator of <span class="math inline">\(\mathbb{E}\left[\vert\vert \boldsymbol{\mu}-\hat{\boldsymbol{\mu}}\vert\vert_2^2\right]\)</span> (MSE), given by
<span class="math display">\[\begin{equation}
\text{SURE}=\sum_{i=1}^n\left(\hat{y}_i-y_i\right)^2+2\sigma^2\sum_{i=1}^n \frac{\partial\hat{\mu}_i}{\partial y_i}-n\sigma^2
\end{equation}\]</span>
<p>Actually, for <span class="math inline">\(Z\sim N(0,\sigma^2)\)</span>, it can be shown that <span class="math inline">\(\mathbb{E}\left[Zf(Z)\right]=\sigma^2\mathbb{E}\left[f^{&#39;}(Z)\right]\)</span> <span class="citation">(Stein <a href="#ref-Stei:81">1981</a>)</span>, so that <span class="math inline">\(\sigma^2\sum_{i=1}^n\mathbb{E}\left[\partial\hat{\mu}_i/\partial Y_i\right]=\sum_{i=1}^n\text{cov}\left(Y_i,\hat{Y}_i\right)\)</span>. This implies that, in the covariance penalty framework of Section 2.3, for the normal model, <span class="math inline">\(\sigma^2\sum_{i=1}^n \partial\hat{\mu}_i/\partial y_i\)</span> is a suitable estimator of <span class="math inline">\(\sum_{i=1}^n\text{cov}\left(Y_i,\hat{Y}_i\right)\)</span>.</p>
<p>For the linear model with <span class="math inline">\(\boldsymbol{\mu}=\mathbf{X}\boldsymbol{\beta}\)</span> and considering the OLS, <span class="math inline">\(\hat{\boldsymbol{\mu}}=\mathbf{X}(\mathbf{X}^T\mathbf{X})^{^-1}\mathbf{X}^T\mathbf{y}\)</span>, we have <span class="math inline">\(\partial\hat{\mu}_i/\partial y_i=\mathbf{x}_i(\mathbf{X}^T\mathbf{X})^{^-1}\mathbf{x}_i^T\)</span>, and hence the SURE is not equal to the <span class="math inline">\(C_p\)</span>. It is however more general, i.e. for any differentiable estimator <span class="math inline">\(\hat{\boldsymbol{\mu}}\)</span> of <span class="math inline">\(Y\)</span>, and hence useful in nonparametric regression (see e.g. <span class="citation">Donoho and Johnstone (<a href="#ref-DoJo:95">1995</a>)</span>)</p>
<blockquote>
<p>Exercise: show that for <span class="math inline">\(Z\sim N(0,\sigma^2)\)</span>, <span class="math inline">\(\mathbb{E}\left[Zf(Z)\right]=\sigma^2\mathbb{E}\left[f^{&#39;}(Z)\right]\)</span></p>
</blockquote>
</div>
<div id="the-focused-information-criterion-fic" class="section level3">
<h3><span class="header-section-number">2.5.2</span> The Focused Information Criterion (FIC)</h3>
<p>The FIC in its original format (see <span class="citation">Claeskens and Hjort (<a href="#ref-ClHj:03">2003</a>)</span>) interprets <em>best</em> model in the sense of <em>minimizing the mean squared error</em> (MSE) of the estimator of the quantity of interest. The FIC philosophy puts less emphasis on which variables are in the model but rather on the accuracy of the estimator of a focus.</p>
<p>To build the FIC, one considers a model of the form <span class="math inline">\(F_{\boldsymbol{\nu}, \boldsymbol{\gamma}}\)</span>, with density <span class="math inline">\(f(\cdot;\boldsymbol{\nu},\boldsymbol{\gamma})\)</span>, with <span class="math inline">\(\boldsymbol{\nu} \in \mathbb{R}^p\)</span> not subject to model selection (i.e. included in all considered models), <span class="math inline">\(\boldsymbol{\gamma} \in \mathbb{R}^q\)</span> the parameters on which model selection is performed. <span class="math inline">\(\boldsymbol{\gamma}\)</span> and <span class="math inline">\(q\)</span> are allowed to depend on the sample size <span class="math inline">\(n\)</span>, hence <span class="math inline">\(\boldsymbol{\gamma}:= \boldsymbol{\gamma}_{n}\)</span> and <span class="math inline">\(q:= q_n\)</span>. For the linear regression model <span class="math inline">\(Y_i|\mathbf{x}_i \sim \mathcal{N}(\beta_0+\mathbf{x}_i\boldsymbol{\beta},\sigma^2)\)</span>(with <span class="math inline">\(\mathbf{x}_i\)</span> not containing the one in the first column), a natural choice is <span class="math inline">\(\boldsymbol{\nu} = (\beta_0,\sigma^2)\)</span> and <span class="math inline">\(\boldsymbol{\gamma}_n = \boldsymbol{\beta}\)</span>. The focus, or quantity of interest, is <span class="math inline">\(\boldsymbol{\mu}:=\boldsymbol{\mu}(\boldsymbol{\nu},\boldsymbol{\gamma}_n)\)</span>, which can be, but not necessarily, the prediction for one particular new observation. Given a (chosen) estimator for the focus, <span class="math inline">\(\hat{\boldsymbol{\mu}}:=\boldsymbol{\mu}(\hat{\boldsymbol{\nu}},\hat{\boldsymbol{\gamma}}_n)\)</span>, assuming <span class="math inline">\(\boldsymbol{\gamma}_n= \boldsymbol{\gamma} + \boldsymbol{\delta}/\sqrt{n}\)</span> and considering a fixed value <span class="math inline">\(q\)</span>, <span class="citation">Claeskens and Hjort (<a href="#ref-ClHj:03">2003</a>)</span> use a Taylor expansion of <span class="math inline">\(\sqrt{n}\left(\hat{\boldsymbol{\mu}}-\boldsymbol{\mu}\right)\)</span> to derive the bias and variance to build the (first order) MSE.</p>
More specifically, consider the set of indices <span class="math inline">\(S\subseteq \left\{1,\ldots,q\right\}\)</span> such that <span class="math inline">\(\boldsymbol{\gamma}_S\subseteq \boldsymbol{\gamma}\)</span> is the corresponding subset of parameters, hence forming a submodel <span class="math inline">\(S\)</span> of <span class="math inline">\(F_{\boldsymbol{\nu},\boldsymbol{\gamma}}\)</span>, one gets
<span class="math display">\[\begin{equation}
\sqrt{n}\left(\hat{\boldsymbol{\mu}}_S-\boldsymbol{\mu}\right)\approx \left[\frac{\partial\boldsymbol{\mu}(\boldsymbol{\nu},\boldsymbol{\gamma})}{\partial\boldsymbol{\nu}}\right]^T\sqrt{n}\left(\hat{\boldsymbol{\nu}}-\boldsymbol{\nu}\right)+
\left[
\frac{\partial\boldsymbol{\mu}(\boldsymbol{\nu},\boldsymbol{\gamma})}{\partial\boldsymbol{\gamma}_S}
\right]^T\sqrt{n}\left(\hat{\boldsymbol{\gamma}}_S-\boldsymbol{\gamma}\right)-\frac{\partial\boldsymbol{\mu}(\boldsymbol{\nu},\boldsymbol{\gamma})}{\partial\boldsymbol{\gamma}}\boldsymbol{\delta} 
\end{equation}\]</span>
<p>To derive the MSE (of <span class="math inline">\(\hat{\boldsymbol{\mu}}_S\)</span>), <span class="citation">Claeskens and Hjort (<a href="#ref-ClHj:03">2003</a>)</span> use the asymptotic distribution of <span class="math inline">\(\sqrt{n}\left(\hat{\boldsymbol{\mu}}_S-\boldsymbol{\mu}\right)\)</span>.</p>
<blockquote>
<p>Project:<br />
- For the Malnutrition in Zambia dataset, considering the <code>breastfeeding duration</code> as the most important variable, develop an estimator for the FIC, build a model that optimizes the FIC and compare the resulting model with the one obtained with other methods.</p>
</blockquote>
</div>
</div>
<div id="classification-measures" class="section level2">
<h2><span class="header-section-number">2.6</span> Classification measures</h2>
<p>When model building is about <em>predicting</em> binary outcome, like for example the success (or not) of a treatment in medical research, one enters in the world of <em>classification</em>. We are concerned here in <em>supervised classification</em>, i.e. the exercise of building a <em>classifier</em> from a dataset containing the outcome (response) variable. When the outcome variable is not available, the exercise is referred to as <em>unsupervised classification</em> (or <em>clustering</em>).</p>
<p>In a <em>model building</em> framework, the obvious <em>model</em> for binary classification is the logistic regression, a member of the family of generalized linear models (GLM). In this setting, prediction error becomes <em>classification error</em> and as it will be seeing below, there is no unique and natural measure, which imply that assessing the validity of the model (i.e. the <em>classification rule</em>) is not a straightforward task.</p>
<p>Indeed, the nature of the problem can make classical prediction error measures rather useless. For example, suppose that the objective is to build a logistic model to classify patients into two groups, the ones with a serious disease and the other healthy ones. Most often, the collected data will contain a large majority of healthy patients (representing what happens in the population), say 95%, so that a <em>good classifier</em> (that is difficult to beat), is the one that predicts ALL patients as healthy. In that case, the classification error as measured by a prediction criterion would be of only 5%. Hence, other criteria need to be considered when assessing the validity of a classifier (model).</p>
<div id="the-logistic-model" class="section level3">
<h3><span class="header-section-number">2.6.1</span> The logistic model</h3>
<p>The logistic model links, through the Bernoulli distribution, the outcome variable (response) <span class="math inline">\(Y\)</span> and the linear predictor <span class="math inline">\(\mathbf{X}\boldsymbol{\beta}\)</span>, <span class="math inline">\(\mathbf{X}\)</span> being an <span class="math inline">\(n\times p\)</span> matrix of fixed covariates with row <span class="math inline">\(\mathbf{x}_i,i=1,\ldots,n\)</span>, by means of <span class="math inline">\(Y_i\vert \mathbf{x}_i \sim_{iid} B(\pi_i)\)</span>, with <span class="math inline">\(\pi_i=\mathbb{E}[Y_i]=P(Y_i=1)=\exp(\mathbf{x}_i\boldsymbol{\beta})/(1+\exp(\mathbf{x}_i\boldsymbol{\beta}))\)</span> (logistic link).</p>
An estimator for <span class="math inline">\(\boldsymbol{\beta}\)</span> is the maximum likelihood estimator (MLE) with score functions
<span class="math display">\[\begin{equation}
\mathbf{S}(\boldsymbol{\beta}\vert \mathbf{X},\mathbf{y})=\sum_{i=1}^n\mathbf{x}^T_iy_i- \sum_{i=1}^n\frac{\exp(\mathbf{x}_i\boldsymbol{\beta})}{1+\exp(\mathbf{x}_i\boldsymbol{\beta})}\mathbf{x}^T_i
\end{equation}\]</span>
<p>so that <span class="math inline">\(\hat{\boldsymbol{\beta}}=\mbox{argzero}_{\boldsymbol{\beta}}\mathbf{S}(\boldsymbol{\beta}\vert \mathbf{X},\mathbf{y})\)</span> which has no closed form.</p>
In high dimensions, this feature adds numerical difficulties compared to the linear regression model. Indeed, a popular algorithm for finding <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the <em>iteratively reweighted least squares</em> with iterative <span class="math inline">\(k\)</span>th step given by
<span class="math display">\[\begin{equation}
\hat{\boldsymbol{\beta}}^{k}\equiv \hat{\boldsymbol{\beta}}^{k-1}+\mathbf{J}^{-1}\left(\hat{\boldsymbol{\beta}}^{k-1}\right)\mathbf{S}(\boldsymbol{\beta}\vert \mathbf{X},\mathbf{y})
\end{equation}\]</span>
<p>with <span class="math inline">\(\mathbf{J}\left(\hat{\boldsymbol{\beta}}\right)\)</span> being the negative of the Hessian matrix (partial derivative of <span class="math inline">\(\mathbf{S}(\boldsymbol{\beta}\vert \mathbf{X},\mathbf{y})\)</span> with respect to <span class="math inline">\(\boldsymbol{\beta}\)</span>) that depends on the current value of <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, hence requiring numerous inversions of high dimensional matrices.</p>
</div>
<div id="prediction-error-measures-for-binary-classification" class="section level3">
<h3><span class="header-section-number">2.6.2</span> Prediction error measures for Binary classification</h3>
<p>Before considering a prediction error measure, one should define a prediction measure for the logistic model. There are two common measures:<br />
- The <em>fitted probabilities</em>: <span class="math inline">\(\hat{\pi}_i=\widehat{\mathbb{E}[Y_i]}=\exp(\mathbf{x}_i\hat{\boldsymbol{\beta}})/(1+\exp(\mathbf{x}_i\hat{\boldsymbol{\beta}}))\)</span><br />
- The <em>predicted outcome</em>: <span class="math inline">\(I\left(\hat{\pi}_i\geq c\right)\)</span> with <span class="math inline">\(I\left(\right)\)</span> being the indicator function and <span class="math inline">\(c\)</span> a chosen threshold with a priori <span class="math inline">\(c=0.5\)</span>.</p>
<p>A classification error then builds, in general, upon <span class="math inline">\(\left(y_i-\hat{\pi}_i\right)\)</span> or <span class="math inline">\(\left(y_i-I\left(\hat{\pi}_i\geq c\right)\right)\)</span>. Popular measures include: </p>
<ul>
<li>The quadratic error: <span class="math inline">\(\Vert \mathbf{y}-\hat{\boldsymbol{\pi}}(\mathbf{X})\Vert_2\)</span> with <span class="math inline">\(\mathbf{y}=[y_i]_{i=1,\ldots,n}\)</span>, <span class="math inline">\(\hat{\boldsymbol{\pi}}(\mathbf{X})=[\hat{\pi}_i(\mathbf{x}_i)]_{i=1,\ldots,n}\)</span> and <span class="math inline">\(\Vert \cdot \Vert_2\)</span> denoting the <span class="math inline">\(L2\)</span>-norm.<br />
<br />
</li>
<li>The misclassification error or Hamming distance: <span class="math inline">\(\sum_{i=1}^n(y_i-I\left(\hat{\pi}_i(\mathbf{x}_i)\geq c\right))^2\)</span>.<br />
<br />
</li>
<li>The Binomial <em>deviance</em> (or cross-entropy): <span class="math inline">\(\sum_{i=1}^n\left(y_i\log(\hat{\pi}_i(\mathbf{x}_i))-(1-y_i)\log(1-\hat{\pi}_i(\mathbf{x}_i))\right)\)</span>. <br />
</li>
<li>The hinge loss (or support vector machines, <span class="citation">Vapnik (<a href="#ref-Vapn:96">1996</a>)</span>): <span class="math inline">\(\sum_{i=1}^n \max\left(1−(2y_i −1)\mbox{sign}(\hat{\pi}_i(\mathbf{x}_i) −c), 0\right)\)</span>, with <span class="math inline">\(c=0.5\)</span>.<br />
<br />
</li>
<li>The exponential loss (from the AdaBoost algorithm in generalized additive models, <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-HaTiFr:09">2009</a>)</span>): <span class="math inline">\(\sum_{i=1}^n\exp\left(-(2y_i-1)\mathbf{x}_i\hat{\boldsymbol{\beta}}\right)\)</span>.</li>
</ul>
<p>The Hamming distance does not really <em>predict</em> above (below) the threshold <span class="math inline">\(c\)</span>, since they are the same for say <span class="math inline">\(\hat{\pi}_i(\mathbf{x}_i)=0.6\)</span> and <span class="math inline">\(\hat{\pi}_i(\mathbf{x}_i)=0.9\)</span> with <span class="math inline">\(c=0.5\)</span>. There is therefore an information loss.</p>
In general, the (total) deviance for a model with estimates <span class="math inline">\(\hat{\boldsymbol{\mu}}(\mathbf{X}) =\widehat{\mathbb{E}}\left[ \mathbf{Y} \vert \mathbf{X},\hat{\boldsymbol{\beta}}\right]\)</span> may be constructed by its likelihood function as
<span class="math display">\[\begin{equation}
D \left( \mathbf{Y} , \hat{\boldsymbol{\mu}}(\mathbf{X}) \right) = 2 \left( \log P ( \mathbf{Y} \vert \mathbf{X},\hat{\boldsymbol{\beta}}^s )  − \log (P (\mathbf{Y} \vert \mathbf{X},\hat{\boldsymbol{\beta}}  )  \right) 
\end{equation}\]</span>
<p>where <span class="math inline">\(\hat{\boldsymbol{\beta}}^s\)</span> denotes the fitted parameters for the saturated model, i.e. with a parameter for every observation so that the data are fitted exactly. Hence, <span class="math inline">\(P ( \mathbf{Y} \vert \mathbf{X},\hat{\boldsymbol{\beta}}^s )=\mathbf{1}\)</span> so that <span class="math inline">\(\log P ( \mathbf{Y} \vert \mathbf{X},\hat{\boldsymbol{\beta}}^s )=\mathbf{0}\)</span> and for the logistic model with <span class="math inline">\(\hat{\boldsymbol{\mu}}(\mathbf{X})=\hat{\boldsymbol{\pi}}(\mathbf{X})\)</span>, <span class="math inline">\(D \left( \mathbf{Y} , \hat{\boldsymbol{\mu}} (\mathbf{X})\right)\)</span> becomes the Binomial deviance.</p>
<p>The hinge and exponential loss functions are particular cases of a general loss function used in machine learning: for binary outcomes <span class="math inline">\(y_i^{\star}=2y_i-1\)</span> taking the values <span class="math inline">\(-1\)</span> or <span class="math inline">\(1\)</span> and a prediction function <span class="math inline">\(\hat{\mu}_i(\mathbf{x}_i)\)</span>, a general loss function is <span class="math inline">\(V\left(y_i^{\star},\hat{\mu}_i(\mathbf{x}_i)\right)=\phi(-y_i^{\star}\hat{\mu}_i(\mathbf{x}_i))\)</span>. The hinge loss uses <span class="math inline">\(\hat{\mu}_i(\mathbf{x}_i)=\mbox{sign}(\hat{\pi}_i(\mathbf{x}_i) −c)\)</span>, hence is based on the fitted probabilities, while the exponential loss uses <span class="math inline">\(\hat{\mu}_i(\mathbf{x}_i)=\mathbf{x}_i\hat{\boldsymbol{\beta}}\)</span>. Note that <span class="math inline">\(\mathbf{x}_i\hat{\boldsymbol{\beta}}=\log\left(\hat{\pi}_i(\mathbf{x}_i)/(1-\hat{\pi}_i(\mathbf{x}_i))\right)\)</span></p>
<p>A comparison between different classification error measures (loss functions) can be found in <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-HaTiFr:09">2009</a>)</span>, Section 10.6.</p>
</div>
<div id="classification-error-estimation" class="section level3">
<h3><span class="header-section-number">2.6.3</span> Classification error estimation</h3>
<p>Efron’s optimism Theorem <span class="citation">(Efron <a href="#ref-efron2004estimation">2004</a>)</span> can also be used to obtain an estimator of the out-of-sample classification error, at least up to <span class="math inline">\(\widehat{\text{cov}} \left( \dot{q}(\hat{\mu}_i),Y_i \right)\)</span>, with the appropriately chosen prediction function <span class="math inline">\(\hat{\mu}_i\)</span>.</p>
<blockquote>
<p>Exercise:<br />
- Show that when <span class="math inline">\(q(v) = v(1-v)\)</span> in <a href="assessing-the-validity-of-a-model.html#eq:Quv">(2.3)</a>, one obtains for the loss function the quadratic error.<br />
- Show that when <span class="math inline">\(q(v)=\min\{v,(1-v)\}\)</span> in <a href="assessing-the-validity-of-a-model.html#eq:Quv">(2.3)</a>, one obtains for the loss function the misclassification error.<br />
- Show that when <span class="math inline">\(q(v)= -2[v \log(v) + (1-v)\log(1-v)]\)</span> in <a href="assessing-the-validity-of-a-model.html#eq:Quv">(2.3)</a>, one obtains for the loss function the binomial deviance or twice the Kullback-Leibler divergence.<br />
- Show that when <span class="math inline">\(q(v)=2\min\{v,(1-v)\}\)</span> in <a href="assessing-the-validity-of-a-model.html#eq:Quv">(2.3)</a>, one obtains for the loss function the hinge loss.<br />
- Show that when <span class="math inline">\(q(v)=2\sqrt{v,(1-v)}\)</span> in <a href="assessing-the-validity-of-a-model.html#eq:Quv">(2.3)</a>, one obtains for the loss function the exponential loss.</p>
</blockquote>
<p>Deriving <span class="math inline">\(\text{cov} \left( \dot{q}(\hat{\mu}_i),Y_i \right)\)</span> analytically and estimating it by then plugging in parameter’s estimates is not a straightforward task. Approximations provided in <span class="citation">Efron (<a href="#ref-Efro:78">1978</a>)</span> and <span class="citation">B. Efron (<a href="#ref-Efro:86">1986</a><a href="#ref-Efro:86">b</a>)</span> can be used. Alternatively, one can rely on simulation based (bootstrap) estimation of the covariance, as is done in <span class="citation">Efron (<a href="#ref-efron2004estimation">2004</a>)</span>, but in high dimensions, the computational cost could be prohibitive.</p>
<blockquote>
<p>Ecercise (optional):<br />
- Derive <span class="math inline">\(\text{cov} \left( \dot{q}(\hat{\mu}_i),Y_i \right)\)</span> for the quadratic error, using a Efron’s approximation <span class="math inline">\(\hat{\boldsymbol{\beta}} = \Sigma^{-1} \hat{\boldsymbol{\theta}}\)</span>, with <span class="math inline">\(\hat{\boldsymbol{\beta}}=\sum_{i=1}^n \mathbf{x}_{i}y_{i}\)</span> and <span class="math inline">\(\Sigma=\sum_{i=1}^n \pi_{i} (1-\pi_{i})\)</span>, <span class="math inline">\(\mathbf{x}_{i} \mathbf{x}_{i}^{T}\)</span> and also using first order Taylor expansions.</p>
</blockquote>
</div>
<div id="the-roc-curve" class="section level3">
<h3><span class="header-section-number">2.6.4</span> The ROC curve</h3>
<p>The <em>receiver operating characteristic</em> curve, i.e. ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.</p>
<p>Indeed, with binary classification, supposing two possible outcomes denoted as <em>positive</em>, respectively, <em>negative</em>, there are four possible classification outcomes:<br />
- True positive (TP): the outcome is positive and correctly predicted as positive.<br />
- False positive (FP) occurs when the outcome is negative and predicted as positive.<br />
- True negative (TN): the outcome is negative and predicted as negative.<br />
- False negative (FN) occurs when the outcome is positive and predicted as negative.</p>
<p>The corresponding <em>rates</em> are the probability (proportion) of occurrences that are correctly identified as such. For example, the TPR is the probability of positive prediction given that true state is positive, or in other words, the proportion of positive predictions among all positive outcomes. The rates are estimated from the sample for a selected model (classifier).</p>
<p>The TPR is also known as <em>sensitivity</em> and the TNR as the <em>specificity</em> (the probability of negative prediction given that the true state is negative). Sensitivity therefore quantifies the avoiding of FN, and specificity does the same for FP. There is actually a trade-off between the measures like with hypothesis testing and the associated two error-types. This trade-off can be represented graphically using the ROC curve which plots <em>1 - specificity</em> versus <em>sensitivity</em>. These measures and representations are widely used in medical sciences, where, ideally, a perfect model (classifier) would be described as 100% <em>sensitive</em>, meaning e.g. all sick individuals are correctly identified as sick, and 100% <em>specific</em>, meaning no healthy individuals are incorrectly identified as sick.</p>
<blockquote>
Example: ROC curve and associated trade-off measures<br />

</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Taken from https://en.wikipedia.org/wiki/Receiver_operating_characteristic.</span>
<span class="co">#</span>
<span class="co"># The ideal model (classifier) would reach the (0,100)% point.</span>
<span class="co">#</span>
knitr<span class="op">::</span><span class="kw">include_graphics</span>(<span class="st">&quot;Figures/ROC_curves.png&quot;</span>)</code></pre></div>
<p><img src="Figures/ROC_curves.png" style="display: block; margin: auto;" /></p>
<p>The ROC curve is mostly used as a <em>diagnostic</em> tool to evaluate a selected model (classifier). Indeed, in subset selection, a set of predictors (say <span class="math inline">\(\mathbf{X}_S\)</span>) are chosen and from which, using the logistic regression, fitted probabilities <span class="math inline">\(\hat{\pi}_i(\mathbf{x}_{iS})\)</span> can be computed. The ROC curve is then computed by varying the threshold <span class="math inline">\(c\)</span> in predicting outcomes using <span class="math inline">\(\hat{y}_i=I(\hat{\pi}_i(\mathbf{x}_{iS})\geq c)\)</span>. Indeed, for different values of <span class="math inline">\(c\)</span>, the predictions <span class="math inline">\(\hat{y}_i\)</span> are different, hence leading to different measures for the sensitivity and the specificity. Conventionally, the sensitivity and specificity are computed at <span class="math inline">\(c=0.5\)</span>.</p>
<blockquote>
Exercise (solutions presented in <a href="solutions.html#roc">5.2.3</a>)<br />

</blockquote>
<blockquote>
Read the information on the <a href="https://caesarxvii.github.io/MSHD-book-and-datasets/index.html#prognostic-factors-in-childhood-leukemia">Leukemia Dataset</a> in the first chapter of the book. Then load the <a href="https://github.com/CaesarXVII/MSHD-book-and-datasets/blob/master/datasets/data_leukemia_reduced.Rda">Leukemia dataset reduced</a> which contains a subset of 11 eleven predictors among the 3571 present in the <em>leukemia_small.csv</em>. These variables have been selected, because of their importance, by the binary lasso which is a shrinkage method that will be discussed later on in the course. Now perform the following steps:<br />
(a) Fit the appropriate GLM for the situation using only one of the available predictors (e.g. V457)<br />
(b) Read the <a href="https://caesarxvii.github.io/MSHD-book-and-datasets/assessing-the-validity-of-a-model.html#the-roc-curve">ROC curve</a> section of the e-book. Then find the TPR (i.e. true positive rate), FPR (i.e. false positive rate), TNR (i.e. true negative rate), FNR (i.e. false negative rate) of the fitted values found at point (a) with a cut-off value <span class="math inline">\(c = 0.5\)</span>.<br />
(c) For a given cut-off grid of values, that you can choose as you wish, plot the ROC curve relative to the estimated model at point (a).<br />
(d) Check the quality of your result at point (c) with the R package <em>pROC</em>.<br />

</blockquote>
<blockquote>
Exercise (optional):<br />
- For a given selected model (classifier), using the logistic model and estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>, express (formula) the estimated sensitivity and specificity as a function of the threshold <span class="math inline">\(c\)</span>.<br />

</blockquote>
<p>The area under the ROC is a scalar summary measure that is sometimes used as model validity. It is sometimes called the <em>c</em>-statistic. Alternative summary statistics include the <em>Gini coefficient</em> which corresponds to the area between the ROC curve and the no-discrimination line. However, for evaluating the contribution of an additional predictor when added to a standard model, any summary statistic associated to the ROC curve, like the the <em>c</em>-statistic, may not be an informative measure. The new predictor can be very significant in terms of the change in e.g. model deviance, but show only a small increase in the <em>c</em>-statistic (see <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-HaTiFr:09">2009</a>)</span>).</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-efron2004estimation">
<p>Efron, B. 2004. “The Estimation of Prediction Error.” <em>Journal of the American Statistical Association</em> 99 (467). Taylor &amp; Francis: 619–32.</p>
</div>
<div id="ref-EfHa:16">
<p>Efron, B., and T. Hastie. 2016. <em>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</em>. Cambridge University Press.</p>
</div>
<div id="ref-Mall:73">
<p>Mallows, C. L. 1973. “Some Comments on <span class="math inline">\(C_p\)</span>.” <em>Technometrics</em> 15: 661–75.</p>
</div>
<div id="ref-efron1986biased">
<p>Efron, B. 1986a. “How Biased is the Apparent Error Rate of a Prediction Rule?” <em>Journal of the American Statistical Association</em> 81 (394). Taylor &amp; Francis: 461–70.</p>
</div>
<div id="ref-Shan:48a">
<p>Shannon, C. E. 1948a. “A Mathematical Theory of Communication.” <em>Bell System Technical Journal</em> 27: 379–423.</p>
</div>
<div id="ref-Shan:48b">
<p>Shannon, C. 1948b. “A Mathematical Theory of Communication.” <em>Bell System Technical Journal</em> 27: 623–66.</p>
</div>
<div id="ref-Akai:74">
<p>Akaike, H. 1974. “A New Look at the Statistical Model Identification.” <em>IEEE Transactions on Automatic Control</em> AC-19: 716–23.</p>
</div>
<div id="ref-Schw:78">
<p>Schwarz, G. 1978. “Estimating the Dimension of a Model.” <em>Annst</em> 6: 461–64.</p>
</div>
<div id="ref-Stei:81">
<p>Stein, C. M. 1981. “Estimation of the Mean of a Multivariate Normal Distribution.” <em>The Annals of Statistics</em> 9: 1135–51.</p>
</div>
<div id="ref-DoJo:95">
<p>Donoho, David L., and Iain M. Johnstone. 1995. “Adapting to Unknown Smoothness via Wavelet Shrinkage.” <em>Journal of the American Statistical Association</em> 90: 1200–1224.</p>
</div>
<div id="ref-ClHj:03">
<p>Claeskens, Gerda, and Nils Lid Hjort. 2003. “The Focused Information Criterion.” <em>Journal of the American Statistical Association</em> 98: 900–945.</p>
</div>
<div id="ref-Vapn:96">
<p>Vapnik, V. 1996. <em>The Nature of Statistical Learning Theory</em>. New York: Springer.</p>
</div>
<div id="ref-HaTiFr:09">
<p>Hastie, Trevor, Tibshirani Robert, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Second Edition. Springer Series in Statistics. Springer.</p>
</div>
<div id="ref-Efro:78">
<p>Efron, B. 1978. “The Geometry of Exponential Families.” <em>The Annals of Statistics</em> 6: 362–76.</p>
</div>
<div id="ref-Efro:86">
<p>Efron, B. 1986b. “How Biased Is the Apparent Error Rate of a Prediction Rule?” <em>Journal of the American Statistical Association</em> 81 (394): 461–70. <a href="http://www.jstor.org/stable/2289236" class="uri">http://www.jstor.org/stable/2289236</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="3">
<li id="fn3"><p>Mallows <span class="math inline">\(C_p\)</span> is originally defined as <span class="math inline">\(C_p=\frac{1}{\sigma^2}\sum_{i=1}^n\left(\hat{y}_i-y_i\right)^2+2p-n\)</span>, which provides the same information when comparing models.<a href="assessing-the-validity-of-a-model.html#fnref3">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ordering-the-variables.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
