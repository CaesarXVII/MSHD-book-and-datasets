<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>7 Solutions | Model Selection in High Dimensions</title>
  <meta name="description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="7 Solutions | Model Selection in High Dimensions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7 Solutions | Model Selection in High Dimensions" />
  
  <meta name="twitter:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  

<meta name="author" content="Maria-Pia Victoria-Feser (professor), Cesare Miglioli and Guillaume Blanc (teaching assistants)">


<meta name="date" content="2019-02-25">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="post-selection-inference.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#read-this-part-first"><i class="fa fa-check"></i><b>1.1</b> Read this part first</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#content-choice-and-structure"><i class="fa fa-check"></i><b>1.2</b> Content choice and structure</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#bibliography"><i class="fa fa-check"></i><b>1.2.1</b> Bibliography</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#useful-links"><i class="fa fa-check"></i><b>1.2.2</b> Useful links</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#using-r"><i class="fa fa-check"></i><b>1.3</b> Using R</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#useful-r-packages"><i class="fa fa-check"></i><b>1.3.1</b> Useful R packages</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#managing-data"><i class="fa fa-check"></i><b>1.3.2</b> Managing Data</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#loading-data-from-an-r-package"><i class="fa fa-check"></i><b>1.3.3</b> Loading data from an R package</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#loading-data-from-a-local-file"><i class="fa fa-check"></i><b>1.3.4</b> Loading data from a local file</a></li>
<li class="chapter" data-level="1.3.5" data-path="index.html"><a href="index.html#loading-data-from-an-online-file"><i class="fa fa-check"></i><b>1.3.5</b> Loading data from an online file</a></li>
<li class="chapter" data-level="1.3.6" data-path="index.html"><a href="index.html#loading-data-from-an-online-database-using-a-mysql-query-optional"><i class="fa fa-check"></i><b>1.3.6</b> Loading data from an online database using a mySQL query (Optional)</a></li>
<li class="chapter" data-level="1.3.7" data-path="index.html"><a href="index.html#data-wrangling"><i class="fa fa-check"></i><b>1.3.7</b> Data Wrangling</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#writing-reports"><i class="fa fa-check"></i><b>1.4</b> Writing reports</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#r-markdown"><i class="fa fa-check"></i><b>1.4.1</b> R Markdown</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4.2</b> GitHub</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.5</b> Examples</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#data-on-malnutrition-in-zambia"><i class="fa fa-check"></i><b>1.5.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#prognostic-factors-in-childhood-leukemia"><i class="fa fa-check"></i><b>1.5.2</b> Prognostic Factors in Childhood Leukemia</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#gene-expression-in-prostate-cancer"><i class="fa fa-check"></i><b>1.5.3</b> Gene Expression in Prostate Cancer</a></li>
<li class="chapter" data-level="1.5.4" data-path="index.html"><a href="index.html#gene-expression-ratios-in-lung-cancer-and-mesothelioma"><i class="fa fa-check"></i><b>1.5.4</b> Gene Expression Ratios in Lung Cancer and Mesothelioma</a></li>
<li class="chapter" data-level="1.5.5" data-path="index.html"><a href="index.html#r-package-quantmod"><i class="fa fa-check"></i><b>1.5.5</b> R package quantmod</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#fundamental-statistical-concepts"><i class="fa fa-check"></i><b>1.6</b> Fundamental statistical concepts</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#sample-and-population"><i class="fa fa-check"></i><b>1.6.1</b> Sample and population</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#models-and-risk"><i class="fa fa-check"></i><b>1.6.2</b> Models and risk</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#estimators-and-associated-variability"><i class="fa fa-check"></i><b>1.6.3</b> Estimators and associated variability</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#simulating-the-population-using-resampling-techniques"><i class="fa fa-check"></i><b>1.6.4</b> Simulating the population using resampling techniques</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#model-selection"><i class="fa fa-check"></i><b>1.6.5</b> Model Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html"><i class="fa fa-check"></i><b>2</b> Assessing the validity of a model</a><ul>
<li class="chapter" data-level="2.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#cross-validation"><i class="fa fa-check"></i><b>2.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="2.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#covariance-penalties-criteria"><i class="fa fa-check"></i><b>2.3</b> Covariance Penalties Criteria</a><ul>
<li class="chapter" data-level="2.3.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-2"><i class="fa fa-check"></i><b>2.3.1</b> Introduction</a></li>
<li class="chapter" data-level="2.3.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mallows-c_p"><i class="fa fa-check"></i><b>2.3.2</b> Mallows <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="2.3.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#efrons-q-class"><i class="fa fa-check"></i><b>2.3.3</b> Efron’s <span class="math inline">\(q\)</span>-class</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#information-theory-and-bayesian-criteria"><i class="fa fa-check"></i><b>2.4</b> Information Theory and Bayesian Criteria</a><ul>
<li class="chapter" data-level="2.4.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#aic-akaike-information-criterion"><i class="fa fa-check"></i><b>2.4.1</b> AIC: Akaike Information Criterion</a></li>
<li class="chapter" data-level="2.4.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#bic-bayesian-information-criterion"><i class="fa fa-check"></i><b>2.4.2</b> BIC: Bayesian Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mean-squared-error-based-criteria"><i class="fa fa-check"></i><b>2.5</b> Mean Squared Error Based Criteria</a><ul>
<li class="chapter" data-level="2.5.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#steins-unbiased-risk-estimator-sure"><i class="fa fa-check"></i><b>2.5.1</b> Stein’s unbiased risk estimator (SURE)</a></li>
<li class="chapter" data-level="2.5.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-focused-information-criterion-fic"><i class="fa fa-check"></i><b>2.5.2</b> The Focused Information Criterion (FIC)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-measures"><i class="fa fa-check"></i><b>2.6</b> Classification measures</a><ul>
<li class="chapter" data-level="2.6.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-logistic-model"><i class="fa fa-check"></i><b>2.6.1</b> The logistic model</a></li>
<li class="chapter" data-level="2.6.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#prediction-error-measures-for-binary-classification"><i class="fa fa-check"></i><b>2.6.2</b> Prediction error measures for Binary classification</a></li>
<li class="chapter" data-level="2.6.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-error-estimation"><i class="fa fa-check"></i><b>2.6.3</b> Classification error estimation</a></li>
<li class="chapter" data-level="2.6.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-roc-curve"><i class="fa fa-check"></i><b>2.6.4</b> The ROC curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html"><i class="fa fa-check"></i><b>3</b> Ordering the variables</a><ul>
<li class="chapter" data-level="3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#stepwise-forward-regression"><i class="fa fa-check"></i><b>3.2</b> Stepwise forward regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#partial-correlations"><i class="fa fa-check"></i><b>3.2.1</b> Partial correlations</a></li>
<li class="chapter" data-level="3.2.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#selection-by-hypothesis-testing"><i class="fa fa-check"></i><b>3.2.2</b> Selection by hypothesis testing</a></li>
<li class="chapter" data-level="3.2.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#orthogonal-matching-pursuit"><i class="fa fa-check"></i><b>3.2.3</b> Orthogonal matching pursuit</a></li>
<li class="chapter" data-level="3.2.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#forward-stagewise-regression"><i class="fa fa-check"></i><b>3.2.4</b> Forward stagewise regression</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#streamwise-regression"><i class="fa fa-check"></i><b>3.3</b> Streamwise regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-4"><i class="fa fa-check"></i><b>3.3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#sure-independence-screening"><i class="fa fa-check"></i><b>3.3.2</b> Sure Independence Screening</a></li>
<li class="chapter" data-level="3.3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#pc-simple-algorithm"><i class="fa fa-check"></i><b>3.3.3</b> PC-simple algorithm</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-and-regression-tree-cart"><i class="fa fa-check"></i><b>3.4</b> Classification And Regression Tree (CART)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#regression-tree"><i class="fa fa-check"></i><b>3.4.1</b> Regression tree</a></li>
<li class="chapter" data-level="3.4.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-trees"><i class="fa fa-check"></i><b>3.4.2</b> Classification Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html"><i class="fa fa-check"></i><b>4</b> Shrinkage Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#ridge-regression"><i class="fa fa-check"></i><b>4.2</b> Ridge regression</a></li>
<li class="chapter" data-level="4.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-lasso-estimator"><i class="fa fa-check"></i><b>4.3</b> The lasso estimator</a></li>
<li class="chapter" data-level="4.4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#alternative-penalized-regression-methods"><i class="fa fa-check"></i><b>4.4</b> Alternative penalized regression methods</a><ul>
<li class="chapter" data-level="4.4.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-adaptive-and-relaxed-lasso"><i class="fa fa-check"></i><b>4.4.1</b> The adaptive and relaxed lasso</a></li>
<li class="chapter" data-level="4.4.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-elastic-net"><i class="fa fa-check"></i><b>4.4.2</b> The elastic net</a></li>
<li class="chapter" data-level="4.4.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-nonnegative-garotte"><i class="fa fa-check"></i><b>4.4.3</b> The nonnegative garotte</a></li>
<li class="chapter" data-level="4.4.4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#non-convex-penalties"><i class="fa fa-check"></i><b>4.4.4</b> Non convex penalties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html"><i class="fa fa-check"></i><b>5</b> Properties of model selection criteria</a><ul>
<li class="chapter" data-level="5.1" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#introduction-6"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#selection-consistency"><i class="fa fa-check"></i><b>5.2</b> Selection consistency</a></li>
<li class="chapter" data-level="5.3" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#selection-efficiency"><i class="fa fa-check"></i><b>5.3</b> Selection efficiency</a></li>
<li class="chapter" data-level="5.4" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#the-oracle-property"><i class="fa fa-check"></i><b>5.4</b> The oracle property</a></li>
<li class="chapter" data-level="5.5" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#probability-of-overfitting"><i class="fa fa-check"></i><b>5.5</b> Probability of overfitting</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="post-selection-inference.html"><a href="post-selection-inference.html"><i class="fa fa-check"></i><b>6</b> Post-Selection Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="post-selection-inference.html"><a href="post-selection-inference.html#introduction-7"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="post-selection-inference.html"><a href="post-selection-inference.html#inference-via-the-nonparametric-bootstrap"><i class="fa fa-check"></i><b>6.2</b> Inference via the nonparametric Bootstrap</a></li>
<li class="chapter" data-level="6.3" data-path="post-selection-inference.html"><a href="post-selection-inference.html#improving-on-the-bootstrap-smoothed-bootstrap-or-bagging"><i class="fa fa-check"></i><b>6.3</b> Improving on the Bootstrap: Smoothed Bootstrap or Bagging</a></li>
<li class="chapter" data-level="6.4" data-path="post-selection-inference.html"><a href="post-selection-inference.html#post-selection-significance-testing"><i class="fa fa-check"></i><b>6.4</b> Post selection significance testing</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i><b>7</b> Solutions</a><ul>
<li class="chapter" data-level="7.1" data-path="solutions.html"><a href="solutions.html#chapter-1"><i class="fa fa-check"></i><b>7.1</b> Chapter 1</a><ul>
<li class="chapter" data-level="7.1.1" data-path="solutions.html"><a href="solutions.html#zam"><i class="fa fa-check"></i><b>7.1.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="7.1.2" data-path="solutions.html"><a href="solutions.html#leuk"><i class="fa fa-check"></i><b>7.1.2</b> Prognostic Factors in Childhood Leukemia</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="solutions.html"><a href="solutions.html#chapter-2"><i class="fa fa-check"></i><b>7.2</b> Chapter 2</a><ul>
<li class="chapter" data-level="7.2.1" data-path="solutions.html"><a href="solutions.html#cv"><i class="fa fa-check"></i><b>7.2.1</b> Cross-validation</a></li>
<li class="chapter" data-level="7.2.2" data-path="solutions.html"><a href="solutions.html#aic"><i class="fa fa-check"></i><b>7.2.2</b> Akaike Information Criterion</a></li>
<li class="chapter" data-level="7.2.3" data-path="solutions.html"><a href="solutions.html#roc"><i class="fa fa-check"></i><b>7.2.3</b> ROC curves</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="solutions.html"><a href="solutions.html#chapter-3"><i class="fa fa-check"></i><b>7.3</b> Chapter 3</a><ul>
<li class="chapter" data-level="7.3.1" data-path="solutions.html"><a href="solutions.html#HT"><i class="fa fa-check"></i><b>7.3.1</b> Selection by Hypothesis Testing</a></li>
<li class="chapter" data-level="7.3.2" data-path="solutions.html"><a href="solutions.html#SIS"><i class="fa fa-check"></i><b>7.3.2</b> Sure Independence Screening</a></li>
<li class="chapter" data-level="7.3.3" data-path="solutions.html"><a href="solutions.html#PC"><i class="fa fa-check"></i><b>7.3.3</b> PC-simple algorithm</a></li>
<li class="chapter" data-level="7.3.4" data-path="solutions.html"><a href="solutions.html#RT"><i class="fa fa-check"></i><b>7.3.4</b> Regression Tree</a></li>
<li class="chapter" data-level="7.3.5" data-path="solutions.html"><a href="solutions.html#CT"><i class="fa fa-check"></i><b>7.3.5</b> Classification Tree</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="solutions.html"><a href="solutions.html#chapter-4"><i class="fa fa-check"></i><b>7.4</b> Chapter 4</a><ul>
<li class="chapter" data-level="7.4.1" data-path="solutions.html"><a href="solutions.html#Ridge"><i class="fa fa-check"></i><b>7.4.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="solutions.html"><a href="solutions.html#lasso"><i class="fa fa-check"></i><b>7.4.2</b> Lasso</a></li>
<li class="chapter" data-level="7.4.3" data-path="solutions.html"><a href="solutions.html#nonconvex"><i class="fa fa-check"></i><b>7.4.3</b> Non Convex Penalties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Model Selection in High Dimensions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="solutions" class="section level1">
<h1><span class="header-section-number">7</span> Solutions</h1>
<div id="chapter-1" class="section level2">
<h2><span class="header-section-number">7.1</span> Chapter 1</h2>
<div id="zam" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Data on Malnutrition in Zambia</h3>
<blockquote>
<ul>
<li>Load the dataset and build the variables so that they can be used for a regression analysis.</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(foreign)  <span class="co"># install foreign package if you do not have it yet</span>

<span class="co"># See section 1.6.2 e-book for information on the dataset.</span>

<span class="co"># dat = read.spss(&quot;Zambia.SAV&quot;, add.undeclared.levels = &quot;no&quot;)</span>

dat =<span class="st"> </span><span class="kw">read.spss</span>(<span class="st">&quot;Zambia.SAV&quot;</span>)

<span class="co"># Construct system matrix</span>

<span class="co"># The idea behind this exercise is to be aware that data cleaning is most of the times the real issue </span>
<span class="co"># with a real problem. It is sensitive to say that 80% of the work is cleaning and only 20% is modeling.</span>

<span class="co"># Extract response variable i.e. HW70 Height for age standard deviation (according to WHO)</span>
y =<span class="st"> </span>dat<span class="op">$</span>HW70
y[y <span class="op">==</span><span class="st"> </span><span class="dv">9996</span>] =<span class="st"> </span><span class="ot">NA</span>
y[y <span class="op">==</span><span class="st"> </span><span class="dv">9997</span>] =<span class="st"> </span><span class="ot">NA</span>
y[y <span class="op">==</span><span class="st"> </span><span class="dv">9998</span>] =<span class="st"> </span><span class="ot">NA</span>
y[y <span class="op">==</span><span class="st"> </span><span class="dv">9999</span>] =<span class="st"> </span><span class="ot">NA</span>

<span class="co"># Revert tranformation (i.e. z-score)</span>
y =<span class="st"> </span>y<span class="op">/</span><span class="dv">100</span>

<span class="co"># Variable 1: The calculated months of breastfeeding gives the duration of breastfeeding</span>
x1 =<span class="st"> </span>dat<span class="op">$</span>M5
x1[x1 <span class="op">==</span><span class="st"> </span><span class="dv">94</span>] =<span class="st"> </span><span class="dv">0</span>
x1[x1 <span class="op">==</span><span class="st"> </span><span class="dv">97</span>] =<span class="st"> </span><span class="ot">NA</span>
x1[x1 <span class="op">==</span><span class="st"> </span><span class="dv">98</span>] =<span class="st"> </span><span class="ot">NA</span>
x1[x1 <span class="op">==</span><span class="st"> </span><span class="dv">99</span>] =<span class="st"> </span><span class="ot">NA</span>
x1[x1 <span class="op">&gt;</span><span class="st"> </span><span class="dv">40</span>] =<span class="st"> </span><span class="ot">NA</span>

<span class="co"># Variable 2: Age in months of the child</span>
x2 =<span class="st"> </span>dat<span class="op">$</span>HW1

<span class="co"># Variable 3: Age of the mother at birth</span>
x3 =<span class="st"> </span>dat<span class="op">$</span>V012 <span class="op">-</span><span class="st"> </span>dat<span class="op">$</span>B8
x3[x3<span class="op">&gt;</span><span class="dv">45</span>] =<span class="st"> </span><span class="ot">NA</span>

<span class="co"># Variable 4: Body mass index (BMI) of the mother</span>
x4 =<span class="st"> </span>dat<span class="op">$</span>V445

x4 =<span class="st"> </span>x4<span class="op">/</span><span class="dv">100</span>  <span class="co"># no sense without this division</span>

<span class="co"># Variable 5: Height of the mother in meters</span>
x5 =<span class="st"> </span>dat<span class="op">$</span>V438
x5[x5 <span class="op">==</span><span class="st"> </span><span class="dv">9998</span>] =<span class="st"> </span><span class="ot">NA</span>
x5[x5 <span class="op">==</span><span class="st"> </span><span class="dv">9999</span>] =<span class="st"> </span><span class="ot">NA</span>
x5[x5 <span class="op">&lt;</span><span class="st"> </span><span class="dv">1300</span>] =<span class="st"> </span><span class="ot">NA</span>
x5[x5 <span class="op">&gt;</span><span class="st"> </span><span class="dv">1900</span>] =<span class="st"> </span><span class="ot">NA</span>

x5 =<span class="st"> </span>x5<span class="op">/</span><span class="dv">1000</span>  <span class="co"># it was in mm, we need to transform from original</span>

<span class="co"># Variable 6: Weight of the mother in kilograms</span>
x6 =<span class="st"> </span>dat<span class="op">$</span>V437

x6=x6<span class="op">/</span><span class="dv">10</span> <span class="co"># we need to go back to Kg</span>

<span class="co"># Variable 7: De facto region of residence</span>

<span class="co"># Creating dummies (i.e. indicator functions) for each level of an existing factor enables</span>
<span class="co"># to check the coefficients of each level in a possible future model estimation</span>

x7 =<span class="st"> </span><span class="kw">as.factor</span>(dat<span class="op">$</span>V101)


x7 =<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>x7<span class="op">-</span><span class="dv">1</span>)

<span class="kw">dim</span>(x7)

<span class="co"># Variable 8: Mother highest education level attended</span>
x8 =<span class="st"> </span><span class="kw">as.factor</span>(dat<span class="op">$</span>V106)
x8 =<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>x8<span class="op">-</span><span class="dv">1</span>)

<span class="kw">dim</span>(x8)

<span class="co"># Variable 9: Wealth index factor score</span>
x9 =<span class="st"> </span>dat<span class="op">$</span>V191

<span class="co"># Variable 10: Weight of child at birth given in kilograms with three implied decimal places</span>
x10 =<span class="st"> </span>dat<span class="op">$</span>M19
x10[x10 <span class="op">==</span><span class="st"> </span><span class="dv">9996</span>] =<span class="st"> </span><span class="ot">NA</span>
x10[x10 <span class="op">==</span><span class="st"> </span><span class="dv">9997</span>] =<span class="st"> </span><span class="ot">NA</span>
x10[x10 <span class="op">==</span><span class="st"> </span><span class="dv">9998</span>] =<span class="st"> </span><span class="ot">NA</span>
x10[x10 <span class="op">==</span><span class="st"> </span><span class="dv">9999</span>] =<span class="st"> </span><span class="ot">NA</span>
x10 =<span class="st"> </span>x10<span class="op">/</span><span class="dv">1000</span>

<span class="co"># Variable 11: Child Sex</span>
x11 =<span class="st"> </span>dat<span class="op">$</span>B4

<span class="co"># Variable 12: Preceding birth interval is calculated as the difference in months between the current birth and the previous birth</span>
x12 =<span class="st"> </span>dat<span class="op">$</span>B11
x12[x12 <span class="op">&gt;</span><span class="st"> </span><span class="dv">125</span>] =<span class="st"> </span><span class="ot">NA</span>

<span class="co"># Variable 13: Drinking Water</span>
x13 =<span class="st"> </span>dat<span class="op">$</span>V113
x13 =<span class="st"> </span><span class="kw">model.matrix</span>(<span class="op">~</span>x13<span class="op">-</span><span class="dv">1</span>)
x13 =<span class="st"> </span>x13[,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">13</span>,<span class="dv">17</span>,<span class="dv">18</span>)]

<span class="kw">dim</span>(x13)

<span class="kw">levels</span>(x13)

mat.sys =<span class="st"> </span><span class="kw">na.omit</span>(<span class="kw">cbind</span>(y,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13))
<span class="kw">dim</span>(mat.sys)[<span class="dv">2</span>]


<span class="co"># Number of regressor</span>
p =<span class="st"> </span><span class="kw">dim</span>(mat.sys)[<span class="dv">2</span>]

<span class="co"># Construct X and Y</span>
y =<span class="st"> </span>mat.sys[,<span class="dv">1</span>]
X =<span class="st"> </span>mat.sys[,<span class="dv">2</span><span class="op">:</span>p]

<span class="co"># Create a dataframe</span>

data_zambia =<span class="st"> </span><span class="kw">cbind</span>(y,X)

data_zambia =<span class="st"> </span><span class="kw">data.frame</span>(data_zambia)</code></pre></div>
<blockquote>
<ul>
<li>Associate proper names to each variable (hint: look at the previous comments in the r chunk).</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colnames</span>(data_zambia) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Height for age sd&quot;</span>, <span class="st">&quot;Breastfeeding duration (months)&quot;</span>,<span class="st">&quot;Age of the child (months)&quot;</span>, <span class="st">&quot;Age of the mother (years)&quot;</span>, <span class="st">&quot;BMI mother&quot;</span>, <span class="st">&quot;Heigth mother (meter)&quot;</span>, <span class="st">&quot;Weight mother (kg)&quot;</span>, <span class="st">&quot;Region:Central&quot;</span>, <span class="st">&quot;Region:Copperbelt&quot;</span>, <span class="st">&quot;Region:Eastern&quot;</span>, <span class="st">&quot;Region:Luapula&quot;</span>, <span class="st">&quot;Region:Lusaka&quot;</span>, <span class="st">&quot;Region:Northern&quot;</span>, <span class="st">&quot;Region:Northwestern&quot;</span>, <span class="st">&quot;Region:Southern&quot;</span>, <span class="st">&quot;Region:Western&quot;</span>, <span class="st">&quot;Ed:No education&quot;</span>, <span class="st">&quot;Ed:Primary&quot;</span>, <span class="st">&quot;Ed:Secondary&quot;</span>, <span class="st">&quot;Ed:Higher&quot;</span>, <span class="st">&quot;Wealth index factor score&quot;</span>, <span class="st">&quot;Child weight at birth (kg)&quot;</span>, <span class="st">&quot;Child sex&quot;</span>, <span class="st">&quot;Interval between births&quot;</span>,<span class="st">&quot;Water:Piped into dwelling&quot;</span>, <span class="st">&quot;Water:Piped to yard/plot&quot;</span>, <span class="st">&quot;Water:Public tap/standpipe&quot;</span>, <span class="st">&quot;Water:Protected well&quot;</span>, <span class="st">&quot;Water:Unprotected well&quot;</span>, <span class="st">&quot;Water:River/dam/lake/ponds/stream/canal/irrigation channel&quot;</span>, <span class="st">&quot;Water:Bottled water&quot;</span>, <span class="st">&quot;Water:Other&quot;</span>)</code></pre></div>
<blockquote>
<ul>
<li>Perform a linear regression on all the available variables.</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">attach</span>(data_zambia)

lm_zambia =<span class="st"> </span><span class="kw">lm</span>(<span class="st">`</span><span class="dt">Height for age sd</span><span class="st">`</span> <span class="op">~</span><span class="st"> </span>. <span class="op">-</span><span class="st">`</span><span class="dt">Region:Central</span><span class="st">`</span><span class="op">-</span><span class="st"> `</span><span class="dt">Ed:No education</span><span class="st">`</span>, <span class="dt">data =</span> data_zambia)

<span class="co"># We take off two levels to avoid multicollinearity. This should always be done when you create dummies.</span>
<span class="kw">summary</span>(lm_zambia) <span class="co"># read the output understand the benchmark of the factor</span>

lm_zambia_full =<span class="st"> </span><span class="kw">lm</span>(<span class="st">`</span><span class="dt">Height for age sd</span><span class="st">`</span> <span class="op">~</span><span class="st"> </span>. , <span class="dt">data =</span> data_zambia)

<span class="kw">summary</span>(lm_zambia_full) <span class="co">#here it is R who choses the benchmark for the factors (i.e. NA variables)</span>

<span class="kw">detach</span>(data_zambia)</code></pre></div>
<blockquote>
<ul>
<li>Reduce the number of covariates (e.g. using the <em>t</em>-test) and add some interactions. Perform a linear regression on the new dataset.</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">attach</span>(data_zambia)

<span class="co"># Eliminate variables with t-test in a stepwise manner (fixed alfa = 0.05 in this case)</span>

model_zambia_reduced =<span class="st"> </span><span class="kw">lm</span>(<span class="st">`</span><span class="dt">Height for age sd</span><span class="st">`</span> <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> data_zambia[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">9</span><span class="op">:</span><span class="dv">16</span>,<span class="dv">21</span><span class="op">:</span><span class="dv">23</span>)])


<span class="kw">summary</span>(model_zambia_reduced) <span class="co"># notice what is happening to the age of the mother variable </span>


<span class="co"># Introduce one interaction in the reduced model. We start with the childsex factor.</span>

model_zambia_int =<span class="st"> </span><span class="kw">lm</span>(<span class="st">`</span><span class="dt">Height for age sd</span><span class="st">`</span> <span class="op">~</span><span class="st"> </span>. <span class="op">+</span><span class="st"> `</span><span class="dt">Breastfeeding duration (months)</span><span class="st">`</span><span class="op">*</span><span class="st">`</span><span class="dt">Child sex</span><span class="st">`</span>, <span class="dt">data =</span> data_zambia[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">9</span><span class="op">:</span><span class="dv">16</span>,<span class="dv">21</span><span class="op">:</span><span class="dv">23</span>)])


<span class="kw">summary</span>(model_zambia_int) <span class="co">#We take out the interaction from the model as it is not significant</span>

#### Remember: the hierarchical effect states that anytime you add an interaction also the marginal effects

#### should be part of your model

<span class="kw">detach</span>(data_zambia)</code></pre></div>
<blockquote>
<ul>
<li>Other available procedures for a first model selection in this specific case:</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># (1) VIF (variance inflation factor) for avoiding multicollinearity, </span>

<span class="co"># (2) Automatic Stepwise procedures (e.g. forward and backward) </span>

<span class="co"># (3) Exhaustive search (See practical 3 exercises)</span>

<span class="co"># Example with an automatic stepwise procedure</span>

<span class="kw">help</span>(<span class="st">&quot;step&quot;</span>)

stepwise_procedue =<span class="st"> </span><span class="kw">step</span>(lm_zambia_full,<span class="dt">direction =</span> <span class="st">&quot;backward&quot;</span>) <span class="co">#or forward</span>

<span class="co"># This procedure evaluates, given a criterion, a sequence of variables stopping when</span>
<span class="co"># the criterion is increasing</span></code></pre></div>
<blockquote>
<ul>
<li>Analyse your chosen estimated model with a residual analysis (e.g. residuals vs fitted plot, normal QQ plot etc.).</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Validate your model looking at residuals vs fitted plot and normal QQ plot</span>

<span class="kw">plot</span>(model_zambia_reduced, <span class="dt">which =</span> <span class="dv">1</span>)  <span class="co"># Residuals vs fitted: no particular structure</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(model_zambia_reduced, <span class="dt">which =</span> <span class="dv">2</span>) </code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-28-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Normal QQ plot: We observe right tail which is not compatible with a normal assumption</span></code></pre></div>
</div>
<div id="leuk" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Prognostic Factors in Childhood Leukemia</h3>
<blockquote>
<p>Exercises</p>
</blockquote>
<ul>
<li>Load the data from the URL <a href="http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv" class="uri">http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv</a></li>
</ul>
<div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">leukemia_big &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv&quot;</span>)</code></pre></div>
</div>
<ul>
<li>Create the response variable y according to the number of ALL and AML patients. In the same fashion create the matrix X of independent variables.</li>
</ul>
<p>See <a href="https://web.stanford.edu/~hastie/CASI_files/DATA/leukemia.html" class="uri">https://web.stanford.edu/~hastie/CASI_files/DATA/leukemia.html</a> for further details.</p>
<div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">leukemia_mat =<span class="st"> </span><span class="kw">as.matrix</span>(leukemia_big)

<span class="kw">dim</span>(leukemia_mat)

leukemia_mat =<span class="st"> </span><span class="kw">t</span>(leukemia_mat) <span class="co">#this is the design matrix for the analysis</span>

<span class="co"># Generate the 0 and 1 values for the two different categories: there are 20 ALL, 14 AML, 27 ALL and</span>

<span class="co"># 11 AML for a total of 47 ALL and 25 AML.</span>

<span class="co"># Given the above excerpt from the cancer society, I have decided to code ALL as 1 and AML as 0 since</span>

<span class="co"># doctors are interested in knowing the characteristics which differentiate ALL from AML in order to</span>

<span class="co"># understand if we can use standard treatment or a more aggressive one.</span>

y =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">20</span>),<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">14</span>), <span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">27</span>), <span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">11</span>))  <span class="co">#the response vector</span>

<span class="kw">length</span>(y)

X =<span class="st"> </span>leukemia_mat

<span class="kw">dim</span>(X)</code></pre></div>
</div>
<ul>
<li>Choose the correct exponential family for this situation and perform a GLM on the data. Comment on the results that you obtain.</li>
</ul>
<!-- Since p>>n there are problems -->
<div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_glm =<span class="st"> </span><span class="kw">glm</span>(<span class="dt">formula =</span> y <span class="op">~</span><span class="st"> </span>X,<span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)


<span class="kw">summary</span>(model_glm)  <span class="co">#singularity issues in the IWLS algorithm of GLM. It is impossible to invert the matrix.</span>

<span class="co"># The binary Lasso is a possible way to solve the issue and have an actual estimate. See glmnet package.</span></code></pre></div>
</div>
</div>
</div>
<div id="chapter-2" class="section level2">
<h2><span class="header-section-number">7.2</span> Chapter 2</h2>
<div id="cv" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Cross-validation</h3>
<p>(solutions provided by <strong>Alexander Maslev</strong>, <strong>Hanxiong Wang</strong> and <strong>Minyoung Lee</strong>). </p>
<p>Program k-fold Cross-Validation (with k=2) and do model selection in a specific simulation setting with an exhaustive search. Follow these steps:</p>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Generate from a MVN (multivariate normal) a matrix <span class="math inline">\(\mathbf{X_{n*p}}\)</span> with <span class="math inline">\(n = 1000\)</span> and <span class="math inline">\(p = 5\)</span>. You can choose the location vector as you wish but set the scale matrix as the identity.<br />
</li>
</ol>
</blockquote>
<p>We have chosen the location vector [2,4,6,8,10] and the scale matrix as the identity.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n&lt;-<span class="dv">1000</span>
p&lt;-<span class="dv">5</span>
Mu&lt;-<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">8</span>,<span class="dv">10</span>) <span class="co"># location vector</span>
sigma&lt;-<span class="kw">diag</span>(<span class="dv">5</span>) <span class="co"># scale matrix as the identity</span>
X&lt;-<span class="kw">mvrnorm</span>(n , Mu, sigma)</code></pre></div>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Choose the generating vector <span class="math inline">\(\boldsymbol{\beta }= [3 \; 1.5 \; 0 \; 2 \; 0]\)</span> and retrieve the signal to noise ratio of this setting.<br />
</li>
</ol>
</blockquote>
<p>We found in Wikipedia that the statistical definition of SNR is the reciprocal of the coefficient of variation (i.e. the ratio of mean to standard deviation of a signal or measurement) : <span class="math display">\[SNR = \frac{\mu}{\sigma}\]</span> where <span class="math inline">\(\mu\)</span> is the signal mean or expected value and <span class="math inline">\(\sigma\)</span> is the standard deviation of the noise.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">beta&lt;-<span class="kw">c</span>(<span class="dv">3</span>,<span class="fl">1.5</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>)
e&lt;-<span class="kw">rnorm</span>(n, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)

SNR&lt;-<span class="kw">mean</span>(X<span class="op">%*%</span>beta)<span class="op">/</span><span class="kw">sqrt</span>(<span class="kw">var</span>(e))

SNR</code></pre></div>
<pre><code>## [1] 27.90095</code></pre>
<p>In the engineering literature, there is an alternative definition:</p>
<p><span class="math display">\[SNR_{eng} = \frac{Var(f(x))}{Var(\epsilon)}\]</span> where <span class="math inline">\(f(x)\)</span> is the chosen prediction rule (e.g. linear function in the OLS case) and <span class="math inline">\(\epsilon\)</span> is the noise. You can fix in advance your SNR with the following code and generate the data accordingly:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">signal_to_noise_ratio =<span class="st"> </span><span class="co">#number to fix as you wish#</span>

data =<span class="st"> </span>X<span class="op">%*%</span>beta

noise =<span class="st"> </span><span class="kw">rnorm</span>(n,<span class="dv">0</span>,<span class="dv">1</span>)

k =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">var</span>(data)<span class="op">/</span>(signal_to_noise_ratio<span class="op">*</span><span class="kw">var</span>(noise)))


y_hat_eng =<span class="st"> </span>data <span class="op">+</span><span class="st"> </span>k<span class="op">*</span>noise <span class="co"># how you generate data to retrieve your fixed signal to noise ratio</span></code></pre></div>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Generate <span class="math inline">\(\hat{\mathbf{y}}\)</span> thanks to the relation <span class="math inline">\(\mathbf{y} = \mathbf{X_{n*p}} \; \boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span> where <span class="math inline">\(\epsilon_{i}\)</span> is a standard normal, <span class="math inline">\(n = 1000\)</span> and <span class="math inline">\(p = 5\)</span>. Suppose for simplicity that the errors are uncorrelated.<br />
</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Y_hat&lt;-X<span class="op">%*%</span>beta<span class="op">+</span>e</code></pre></div>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li>Split the data randomly in two halves (k=2) and use the training set to determine <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span>. Then, compute the squared loss function as prediction error measure for each possible model. Observe which model is the best model.<br />
</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>n, <span class="dt">size=</span><span class="fl">0.5</span><span class="op">*</span>n)

<span class="co"># Split data</span>
y_train&lt;-<span class="st"> </span>Y_hat[<span class="op">-</span>index,]
x_train&lt;-X[<span class="op">-</span>index,]
y_test&lt;-<span class="st"> </span>Y_hat[index,]
x_test&lt;-X[index,]

index_sub_choose&lt;-<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span>p)
sub_matrix &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>,<span class="dt">ncol =</span> p,<span class="dt">nrow =</span> <span class="dv">2</span><span class="op">^</span>p<span class="op">-</span><span class="dv">1</span>)
t=<span class="dv">0</span>
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>)
{
  index_matrix &lt;-<span class="st"> </span><span class="kw">combn</span>(index_sub_choose,i)
  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(index_matrix))
  {
    t &lt;-<span class="st"> </span>t<span class="op">+</span><span class="dv">1</span>
    index_sub &lt;-<span class="st"> </span>index_matrix[,j]
    sub_matrix[t,<span class="kw">c</span>(index_sub)]  &lt;-<span class="st">  </span><span class="dv">1</span>
  }
}

k&lt;-<span class="kw">nrow</span>(sub_matrix)
cv &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data=</span><span class="ot">NA</span>,<span class="dt">nrow =</span> k,<span class="dt">ncol =</span> <span class="dv">1</span>)
<span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k){
  Xsub    &lt;-x_train[,<span class="kw">which</span>(sub_matrix[j,]<span class="op">==</span><span class="dv">1</span>)]
  betaMLE &lt;-<span class="kw">solve</span>(<span class="kw">t</span>(Xsub)<span class="op">%*%</span>Xsub)<span class="op">%*%</span><span class="kw">t</span>(Xsub)<span class="op">%*%</span>y_train
  new_Y   &lt;-x_test[,<span class="kw">which</span>(sub_matrix[j,]<span class="op">==</span><span class="dv">1</span>)]<span class="op">%*%</span>betaMLE
  cv[j,]      &lt;-<span class="st"> </span><span class="kw">t</span>(y_test<span class="op">-</span>new_Y)<span class="op">%*%</span>(y_test<span class="op">-</span>new_Y)
  
}

BEST_cv&lt;-<span class="kw">which</span>(sub_matrix[<span class="kw">which.min</span>(cv),]<span class="op">==</span><span class="dv">1</span>)
BEST_cv</code></pre></div>
<pre><code>## [1] 1 2 4 5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Xsub_cv&lt;-x_train[,BEST_cv]
betaMLE_cv&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(Xsub_cv)<span class="op">%*%</span>Xsub_cv)<span class="op">%*%</span><span class="kw">t</span>(Xsub_cv)<span class="op">%*%</span>y_train
betaMLE_cv</code></pre></div>
<pre><code>##             [,1]
## [1,]  3.03433961
## [2,]  1.51075393
## [3,]  2.01760617
## [4,] -0.02911818</code></pre>
<p>Each time it changes but most of the times we obtain the best model when p=5. The beta sometimes is not close to generating vector <span class="math inline">\(\beta\)</span> = [3 1.5 0 2 0].</p>
<blockquote>
<ol start="5" style="list-style-type: lower-alpha">
<li>Suppose now that we increase the size of <span class="math inline">\(\boldsymbol{\beta}\)</span> to 100 (i.e. <span class="math inline">\(p = 100\)</span> ). Calculate the number of possible models to evaluate together with an estimate of the time needed for an exhaustive search (<em>hint: use previous results</em>). Conclude on the feasibility of this task.</li>
</ol>
</blockquote>
<pre><code>## [1] 1.267651e+30</code></pre>
<pre><code>## [1] 6.915153e+27</code></pre>
<p>When we run a CV process with p=5, it takes 0.1691079 seconds. For p = 5, we can have 31 different models thanks to Newton’s binomial theorem (i.e. <span class="math inline">\(2^p - 1\)</span>). Unfortunately when we increase p to 100, we have 1.26e+30 different models. Thus it will take approximately 1.9e+24 hours. This is the case when we do k=2 cross validation. Moreover if we increase the number of k, we will drastically increase the time needed. This task is not feasible.</p>
</div>
<div id="aic" class="section level3">
<h3><span class="header-section-number">7.2.2</span> Akaike Information Criterion</h3>
<p>(solutions provided by <strong>Alexander Maslev</strong>, <strong>Hanxiong Wang</strong> and <strong>Minyoung Lee</strong>). </p>
<blockquote>
<ul>
<li>Program AIC and do model selection in a specific simulation setting with an exhaustive search (follow the passages listed in the CV exercise section).<br />
</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n&lt;-<span class="dv">1000</span>
p&lt;-<span class="dv">5</span>
Mu&lt;-<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">8</span>,<span class="dv">10</span>) <span class="co"># location vector</span>
sigma&lt;-<span class="kw">diag</span>(<span class="dv">5</span>) <span class="co"># scale matrix as the identity</span>
X&lt;-<span class="kw">mvrnorm</span>(n , Mu, sigma)

beta&lt;-<span class="kw">c</span>(<span class="dv">3</span>,<span class="fl">1.5</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>)
e&lt;-<span class="kw">rnorm</span>(n, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)
SNR&lt;-<span class="kw">mean</span>(X<span class="op">%*%</span>beta)<span class="op">/</span><span class="kw">var</span>(e)

Y_hat&lt;-X<span class="op">%*%</span>beta<span class="op">+</span>e</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">index_sub_choose&lt;-<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)
sub_matrix &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> <span class="ot">NA</span>,<span class="dt">ncol =</span> <span class="dv">5</span>,<span class="dt">nrow =</span> <span class="dv">31</span>)
t=<span class="dv">0</span>
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>)
{
  index_matrix &lt;-<span class="st"> </span><span class="kw">combn</span>(index_sub_choose,i)
  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">ncol</span>(index_matrix))
  {
    t &lt;-<span class="st"> </span>t<span class="op">+</span><span class="dv">1</span>
    index_sub &lt;-<span class="st"> </span>index_matrix[,j]
    sub_matrix[t,<span class="kw">c</span>(index_sub)]  &lt;-<span class="st">  </span><span class="dv">1</span>
  }
}

<span class="co"># AIC</span>
RSS&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,k)
AIC&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,k)
k&lt;-<span class="kw">nrow</span>(sub_matrix)
<span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k){
Xsub&lt;-<span class="kw">as.matrix</span>(X[,<span class="kw">which</span>(sub_matrix[j,]<span class="op">==</span><span class="dv">1</span>)])
betaMLE&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(Xsub)<span class="op">%*%</span>Xsub)<span class="op">%*%</span><span class="kw">t</span>(Xsub)<span class="op">%*%</span>Y_hat
new_Y&lt;-Xsub<span class="op">%*%</span>betaMLE
<span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(n<span class="op">/</span><span class="dv">2</span>)){
  RSS[j]&lt;-RSS[j]<span class="op">+</span>(new_Y[i]<span class="op">-</span>Y_hat[i])<span class="op">^</span><span class="dv">2</span>
}
AIC[j]&lt;-RSS[j]<span class="op">/</span><span class="kw">var</span>(e)<span class="op">+</span><span class="dv">2</span><span class="op">*</span><span class="kw">ncol</span>(Xsub)
}
BEST&lt;-<span class="kw">which</span>(sub_matrix[<span class="kw">which.min</span>(AIC),]<span class="op">==</span><span class="dv">1</span>)
BEST</code></pre></div>
<pre><code>## [1] 1 2 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Xsub&lt;-<span class="kw">as.matrix</span>(X[,BEST])
betaMLE&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(Xsub)<span class="op">%*%</span>Xsub)<span class="op">%*%</span><span class="kw">t</span>(Xsub)<span class="op">%*%</span>Y_hat
betaMLE</code></pre></div>
<pre><code>##          [,1]
## [1,] 2.982799
## [2,] 1.440587
## [3,] 2.035385</code></pre>
<p>Each time it changes but most of the times we obtain the best model when p=4 in position[1 2 4 5]. The estimated beta is very close to generating vector <span class="math inline">\(\beta\)</span> = [3 1.5 0 2 0].</p>
<p>The above results hold not considering <span class="math inline">\(\sigma\)</span> and the intercept as parameters. In the general formulation of the Akaike’s information criterion, <span class="math inline">\(p = dim(\Theta)\)</span> where <span class="math inline">\(\Theta\)</span> is the parameters space.</p>
<p>As we computed in point (e) in CV exercise, when p = 100, we have 1.26e+30 different model. The time needed to run the AIC process with p = 5 takes 0.2441621 seconds. When we calculate the time that we need to run when p = 100, we obtain approximately 2.7e+24 hours. Thus we still think that the task to do all the combinations when p = 100 is not feasible.</p>
<pre><code>## [1] 1.267651e+30</code></pre>
<pre><code>## [1] 9.984266e+27</code></pre>
<pre><code>## [1] 2.773407e+24</code></pre>
<blockquote>
<ul>
<li>Compare the performance of your programmed CV and AIC by replicating 100 times the tasks. In particular you should evaluate three specific criteria: the proportion of times the correct model is selected (<em>Exact</em>), the proportion of times the selected model contains the correct one (<em>Correct</em>) and the average number of selected regressors (<em>Average <span class="math inline">\(\sharp\)</span></em>)<br />
</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Rep 100 times AIC

cv &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data=</span><span class="ot">NA</span>,<span class="dt">nrow =</span> k,<span class="dt">ncol =</span> <span class="dv">1</span>)
Exact&lt;-<span class="kw">data.frame</span>(<span class="kw">t</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>)))
<span class="kw">colnames</span>(Exact)&lt;-<span class="kw">c</span>(<span class="st">&quot;AIC&quot;</span>,<span class="st">&quot;CV&quot;</span>)
AverageN&lt;-<span class="kw">matrix</span>(<span class="ot">NA</span>,<span class="dt">nrow=</span><span class="dv">100</span>,<span class="dt">ncol =</span> <span class="dv">2</span>)
<span class="kw">colnames</span>(AverageN)&lt;-<span class="kw">c</span>(<span class="st">&quot;AIC&quot;</span>,<span class="st">&quot;CV&quot;</span>)
Correct&lt;-<span class="kw">data.frame</span>(<span class="kw">t</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>)))
<span class="kw">colnames</span>(Correct)&lt;-<span class="kw">c</span>(<span class="st">&quot;AIC&quot;</span>,<span class="st">&quot;CV&quot;</span>)

<span class="cf">for</span>(l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>){
  
### SETTING <span class="al">###</span>
  n&lt;-<span class="dv">1000</span>
  p&lt;-<span class="dv">5</span>
  Mu&lt;-<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>,<span class="dv">8</span>,<span class="dv">10</span>) <span class="co"># location vector</span>
  sigma&lt;-<span class="kw">diag</span>(<span class="dv">5</span>) <span class="co"># scale matrix as the identity</span>
  X&lt;-<span class="kw">mvrnorm</span>(n , Mu, sigma)
  beta&lt;-<span class="kw">c</span>(<span class="dv">3</span>,<span class="fl">1.5</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>)
  e&lt;-<span class="kw">rnorm</span>(n, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)
  Y_hat&lt;-X<span class="op">%*%</span>beta<span class="op">+</span>e

### AIC <span class="al">###</span>    
  RSS&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,k)
  AIC&lt;-<span class="kw">rep</span>(<span class="dv">0</span>,k)

  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k){
    Xsub&lt;-<span class="kw">as.matrix</span>(X[,<span class="kw">which</span>(sub_matrix[j,]<span class="op">==</span><span class="dv">1</span>)])
    betaMLE&lt;-<span class="kw">solve</span>(<span class="kw">t</span>(Xsub)<span class="op">%*%</span>Xsub)<span class="op">%*%</span><span class="kw">t</span>(Xsub)<span class="op">%*%</span>Y_hat
    new_Y&lt;-Xsub<span class="op">%*%</span>betaMLE
    <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(n<span class="op">/</span><span class="dv">2</span>)){
      RSS[j]&lt;-RSS[j]<span class="op">+</span>(new_Y[i]<span class="op">-</span>Y_hat[i])<span class="op">^</span><span class="dv">2</span>
    }
    AIC[j]&lt;-RSS[j]<span class="op">/</span><span class="kw">var</span>(e)<span class="op">+</span><span class="dv">2</span><span class="op">*</span><span class="kw">ncol</span>(Xsub)
  }
  BEST&lt;-sub_matrix[<span class="kw">which.min</span>(AIC),]
  BEST[<span class="kw">is.na</span>(BEST)] &lt;-<span class="dv">0</span>
  <span class="cf">if</span>(<span class="kw">sum</span>(BEST<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>))<span class="op">==</span><span class="dv">0</span>){
    
    Exact[<span class="dv">1</span>]&lt;-Exact[<span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>
    
  }
  <span class="cf">if</span>(<span class="kw">sum</span>((BEST[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>)]<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)))<span class="op">==</span><span class="dv">0</span>){
    Correct[<span class="dv">1</span>]&lt;-Correct[<span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span>
  }
    AverageN[l,<span class="dv">1</span>]&lt;-<span class="kw">sum</span>(BEST)
  
  
### CV <span class="al">###</span>
  index &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>n, <span class="dt">size=</span><span class="fl">0.5</span><span class="op">*</span>n)
  y_train&lt;-<span class="st"> </span>Y_hat[<span class="op">-</span>index,]
  x_train&lt;-X[<span class="op">-</span>index,]
  y_test&lt;-<span class="st"> </span>Y_hat[index,]
  x_test&lt;-X[index,]
  
  <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k){
    Xsub    &lt;-x_train[,<span class="kw">which</span>(sub_matrix[j,]<span class="op">==</span><span class="dv">1</span>)]
    betaMLE &lt;-<span class="kw">solve</span>(<span class="kw">t</span>(Xsub)<span class="op">%*%</span>Xsub)<span class="op">%*%</span><span class="kw">t</span>(Xsub)<span class="op">%*%</span>y_train
    new_Y   &lt;-x_test[,<span class="kw">which</span>(sub_matrix[j,]<span class="op">==</span><span class="dv">1</span>)]<span class="op">%*%</span>betaMLE
    cv[j,]      &lt;-<span class="st"> </span><span class="kw">t</span>(y_test<span class="op">-</span>new_Y)<span class="op">%*%</span>(y_test<span class="op">-</span>new_Y)
    
  }
  
  BEST_cv&lt;-sub_matrix[<span class="kw">which.min</span>(cv),]
  BEST_cv[<span class="kw">is.na</span>(BEST_cv)] &lt;-<span class="dv">0</span>
  <span class="cf">if</span>(<span class="kw">sum</span>(BEST_cv<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>))<span class="op">==</span><span class="dv">0</span>){
    
    Exact[<span class="dv">2</span>]&lt;-Exact[<span class="dv">2</span>]<span class="op">+</span><span class="dv">1</span>
    
  }
  <span class="cf">if</span>(<span class="kw">sum</span>((BEST_cv[<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>)]<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)))<span class="op">==</span><span class="dv">0</span>){
    Correct[<span class="dv">2</span>]&lt;-Correct[<span class="dv">2</span>]<span class="op">+</span><span class="dv">1</span>
  }
    AverageN[l,<span class="dv">2</span>]&lt;-<span class="kw">sum</span>(BEST_cv)
   
}

Exact<span class="op">/</span><span class="dv">100</span></code></pre></div>
<pre><code>##    AIC   CV
## 1 0.84 0.47</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Correct<span class="op">/</span><span class="dv">100</span></code></pre></div>
<pre><code>##   AIC CV
## 1   1  1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colMeans</span>(AverageN)</code></pre></div>
<pre><code>##  AIC   CV 
## 3.20 3.63</code></pre>
<p>In this part, we have simulated the AIC process and the CV process for 100 times and evaluated the three criteria. We have found that AIC gives better results than CV in terms of Exact models and it has a lower number of selected regressors on average. Both methods achieves model selection consistency in this simple setting as the proportion of correct models reaches the value of 1.</p>
<p>This result is expected since AIC is derived from the likelihood which inherits all the nice properties (e.g. Cramer-Rao bound etc.) when the model is correct. On the other hand, CV is a non parametric method thus inferior by definition to the AIC in this setting. However in a real application our conjectured model maybe far from the truth and CV could be a better choice.</p>
<blockquote>
<ul>
<li>In the same simulation setting outlined in the CV exercise section, generate from a MVN (multivariate normal) a matrix <span class="math inline">\(\mathbf{X_{n*p}}\)</span> with <span class="math inline">\(n = 1000\)</span> and <span class="math inline">\(p = 5\)</span> but now fix the scale matrix with an autoregressive form <span class="math inline">\(\boldsymbol{\Sigma}=[\sigma_{lm}]_{l,m=1,\ldots,p}\)</span> with <span class="math inline">\(\sigma_{lm} = \rho^{\mid l - m\mid}\)</span>. Compare the performance of CV and AIC for <span class="math inline">\(\boldsymbol{\rho} = [0.2 \; 0.5\; 0.7]\)</span> (<span class="math inline">\(\rho = 0\)</span> corresponds to the identity case that you have already treated).<br />
</li>
</ul>
</blockquote>
<p>There are different sources of randomness: X matrix, epsilon (error), y (induced) and the split for CV.We need to choose what to preserve: we replicate every time only the randomness of epsilon and of course the split of the CV thus we need to set a seed for the X matrix. All this discussion on randomness is pivotal for a significant and controlled simulation setting.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(MASS)

<span class="kw">set.seed</span>(<span class="dv">5</span>) <span class="co"># For the X matrix</span>

n =<span class="st"> </span><span class="dv">1000</span>

p =<span class="st"> </span><span class="dv">5</span>

rho =<span class="st"> </span><span class="fl">0.5</span> <span class="co"># change rho as you please to inspect other correlations among the predictors</span>

mu =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,p)

sigma =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,p<span class="op">^</span><span class="dv">2</span>)

sigma =<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> sigma, <span class="dt">ncol =</span> p,<span class="dt">nrow =</span> p)


<span class="co"># Autoregressive structure</span>

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>p) {
  
  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>p) {
    
    
    sigma[i,j] =<span class="st"> </span>rho<span class="op">^</span>(<span class="kw">abs</span>(i<span class="op">-</span>j))
    
    
  }
  
  
}

X =<span class="st"> </span><span class="kw">mvrnorm</span>(n,mu,sigma)


<span class="co"># beta = c(0,0,1,0,0) alternative beta to check bias properties</span>

beta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>,<span class="fl">1.5</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>)

ind =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">1000</span>

pos_CV =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">100</span>)

pos_AIC =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">100</span>)


<span class="cf">for</span>(z <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {
  
  
  y_hat =<span class="st"> </span>X<span class="op">%*%</span>beta <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n,<span class="dv">0</span>,<span class="dv">1</span>) 
  
  data =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">cbind</span>(y_hat,X))
  
  <span class="kw">colnames</span>(data) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;y&quot;</span>,<span class="st">&quot;x1&quot;</span>,<span class="st">&quot;x2&quot;</span>,<span class="st">&quot;x3&quot;</span>,<span class="st">&quot;x4&quot;</span>,<span class="st">&quot;x5&quot;</span>)
  
  index =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> ind,<span class="dt">size =</span> <span class="dv">500</span>,<span class="dt">replace =</span> F)
  
  train_set =<span class="st"> </span>data[index,]
  
  test_set =<span class="st"> </span>data[<span class="op">-</span>index,]
  
  cv_error =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">31</span>)
  
  aic_values =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">31</span>)
  
  <span class="co"># One regressor only models</span>
  
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {
    
    
    m_<span class="dv">1</span> =<span class="st"> </span><span class="kw">lm</span>(train_set<span class="op">$</span>y <span class="op">~</span><span class="st"> </span>train_set[,i<span class="op">+</span><span class="dv">1</span>], <span class="dt">data =</span> train_set)
    
    cv_error[i] =<span class="st"> </span><span class="kw">mean</span>((test_set<span class="op">$</span>y <span class="op">-</span><span class="st"> </span>(<span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">500</span>),test_set[,i<span class="op">+</span><span class="dv">1</span>])<span class="op">%*%</span><span class="st"> </span>m_<span class="dv">1</span><span class="op">$</span>coefficients))<span class="op">^</span><span class="dv">2</span>)
    
    m_<span class="dv">1</span> =<span class="st"> </span><span class="kw">lm</span>(data<span class="op">$</span>y <span class="op">~</span><span class="st"> </span>data[,i<span class="op">+</span><span class="dv">1</span>], data)
    
    aic_values[i] =<span class="st"> </span><span class="op">-</span><span class="dv">2</span><span class="op">*</span><span class="kw">logLik</span>(m_<span class="dv">1</span>) <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="dv">3</span>
    
  }
  
  <span class="co"># Two regressors models</span>
  
  x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>)
  
  M =<span class="st"> </span><span class="kw">combn</span>(x,<span class="dv">2</span>)
  
  
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) {
    
    
    m_<span class="dv">2</span> =<span class="st"> </span><span class="kw">lm</span>(train_set<span class="op">$</span>y <span class="op">~</span><span class="st"> </span>train_set[,M[<span class="dv">1</span>,i]<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>train_set[,M[<span class="dv">2</span>,i]<span class="op">+</span><span class="dv">1</span>], <span class="dt">data =</span> train_set)
    
    cv_error[i<span class="op">+</span><span class="dv">5</span>] =<span class="st"> </span><span class="kw">mean</span>((test_set<span class="op">$</span>y <span class="op">-</span><span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">500</span>),test_set[,M[<span class="dv">1</span>,i]<span class="op">+</span><span class="dv">1</span>], test_set[, M[<span class="dv">2</span>,i] <span class="op">+</span><span class="dv">1</span>])<span class="op">%*%</span>m_<span class="dv">2</span><span class="op">$</span>coefficients)<span class="op">^</span><span class="dv">2</span>)
    
    m_<span class="dv">2</span> =<span class="st"> </span><span class="kw">lm</span>(data<span class="op">$</span>y <span class="op">~</span><span class="st"> </span>data[,M[<span class="dv">1</span>,i]<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>data[,M[<span class="dv">2</span>,i]<span class="op">+</span><span class="dv">1</span>], data)
    
    aic_values[i<span class="op">+</span><span class="dv">5</span>] =<span class="st"> </span><span class="op">-</span><span class="dv">2</span><span class="op">*</span><span class="kw">logLik</span>(m_<span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="dv">4</span>
  }
  
  
  <span class="co"># Three regressors models</span>
  
  M =<span class="st"> </span><span class="kw">combn</span>(x,<span class="dv">3</span>)
  
  
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) {
    
    
    m_<span class="dv">3</span> =<span class="st"> </span><span class="kw">lm</span>(train_set<span class="op">$</span>y <span class="op">~</span><span class="st"> </span>train_set[,M[<span class="dv">1</span>,i]<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>train_set[,M[<span class="dv">2</span>,i]<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>train_set[,M[<span class="dv">3</span>,i]<span class="op">+</span><span class="dv">1</span>], <span class="dt">data =</span> train_set)
    
    cv_error[i<span class="op">+</span><span class="st"> </span><span class="dv">15</span>] =<span class="st"> </span><span class="kw">mean</span>((test_set<span class="op">$</span>y <span class="op">-</span><span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">500</span>),test_set[,M[<span class="dv">1</span>,i]<span class="op">+</span><span class="dv">1</span>], test_set[, M[<span class="dv">2</span>,i] <span class="op">+</span><span class="dv">1</span>],test_set[, M[<span class="dv">3</span>,i]<span class="op">+</span><span class="dv">1</span>])<span class="op">%*%</span>m_<span class="dv">3</span><span class="op">$</span>coefficients)<span class="op">^</span><span class="dv">2</span>)
    
    m_<span class="dv">3</span> =<span class="st"> </span><span class="kw">lm</span>(data<span class="op">$</span>y <span class="op">~</span><span class="st"> </span>data[,M[<span class="dv">1</span>,i]<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>data[,M[<span class="dv">2</span>,i]<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>data[,M[<span class="dv">3</span>,i]<span class="op">+</span><span class="dv">1</span>], data)
    
    aic_values[i<span class="op">+</span><span class="dv">15</span>] =<span class="st"> </span><span class="op">-</span><span class="dv">2</span><span class="op">*</span><span class="kw">logLik</span>(m_<span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="dv">5</span>
    
  }
  
  
  <span class="co"># Four regressors models</span>
  
  M =<span class="st"> </span><span class="kw">combn</span>(x,<span class="dv">4</span>)
  
  
  <span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {
    
    
    m_<span class="dv">4</span> =<span class="st"> </span><span class="kw">lm</span>(train_set<span class="op">$</span>y <span class="op">~</span><span class="st"> </span>train_set[,M[<span class="dv">1</span>,i]<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>train_set[,M[<span class="dv">2</span>,i]<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>train_set[,M[<span class="dv">3</span>,i]<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>train_set[,M[<span class="dv">4</span>,i]<span class="op">+</span><span class="dv">1</span>], <span class="dt">data =</span> train_set)
    
    cv_error[i<span class="op">+</span><span class="dv">25</span>] =<span class="st"> </span><span class="kw">mean</span>((test_set<span class="op">$</span>y <span class="op">-</span><span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">500</span>),test_set[,M[<span class="dv">1</span>,i]<span class="op">+</span><span class="dv">1</span>], test_set[, M[<span class="dv">2</span>,i] <span class="op">+</span><span class="dv">1</span>],test_set[, M[<span class="dv">3</span>,i]<span class="op">+</span><span class="dv">1</span>],test_set[, M[<span class="dv">3</span>,i]<span class="op">+</span><span class="dv">1</span>])<span class="op">%*%</span><span class="st"> </span>m_<span class="dv">4</span><span class="op">$</span>coefficients)<span class="op">^</span><span class="dv">2</span>)
    
    m_<span class="dv">4</span> =<span class="st"> </span><span class="kw">lm</span>(data<span class="op">$</span>y <span class="op">~</span><span class="st"> </span>data[,M[<span class="dv">1</span>,i]<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>data[,M[<span class="dv">2</span>,i]<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>data[,M[<span class="dv">3</span>,i]<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>data[,M[<span class="dv">4</span>,i]<span class="op">+</span><span class="dv">1</span>], data)
    
    aic_values[i<span class="op">+</span><span class="dv">25</span>] =<span class="st"> </span><span class="op">-</span><span class="dv">2</span><span class="op">*</span><span class="kw">logLik</span>(m_<span class="dv">4</span>) <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="dv">6</span>
    
  }
  
  <span class="co"># Full model </span>
  
  full_model =<span class="st"> </span><span class="kw">lm</span>(train_set<span class="op">$</span>y <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train_set)
  
  full_model<span class="op">$</span>coefficients
  
  cv_error_full =<span class="st"> </span><span class="kw">mean</span>((test_set<span class="op">$</span>y <span class="op">-</span><span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">500</span>), test_set<span class="op">$</span>x1,test_set<span class="op">$</span>x2,test_set<span class="op">$</span>x3,test_set<span class="op">$</span>x4,test_set<span class="op">$</span>x5)<span class="op">%*%</span><span class="st"> </span>full_model<span class="op">$</span>coefficients)<span class="op">^</span><span class="dv">2</span>)
  
  cv_error[<span class="dv">31</span>] =<span class="st"> </span>cv_error_full
  
  full_model =<span class="st"> </span><span class="kw">lm</span>(data<span class="op">$</span>y <span class="op">~</span><span class="st"> </span>., data)
  
  aic_values[<span class="dv">31</span>] =<span class="st"> </span><span class="op">-</span><span class="dv">2</span><span class="op">*</span><span class="kw">logLik</span>(full_model) <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span><span class="dv">7</span>
  
  pos_CV[z] =<span class="st"> </span><span class="kw">which.min</span>(cv_error)
  
  pos_AIC[z] =<span class="st"> </span><span class="kw">which.min</span>(aic_values)
  
}

<span class="co"># We know that the right position should be 17 which is the exact model.</span>

<span class="co"># Exact model proportion</span>

<span class="kw">sum</span>(pos_CV <span class="op">==</span><span class="st"> &quot;17&quot;</span>)<span class="op">/</span><span class="dv">100</span> </code></pre></div>
<pre><code>## [1] 0.48</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(pos_AIC <span class="op">==</span><span class="st"> &quot;17&quot;</span>)<span class="op">/</span><span class="dv">100</span> </code></pre></div>
<pre><code>## [1] 0.71</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Correct model proportion (linked to the consistency of the model selection procedure)</span>

<span class="kw">sum</span>(pos_CV <span class="op">&gt;</span><span class="st"> </span><span class="dv">16</span>)<span class="op">/</span><span class="dv">100</span> </code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>(pos_AIC <span class="op">&gt;</span><span class="st"> </span><span class="dv">16</span>)<span class="op">/</span><span class="dv">100</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Average number of regressors</span>

counted =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">5</span>),<span class="kw">rep</span>(<span class="dv">2</span>,<span class="dv">10</span>), <span class="kw">rep</span>(<span class="dv">3</span>,<span class="dv">10</span>), <span class="kw">rep</span>(<span class="dv">4</span>,<span class="dv">5</span>),<span class="dv">5</span>)

result_cv =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">100</span>)

result_aic =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">100</span>)


<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {
  
  
  result_cv[i] =<span class="st"> </span>counted[pos_CV[i]]

  result_aic[i] =<span class="st"> </span>counted[pos_AIC[i]]  
  
}


<span class="co"># CV CASE</span>

<span class="kw">mean</span>(result_cv)</code></pre></div>
<pre><code>## [1] 3.8</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># AIC case</span>

<span class="kw">mean</span>(result_aic)</code></pre></div>
<pre><code>## [1] 3.3</code></pre>
<p>CONCLUSIONS: correlation does not play a role here because it biases our estimates but not that much that is necessary to make AIC (less so CV) fails to recognize the significant regressors. This holds especially in the presence of strong signal so for a beta with important components. The transition chain is: we create correlated X, we transmit correlation to the ys, the likelihood theory is not optimal anymore (y should be independent, X fixed) and I get biased estimates. However I need a really important bias to make AIC fails to recognize significant variables in the linear case. The GLM case is different: correlation is a real issue because of the link function. Even a small bias in that direction can alter the order of variables and thus AIC results.</p>
<blockquote>
<ul>
<li>Upload the <a href="https://github.com/CaesarXVII/MSHD-book-and-datasets/blob/master/datasets/malnutrion_zambia_cleaned.Rda">Zambia dataset</a> and perform an exhaustive search on the continuous covariates (i.e. avoiding factors) based on CV and AIC in order to find the best model. You can either employ your codes derived in previous exercises or make use of the existing R packages: <em>leaps</em>, <em>glmulti</em>, <em>MuMIn</em> and <em>caret</em>.<br />
</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load Zambia dataset </span>

<span class="kw">load</span>(<span class="dt">file =</span> <span class="st">&quot;your_directory/data_zambia.Rda&quot;&quot;)</span>

<span class="st">data_zambia = data_zambia[,c(1:7,21,22,24)] #exclude the factors from the analysis</span>


<span class="st">### Exhaustive search with leaps (AIC case) ###</span>

<span class="st">require(leaps)</span>

<span class="st">regsubsets.out &lt;- regsubsets(data_zambia$`Height for age sd` ~ .,data = data_zambia,nbest = 1,</span>
<span class="st">                             nvmax = NULL,    # NULL for no limit on number of variables</span>
<span class="st">                             force.in = NULL, force.out = NULL,</span>
<span class="st">                             method = &quot;</span>exhaustive<span class="st">&quot;)</span>

<span class="st">summary(regsubsets.out)</span>


<span class="st">plot(regsubsets.out) # BIC is default</span>

<span class="st">plot(regsubsets.out,scale = &quot;</span>Cp<span class="st">&quot;) #C_p case which is equal to AIC with a linear model</span>


<span class="st">### Exhaustive search with glmulti (AIC case) ### </span>


<span class="st">glmulti.lm.out &lt;- glmulti::glmulti(data_zambia$`Height for age sd` ~ .,data = data_zambia,</span>
<span class="st">          level = 1,               # No interaction considered</span>
<span class="st">          method = &quot;</span>h<span class="st">&quot;,            # Exhaustive approach</span>
<span class="st">          crit = &quot;</span>aic<span class="st">&quot;,            # AIC as criteria</span>
<span class="st">          confsetsize = 5,         # Keep 5 best models</span>
<span class="st">          plotty = F, report = F,  # No plot or interim reports</span>
<span class="st">          fitfunction = &quot;</span>lm<span class="st">&quot;)      # lm function</span>



<span class="st">### Exhaustive search with MumIn (AIC case) ###</span>

<span class="st">require(MuMIn)</span>


<span class="st">data_model &lt;- lm(data_zambia$`Height for age sd` ~ .,data = data_zambia)</span>


<span class="st">combinations &lt;- dredge(data_model)</span>

<span class="st">print(combinations)</span>


<span class="st">### Exhaustive search with caret (CV case and AIC case) ###</span>

<span class="st">require(caret)</span>

<span class="st">attach(data_zambia)</span>

<span class="st"># Unfortunately there is no exhaustive search based on CV in Caret, it is just stepwise.</span>

<span class="st">#setting up 10-fold cross-validation</span>

<span class="st">control &lt;- trainControl(method=&quot;</span>cv<span class="st">&quot;, number=10) </span>

<span class="st">#finding the ideal model by AIC criterion</span>

<span class="st">model_AIC = train(`Height for age sd`~.,data=data_zambia,method=&quot;</span>lmStepAIC<span class="st">&quot;,trControl=control)</span>

<span class="st">#finding the ideal model by mean square error</span>

<span class="st">modelCV = train(`Height for age sd`~.,data=data_zambia,method=&quot;</span>lm<span class="st">&quot;,trControl=control)</span>

<span class="st">detach(data_zambia)</span>

<span class="st"># In order to do a full exhaustive search with CV, we should exploit the codes produced in CV exercise. Of course </span>
<span class="st"># as the number of variables increase, the task becomes impossible.</span></code></pre></div>
</div>
<div id="roc" class="section level3">
<h3><span class="header-section-number">7.2.3</span> ROC curves</h3>
<p>Read the information on the <a href="https://caesarxvii.github.io/MSHD-book-and-datasets/index.html#prognostic-factors-in-childhood-leukemia">Leukemia Dataset</a> in the first chapter of the book. Then load the <a href="https://github.com/CaesarXVII/MSHD-book-and-datasets/blob/master/datasets/data_leukemia_reduced.Rda">Leukemia dataset reduced</a> which contains a subset of 11 eleven predictors among the 3571 present in the <em>leukemia_small.csv</em>. These variables have been selected, because of their importance, by the binary lasso which is a shrinkage method that will be discussed later on in the course. Now perform the following steps:</p>
<blockquote>
Fit the appropriate GLM for the situation using only one of the available predictors (e.g. V457)<br />

</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;data_leukemia_reduced.Rda&quot;</span>)

## Part (a)

mod_<span class="dv">1</span> =<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>V457, <span class="dt">data =</span> data_leukemia_reduced,<span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)</code></pre></div>
<blockquote>
Read the <a href="https://caesarxvii.github.io/MSHD-book-and-datasets/assessing-the-validity-of-a-model.html#the-roc-curve">ROC curve</a> section of the e-book. Then find the TPR (i.e. true positive rate), FPR (i.e. false positive rate), TNR (i.e. true negative rate), FNR (i.e. false negative rate) of the fitted values found at point (a) with a cut-off value <span class="math inline">\(c = 0.5\)</span>.<br />

</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Confusion matrix</span>

c0 =<span class="st"> </span><span class="fl">0.5</span>

mod_<span class="dv">1</span> =<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>V457, <span class="dt">data =</span> data_leukemia_reduced,<span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)


conf_mat =<span class="st"> </span><span class="kw">table</span>(data_leukemia_reduced<span class="op">$</span>y, mod_<span class="dv">1</span><span class="op">$</span>fitted.values <span class="op">&gt;</span><span class="st"> </span>c0) <span class="co">#unbalanced sample 47 ones and 25 0s</span>

conf_mat</code></pre></div>
<pre><code>##    
##     FALSE TRUE
##   0    22    3
##   1     3   44</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prop_confmat =<span class="st"> </span><span class="kw">prop.table</span>(conf_mat, <span class="dv">1</span>) <span class="co"># For ROC curve is on the line</span>

prop_confmat</code></pre></div>
<pre><code>##    
##          FALSE       TRUE
##   0 0.88000000 0.12000000
##   1 0.06382979 0.93617021</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">TNR =<span class="st"> </span>conf_mat[<span class="dv">1</span>,<span class="dv">1</span>]<span class="op">/</span>(conf_mat[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>conf_mat[<span class="dv">1</span>,<span class="dv">2</span>]) <span class="co"># TRUE NEGATIVE (TNR)</span>

FPR =<span class="st"> </span>conf_mat[<span class="dv">1</span>,<span class="dv">2</span>]<span class="op">/</span>(conf_mat[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>conf_mat[<span class="dv">1</span>,<span class="dv">2</span>])  <span class="co"># FALSE POSITIVE (FPR)</span>

FNR =<span class="st"> </span>conf_mat[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">/</span>(conf_mat[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>conf_mat[<span class="dv">2</span>,<span class="dv">2</span>])  <span class="co"># FALSE NEGATIVE (FNR)</span>
  
TPR =<span class="st"> </span>conf_mat[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>(conf_mat[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>conf_mat[<span class="dv">2</span>,<span class="dv">2</span>])  <span class="co"># TRUE POSITIVE (TPR)</span>
                               
<span class="co"># Also possible with caret but with train and test arguments. Not full dataset inquiry as in this case.</span>

<span class="co"># require(caret)</span>

<span class="co"># confusionMatrix()</span></code></pre></div>
<blockquote>
For a given cut-off grid of values, that you can choose as you wish, plot the ROC curve relative to the estimated model.<br />

</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">c0 =<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.1</span>,<span class="fl">0.9</span>,<span class="fl">0.1</span>)

TP =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">length</span>(c0))

FP =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">length</span>(c0))

mod_<span class="dv">1</span> =<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>V457, <span class="dt">data =</span> data_leukemia_reduced,<span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(c0)) {
  
  conf_mat =<span class="st"> </span><span class="kw">table</span>(data_leukemia_reduced<span class="op">$</span>y, mod_<span class="dv">1</span><span class="op">$</span>fitted.values <span class="op">&gt;</span><span class="st"> </span>c0[i]) <span class="co">#unbalanced sample 47 ones and 25 0s</span>
  
  prop_confmat =<span class="st"> </span><span class="kw">prop.table</span>(conf_mat, <span class="dv">1</span>) <span class="co"># For ROC curve is on the columns</span>
  
  TP[i] =<span class="st"> </span>prop_confmat[<span class="dv">2</span>,<span class="dv">2</span>]
  
  FP[i] =<span class="st"> </span>prop_confmat[<span class="dv">1</span>,<span class="dv">2</span>]
  
}

FP =<span class="st"> </span><span class="kw">sort</span>(FP) <span class="co"># for a correct line</span>

TP =<span class="st"> </span><span class="kw">sort</span>(TP) <span class="co"># for a correct line</span>

<span class="co"># Every ROC curve passes towards (0,0) and (1,1)</span>

TP =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,TP,<span class="dv">1</span>)


FP =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,FP,<span class="dv">1</span>)


<span class="kw">plot</span>(FP,TP, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>,<span class="dt">main =</span> <span class="st">&quot;ROC Curve&quot;</span>,<span class="dt">xlab =</span> <span class="st">&quot;1 - Specificity&quot;</span>,<span class="dt">ylab =</span> <span class="st">&quot;Sensitivity&quot;</span>,<span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>),<span class="dt">type =</span> <span class="st">&quot;l&quot;</span>)
<span class="kw">lines</span>(<span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">type=</span><span class="st">&#39;l&#39;</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-51-1.png" width="672" /></p>
<blockquote>
Check the quality of your result at point (c) with the R package <em>pROC</em>.<br />

</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># install.packages(&quot;pROC&quot;)</span>

<span class="kw">require</span>(pROC)</code></pre></div>
<pre><code>## Loading required package: pROC</code></pre>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">roc_<span class="dv">1</span> =<span class="st"> </span><span class="kw">roc</span>(<span class="dt">response=</span>data_leukemia_reduced<span class="op">$</span>y, <span class="dt">predictor =</span> mod_<span class="dv">1</span><span class="op">$</span>fitted.values)

<span class="kw">plot</span>(roc_<span class="dv">1</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-52-1.png" width="672" /></p>
</div>
</div>
<div id="chapter-3" class="section level2">
<h2><span class="header-section-number">7.3</span> Chapter 3</h2>
<div id="HT" class="section level3">
<h3><span class="header-section-number">7.3.1</span> Selection by Hypothesis Testing</h3>
<blockquote>
First of all we retrieve the simulation setting used in <em>Practical 3</em>.<br />

</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># install.packages(&quot;selectiveInference&quot;)</span>

<span class="kw">require</span>(selectiveInference)

<span class="kw">require</span>(MASS)

<span class="co"># Create the simulation setting</span>

<span class="kw">set.seed</span>(<span class="dv">11</span>)

n =<span class="st"> </span><span class="dv">1000</span>

p =<span class="st"> </span><span class="dv">10</span>

<span class="co"># change rho as you please to inspect other correlations among the predictors</span>

rho =<span class="st"> </span><span class="dv">0</span> 

mu =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,p)

sigma =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,p<span class="op">^</span><span class="dv">2</span>)

sigma =<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> sigma, <span class="dt">ncol =</span> p,<span class="dt">nrow =</span> p)


<span class="co"># Autoregressive structure</span>

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>p) {
  
  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>p) {
    
    
    sigma[i,j] =<span class="st"> </span>rho<span class="op">^</span>(<span class="kw">abs</span>(i<span class="op">-</span>j))
    
    
  }
  
  
}

X =<span class="st"> </span><span class="kw">mvrnorm</span>(n,mu,sigma)


beta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>,<span class="fl">1.5</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">6</span>))


y =<span class="st"> </span>X<span class="op">%*%</span>beta <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n,<span class="dv">0</span>,<span class="dv">1</span>) <span class="co">#for us sigma is 1</span></code></pre></div>
<blockquote>
Now, after having read the documentation of the R package <em>selectiveInference</em> and installed it, perform the following steps:<br />
a) Use the functions <em>fs()</em>, <em>fsInf()</em> and <em>forwardStop()</em> to do a stepwise regression based on partial correlations and a model selection phase with the ForwardStop rule on your generated data. Try different values for the type one error: how does the choice of <span class="math inline">\(\alpha\)</span> impact the model selection technique?<br />

</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part a ##


fsfit =<span class="st"> </span><span class="kw">fs</span>(X,y) <span class="co">#partial correlations like OMP or stagewise</span>

fsfit<span class="op">$</span>action <span class="co">#order of regressors</span></code></pre></div>
<pre><code>##  [1]  1  4  2 10  9  3  8  7  6  5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fsfit<span class="op">$</span>beta <span class="co">#the beta estimation at each step</span></code></pre></div>
<pre><code>##       1        2        3        4           5           6           7
##  [1,] 0 3.047019 3.014175 2.965561  2.96179608  2.96371055  2.96277233
##  [2,] 0 0.000000 0.000000 1.486185  1.48523930  1.48682723  1.48759323
##  [3,] 0 0.000000 0.000000 0.000000  0.00000000  0.00000000  0.04017513
##  [4,] 0 0.000000 2.061130 2.020475  2.02252158  2.02605822  2.02658398
##  [5,] 0 0.000000 0.000000 0.000000  0.00000000  0.00000000  0.00000000
##  [6,] 0 0.000000 0.000000 0.000000  0.00000000  0.00000000  0.00000000
##  [7,] 0 0.000000 0.000000 0.000000  0.00000000  0.00000000  0.00000000
##  [8,] 0 0.000000 0.000000 0.000000  0.00000000  0.00000000  0.00000000
##  [9,] 0 0.000000 0.000000 0.000000  0.00000000  0.07326696  0.07269192
## [10,] 0 0.000000 0.000000 0.000000 -0.07675033 -0.07526561 -0.07532284
##                 8           9          10
##  [1,]  2.96311034  2.96286548  2.96191272
##  [2,]  1.48716658  1.48859575  1.48770072
##  [3,]  0.03987444  0.03973915  0.04053624
##  [4,]  2.02693788  2.02776730  2.02659795
##  [5,]  0.00000000  0.00000000  0.00000000
##  [6,]  0.00000000  0.00000000  0.01967913
##  [7,]  0.00000000 -0.02997558 -0.02864951
##  [8,] -0.03210415 -0.03298970 -0.03201230
##  [9,]  0.07351783  0.07405473  0.07443275
## [10,] -0.07361395 -0.07454985 -0.07369007</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute sequential p-values and confidence intervals  (sigma estimated from full model) </span>

out =<span class="st"> </span><span class="kw">fsInf</span>(fsfit,<span class="dt">alpha =</span> <span class="fl">0.1</span>) <span class="co">#default</span>

out <span class="co"># the value of forward stop is the last regressor which is active</span></code></pre></div>
<pre><code>## 
## Call:
## fsInf(obj = fsfit, alpha = 0.1)
## 
## Standard deviation of noise (specified or estimated) sigma = 0.997
## 
## Sequential testing results with alpha = 0.100
##  Step Var   Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea
##     1   1  3.047  97.561   0.000     2.995    3.099       0.048      0.048
##     2   4  2.061  65.227   0.000     2.009    2.113       0.049      0.049
##     3   2  1.486  47.535   0.000     1.435    1.538       0.049      0.049
##     4  10 -0.077  -2.420   0.834    -0.077    1.345       0.050      0.050
##     5   9  0.073   2.306   0.019     0.052    1.527       0.050      0.050
##     6   3  0.040   1.267   0.567    -0.334    0.193       0.050      0.050
##     7   8 -0.032  -1.003   0.660    -0.368    0.857       0.050      0.050
##     8   7 -0.030  -0.922   0.261    -0.906    0.291       0.050      0.050
##     9   6  0.020   0.614   0.621    -0.694    0.377       0.050      0.050
##    10   5  0.016   0.482   0.223    -0.175    0.737       0.050      0.050
## 
## Estimated stopping point from ForwardStop rule = 3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># estimate optimal stopping point </span>

last =<span class="st"> </span><span class="kw">forwardStop</span>(out<span class="op">$</span>pv, <span class="dt">alpha=</span><span class="fl">0.4</span>) <span class="co"># you want to reject more often fixing higher alfa</span>

stop_point =<span class="st"> </span><span class="kw">forwardStop</span>(out<span class="op">$</span>pv,<span class="dt">alpha =</span> <span class="fl">0.85</span>)

fsfit<span class="op">$</span>action[<span class="dv">1</span><span class="op">:</span>last] <span class="co"># to catch right components</span></code></pre></div>
<pre><code>## [1]  1  4  2 10  9</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Conclusions: fixing a higher alfa will let you include on average more regressors as you will reject more often H0 the null hypothesis. </span></code></pre></div>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Given the order of variables produced by <em>fs()</em>, use AIC and BIC criteria for model selection to retrieve your final model (<em>Hint: you do not need to program them, use an existing function of the selectiveInference package</em>).<br />
</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part b ##

<span class="co"># We use the same fit, the k step is decided by AIC penalty</span>

out_<span class="dv">2</span> =<span class="st"> </span><span class="kw">fsInf</span>(fsfit,<span class="dt">type =</span> <span class="st">&quot;aic&quot;</span>,<span class="dt">alpha =</span> <span class="fl">0.05</span>) <span class="co">#akaike case, the fixed value of alfa is 0.1</span>

out_<span class="dv">2</span> <span class="co">#be careful check with classic AIC evaluation (sometimes is not correct)</span></code></pre></div>
<pre><code>## 
## Call:
## fsInf(obj = fsfit, alpha = 0.05, type = &quot;aic&quot;)
## 
## Standard deviation of noise (specified or estimated) sigma = 0.997
## 
## Testing results at step = 5, with alpha = 0.050
##  Var   Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea
##    1  2.964  94.680   0.000     2.903    3.103       0.025      0.025
##    4  2.026  63.996   0.000     1.964    2.091       0.024      0.025
##    2  1.487  47.540   0.000     1.425    1.563       0.025      0.024
##   10 -0.075  -2.373   0.839    -0.099    1.711       0.025      0.025
##    9  0.073   2.306   0.024     0.001    1.863       0.025      0.025
## 
## Estimated stopping point from AIC rule = 5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We have already an idea that AIC tends to overfit</span>


### YOU CAN PROGRAM AIC USING ORDER IN fsfit$action <span class="al">###</span>


<span class="co"># We use the same fit, the k step is decided by BIC penalty</span>


out_<span class="dv">3</span> =<span class="st"> </span><span class="kw">fsInf</span>(fsfit, <span class="dt">type =</span> <span class="st">&quot;aic&quot;</span>, <span class="dt">mult =</span> <span class="kw">log</span>(n)) <span class="co">#bic case</span>

out_<span class="dv">3</span></code></pre></div>
<pre><code>## 
## Call:
## fsInf(obj = fsfit, type = &quot;aic&quot;, mult = log(n))
## 
## Standard deviation of noise (specified or estimated) sigma = 0.997
## 
## Testing results at step = 3, with alpha = 0.100
##  Var  Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea
##    1 2.966  94.889       0     2.914    3.018       0.048      0.048
##    4 2.020  63.917       0     1.968    2.073       0.050      0.049
##    2 1.486  47.535       0     1.435    1.538       0.049      0.049
## 
## Estimated stopping point from AIC rule = 3</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Another nice evidence, BIC has harsh penalty than AIC.</span>

<span class="co"># Other possibilities of fsInf() function</span>

out_<span class="dv">4</span> =<span class="st"> </span><span class="kw">fsInf</span>(fsfit,<span class="dt">type =</span> <span class="st">&quot;all&quot;</span>,<span class="dt">k =</span> <span class="dv">4</span>,<span class="dt">mult =</span> <span class="kw">log</span>(n)) 

<span class="co"># this is just to stop at a fixed number of steps</span>

out_<span class="dv">4</span></code></pre></div>
<pre><code>## 
## Call:
## fsInf(obj = fsfit, k = 4, type = &quot;all&quot;, mult = log(n))
## 
## Standard deviation of noise (specified or estimated) sigma = 0.997
## 
## Testing results at step = 4, with alpha = 0.100
##  Var   Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea
##    1  2.962  94.652   0.000     2.910    3.040       0.048      0.049
##    4  2.023  63.959   0.000     1.967    2.075       0.050      0.049
##    2  1.485  47.501   0.000     1.433    1.537       0.049      0.050
##   10 -0.077  -2.420   0.834    -0.077    1.345       0.050      0.050</code></pre>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Calculate how many models are needed for an exhaustive search in this simulation setting. Use your previous results obtained in <em>Practical 3</em> to understand the computational time gained by stepwise regression with respect to exhaustive search. Use the package <em>tictoc</em> for this comparison.<br />
</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part c ##


<span class="co"># Look at Practical 3, see your exhaustive search time for p=10. </span>

<span class="co"># In my case around 17 minutes for 1023 models (2^10 -1) since I used 1sec per model</span>

<span class="kw">require</span>(tictoc)

<span class="kw">tic</span>()

fsfit =<span class="st"> </span><span class="kw">fs</span>(X,y)

out =<span class="st"> </span><span class="kw">fsInf</span>(fsfit) <span class="co">#default</span>

last =<span class="st"> </span><span class="kw">forwardStop</span>(out<span class="op">$</span>pv, <span class="dt">alpha=</span><span class="fl">0.05</span>) <span class="co"># you want to reject more often fixing higher alfa</span>

<span class="kw">Sys.sleep</span>(<span class="dv">1</span>)
<span class="kw">toc</span>()

<span class="co"># 1.81 against 17 minutes... Exhaustive vs stepwise, a huge gain indeed!</span></code></pre></div>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li>(<em>Optional</em>) Change the simulation setting outlined above to an high dimensional one: generate from a MVN (multivariate normal) a matrix <span class="math inline">\(\mathbf{X_{nxp}}\)</span> with <span class="math inline">\(n = 100\)</span> and <span class="math inline">\(p = 150\)</span>. Evaluate the performance of the ForwardStop rule in this high dimensional setting (i.e. by replicating the model selection task 100 times) thanks to the usual three specific criteria: the proportion of times the correct model is selected (<em>Exact</em>), the proportion of times the selected model contains the correct one (<em>Correct</em>) and the average number of selected regressors (<em>Average</em> <span class="math inline">\(\sharp\)</span>}). What do you observe? What is the role of <span class="math inline">\(\alpha\)</span> in this case?<br />
</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part d ##

<span class="co"># Create the simulation setting</span>

<span class="kw">set.seed</span>(<span class="dv">11</span>)

n =<span class="st"> </span><span class="dv">100</span>

p =<span class="st"> </span><span class="dv">100</span>  

<span class="co">#p = 150 --&gt; It does not work as we can not invert this kind of matrix thus we need n &gt; or = than p</span>

<span class="co"># change rho as you please to inspect other correlations among the predictors</span>

rho =<span class="st"> </span><span class="dv">0</span> 

mu =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,p)

sigma =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,p<span class="op">^</span><span class="dv">2</span>)

sigma =<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> sigma, <span class="dt">ncol =</span> p,<span class="dt">nrow =</span> p)


<span class="co"># Autoregressive structure</span>

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>p) {
  
  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>p) {
    
    
    sigma[i,j] =<span class="st"> </span>rho<span class="op">^</span>(<span class="kw">abs</span>(i<span class="op">-</span>j))
    
    
  }
  
  
}

X =<span class="st"> </span><span class="kw">mvrnorm</span>(n,mu,sigma)


beta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>,<span class="fl">1.5</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">96</span>))


y =<span class="st"> </span>X<span class="op">%*%</span>beta <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n,<span class="dv">0</span>,<span class="dv">1</span>) <span class="co">#for us sigma is 1</span>


fsfit =<span class="st"> </span><span class="kw">fs</span>(X,y) <span class="co">#partial correlations like OMP or stagewise</span>


<span class="co"># You need to have p &lt; n to work with this kind of package. Or you can do a first screening based on</span>

<span class="co"># marginal correlations (like in SIS): order them and take the first n. </span>


sigma_hat =<span class="st"> </span><span class="kw">estimateSigma</span>(X,y) 


<span class="co"># You need to estimate sigma in this big models. If not there is bad estimation by default since sigma</span>

<span class="co"># is estimated on the full model.</span>


<span class="co"># compute sequential p-values and confidence intervals</span>

out =<span class="st"> </span><span class="kw">fsInf</span>(fsfit,<span class="dt">sigma =</span> <span class="fl">0.977</span>) 


out <span class="co"># the value of forward stop is the last regressor which is active</span>

<span class="co"># estimate optimal stopping point </span>


last =<span class="st"> </span><span class="kw">forwardStop</span>(out<span class="op">$</span>pv, <span class="dt">alpha=</span><span class="fl">0.05</span>) <span class="co"># you want to reject more often fixing higher alfa</span>

fsfit<span class="op">$</span>action[<span class="dv">1</span><span class="op">:</span>last] <span class="co"># to catch right components</span>


<span class="co"># Evaluation with model selection criteria</span>

order =<span class="st"> </span><span class="kw">list</span>()

<span class="cf">for</span> (z <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {
  
  y =<span class="st"> </span>X<span class="op">%*%</span>beta <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n,<span class="dv">0</span>,<span class="dv">1</span>) 
  
  
  fsfit =<span class="st"> </span><span class="kw">fs</span>(X,y) 
  
  
  sigma_hat =<span class="st"> </span><span class="kw">estimateSigma</span>(X,y) 
  
  a =<span class="st"> </span>sigma_hat<span class="op">$</span>sigmahat
  
  out =<span class="st"> </span><span class="kw">fsInf</span>(fsfit,<span class="dt">sigma =</span> a) 
  
  last =<span class="st"> </span><span class="kw">forwardStop</span>(out<span class="op">$</span>pv, <span class="dt">alpha=</span><span class="fl">0.05</span>) <span class="co"># you want to reject more often fixing higher alfa</span>
  
  order[[z]] =<span class="st"> </span>fsfit<span class="op">$</span>action[<span class="dv">1</span><span class="op">:</span>last] 
  
}

### Exact models <span class="al">###</span>

exact =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">100</span>)

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {
  
  <span class="cf">if</span> (<span class="kw">identical</span>(order[[i]],<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>,<span class="dv">2</span>)) <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>){
    
    exact[i] =<span class="st"> </span><span class="dv">1</span>
    
    
  }
  
  
}

<span class="kw">sum</span>(exact)<span class="op">/</span><span class="dv">100</span>  

### Correct (related to consistency in model selection) <span class="al">###</span>

prop_cor =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">100</span>)

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {
  
  <span class="cf">if</span> (<span class="kw">length</span>(order[[i]]) <span class="op">&gt;=</span><span class="st"> </span><span class="dv">3</span>) {
    
    
    prop_cor[i] =<span class="st"> </span><span class="dv">1</span>
    
  } 
}

<span class="kw">sum</span>(prop_cor)<span class="op">/</span><span class="dv">100</span>  <span class="co">#Due to the low number of observations n=100</span>

  
### Average number of regressors <span class="al">###</span>


<span class="co"># Find the number of elements in each order[[z]]</span>

num.el =<span class="st"> </span><span class="kw">sapply</span>(order, length)

<span class="kw">sum</span>(num.el)<span class="op">/</span><span class="dv">100</span>

<span class="co"># The role of alfa is the same as before, as you increase it you get overfitting while if you lower it</span>

<span class="co"># you will reject more often thus leading to a sparser model. </span></code></pre></div>
<blockquote>
<p>Consider the Malnutrition in Zambia dataset. For simplicity work only on the continuous covariates (i.e. avoiding factors) and order them according to their partial correlations using the R function <em>fs</em> of the <em>Selective Inference</em> R Package (<a href="https://cran.r-project.org/web/packages/selectiveInference/index.html" class="uri">https://cran.r-project.org/web/packages/selectiveInference/index.html</a>). Compare the selected models when using:  </p>
</blockquote>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>the ForwardStop<br />
</li>
<li>the <span class="math inline">\(C_p\)</span> or AIC (equal in linear case)<br />
</li>
<li>the BIC</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;malnutrion_zambia_cleaned.Rda&quot;</span>)


data_zambia =<span class="st"> </span>data_zambia[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>,<span class="dv">21</span>,<span class="dv">22</span>,<span class="dv">24</span>)] <span class="co">#exclude the factors from the analysis</span>

### selective inference package <span class="al">###</span>

## ForwardStop rule ##

X =<span class="st"> </span><span class="kw">as.matrix</span>(data_zambia[,<span class="dv">2</span><span class="op">:</span><span class="dv">10</span>])

y =<span class="st"> </span>data_zambia[,<span class="dv">1</span>]

fsfit =<span class="st"> </span><span class="kw">fs</span>(X,y) 

fsfit<span class="op">$</span>action <span class="co">#order by partial correlations</span></code></pre></div>
<pre><code>## [1] 1 5 8 7 6 3 2 9 4</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># compute sequential p-values and confidence intervals # (sigma estimated from full model)</span>

out =<span class="st"> </span><span class="kw">fsInf</span>(fsfit) 

<span class="co"># The above function is using forward stop and pvalue evaluated at each step with forwardstop </span>

<span class="co"># It is a formula to control FDR.</span>

out </code></pre></div>
<pre><code>## 
## Call:
## fsInf(obj = fsfit)
## 
## Standard deviation of noise (specified or estimated) sigma = 1.592
## 
## Sequential testing results with alpha = 0.100
##  Step Var   Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea
##     1   1 -0.062 -12.229   0.000    -0.070   -0.053       0.049      0.050
##     2   5  5.209   8.937   0.000     3.812    6.179       0.049      0.049
##     3   8  0.283   4.901   0.400    -0.662    0.337       0.050      0.050
##     4   7  0.000   4.719   0.126     0.000    0.000       0.050      0.050
##     5   6  0.011   2.902   0.369    -0.051    0.074       0.050      0.050
##     6   3  0.015   2.380   0.182    -0.029    0.131       0.050      0.050
##     7   2 -0.005  -1.865   0.422    -0.029    0.026       0.050      0.050
##     8   9  0.003   1.616   0.107    -0.002    0.025       0.050      0.050
##     9   4 -0.081  -0.683   0.478    -0.451    0.529       0.050      0.050
## 
## Estimated stopping point from ForwardStop rule = 2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">forwardStop</span>(out<span class="op">$</span>pv, <span class="dt">alpha=</span>.<span class="dv">10</span>) <span class="co">#only breastfeeding and height of the mother matters</span></code></pre></div>
<pre><code>## [1] 2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## AIC and BIC penalty to decide step k ##

out_<span class="dv">2</span> =<span class="st"> </span><span class="kw">fsInf</span>(fsfit,<span class="dt">type =</span> <span class="st">&quot;aic&quot;</span>) <span class="co">#akaike case, the fixed value of alfa is 0.1</span>

out_<span class="dv">2</span> <span class="co">#estimated stopping point is correct. </span></code></pre></div>
<pre><code>## 
## Call:
## fsInf(obj = fsfit, type = &quot;aic&quot;)
## 
## Standard deviation of noise (specified or estimated) sigma = 1.592
## 
## Testing results at step = 9, with alpha = 0.100
##  Var   Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea
##    1 -0.053  -7.681   0.016    -0.105   -0.015       0.050       0.05
##    5  1.348   0.394   0.552   -27.065   17.845       0.000       0.05
##    8  0.285   4.904   0.418    -0.716    0.340       0.050       0.05
##    7  0.000   3.178   0.330     0.000    0.000       0.050       0.05
##    6  0.043   0.909   0.401    -0.196    0.236       0.050       0.05
##    3  0.012   1.804   0.266    -0.048    0.127       0.050       0.05
##    2 -0.006  -1.904   0.414    -0.029    0.026       0.050       0.05
##    9  0.003   1.636   0.483    -0.026    0.024       0.050       0.05
##    4 -0.081  -0.683   0.436    -0.575    0.527       0.049       0.05
## 
## Estimated stopping point from AIC rule = 9</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># You notice that AIC tends to overfit in this framework. Remember that we were doing </span>

<span class="co"># exhaustive search and not stepwise with AIC in the other Practicals.</span>

out_<span class="dv">3</span> =<span class="st"> </span><span class="kw">fsInf</span>(fsfit, <span class="dt">type =</span> <span class="st">&quot;aic&quot;</span>, <span class="dt">mult =</span> <span class="kw">log</span>(n)) <span class="co">#bic case</span>

out_<span class="dv">3</span> <span class="co"># BIC has a harsh penalty that correct AIC tendency to overfit (in this applied context)</span></code></pre></div>
<pre><code>## 
## Call:
## fsInf(obj = fsfit, type = &quot;aic&quot;, mult = log(n))
## 
## Standard deviation of noise (specified or estimated) sigma = 1.592
## 
## Testing results at step = 5, with alpha = 0.100
##  Var   Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea
##    1 -0.061 -12.032   0.000    -0.070   -0.053       0.049      0.049
##    5  3.735   5.775   0.060    -0.271    7.845       0.050      0.050
##    8  0.285   4.917   0.417    -0.716    0.345       0.050      0.049
##    7  0.000   3.292   0.340     0.000    0.000       0.050      0.050
##    6  0.011   2.902   0.369    -0.051    0.074       0.050      0.050
## 
## Estimated stopping point from AIC rule = 5</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Here we are in a reality case so AIC is not optimal anymore as in our simulation setting, we expect</span>

<span class="co"># something different while forward stop control the FRD. BIC has a harsh penalty that is why it </span>

<span class="co"># selects less variables. Moreover we know that, from previous practicals, our residuals inspection</span>

<span class="co"># told us that we had a right tail with respect to the Normal QQ plot (see ebook corrections). </span></code></pre></div>
</div>
<div id="SIS" class="section level3">
<h3><span class="header-section-number">7.3.2</span> Sure Independence Screening</h3>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Load the <em>Leukemia</em> dataset.<br />
</li>
<li>Split the dataset randomly and create a <em>train</em> and <em>test</em> sample.<br />
</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(SIS) <span class="co">#install it if you do not have it</span>

<span class="co"># Leukemia dataset taken from SIS package.</span>

train_set =<span class="st"> </span><span class="kw">data</span>(leukemia.train)

test_set =<span class="st"> </span><span class="kw">data</span>(<span class="st">&quot;leukemia.test&quot;</span>)

train_set =<span class="st"> </span><span class="kw">as.data.frame</span>(train_set)

test_set =<span class="st"> </span><span class="kw">as.data.frame</span>(test_set)


X_train =<span class="st"> </span><span class="kw">as.matrix</span>(leukemia.train[,<span class="dv">1</span><span class="op">:</span><span class="dv">7129</span>])

y_train =<span class="st"> </span>leukemia.train[,<span class="dv">7130</span>]


X_test =<span class="st"> </span><span class="kw">as.matrix</span>(leukemia.test[,<span class="dv">1</span><span class="op">:</span><span class="dv">7129</span>])

y_test =<span class="st"> </span>leukemia.test[,<span class="dv">7130</span>]</code></pre></div>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>The functions SIS() performs first a screening procedure based on marginal correlations and then applies a penalized method (<em>Chapter 4</em> of the e-book) to obtain the final model. Choose among all the available options (i.e. in terms both of penalized methods and tuning constants) three candidates and evaluate the predictions of the selected models on the <em>test</em> sample. Which penalized method performs best in this specific example after the SIS?</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Model Phase</span>

model_leuk_scad=<span class="kw">SIS</span>(X_train, y_train, <span class="dt">family=</span><span class="st">&#39;binomial&#39;</span>) <span class="co">#SCAD is default</span></code></pre></div>
<pre><code>## Iter 1 , screening:  3320 
## Iter 1 , selection:  3320 
## Iter 1 , conditional-screening:  847 
## Iter 2 , screening:  847 3320 
## Iter 2 , selection:  3320 
## Model already selected</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_leuk_lasso =<span class="st"> </span><span class="kw">SIS</span>(X_train, y_train, <span class="dt">family=</span><span class="st">&#39;binomial&#39;</span>,<span class="dt">penalty =</span> <span class="st">&#39;lasso&#39;</span>)</code></pre></div>
<pre><code>## Iter 1 , screening:  3320 
## Iter 1 , selection:  3320 
## Iter 1 , conditional-screening:  847 
## Iter 2 , screening:  847 3320 
## Iter 2 , selection:  847 3320 
## Maximum number of variables selected</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_leuk_mcp =<span class="st"> </span><span class="kw">SIS</span>(X_train, y_train, <span class="dt">family=</span><span class="st">&#39;binomial&#39;</span>,<span class="dt">penalty =</span> <span class="st">&#39;MCP&#39;</span>)</code></pre></div>
<pre><code>## Iter 1 , screening:  3320 
## Iter 1 , selection:  3320 
## Iter 1 , conditional-screening:  847 
## Iter 2 , screening:  847 3320 
## Iter 2 , selection:  3320 
## Model already selected</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Prediction Phase</span>

pred_scad=<span class="kw">predict</span>(model_leuk_scad,X_test,<span class="dt">type=</span><span class="st">&quot;class&quot;</span>)

<span class="kw">sum</span>(pred_scad <span class="op">==</span><span class="st"> </span>y_test)<span class="op">/</span><span class="dv">34</span> <span class="co">#0.7941176 </span></code></pre></div>
<pre><code>## [1] 0.7941176</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred_lasso=<span class="kw">predict</span>(model_leuk_lasso,X_test,<span class="dt">type=</span><span class="st">&quot;class&quot;</span>)

<span class="kw">sum</span>(pred_lasso <span class="op">==</span><span class="st"> </span>y_test)<span class="op">/</span><span class="dv">34</span> <span class="co">#  0.7941176</span></code></pre></div>
<pre><code>## [1] 0.7941176</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred_mcp=<span class="kw">predict</span>(model_leuk_mcp,X_test,<span class="dt">type=</span><span class="st">&quot;class&quot;</span>)

<span class="kw">sum</span>(pred_mcp <span class="op">==</span><span class="st"> </span>y_test)<span class="op">/</span><span class="dv">34</span> <span class="co"># 0.7941176</span></code></pre></div>
<pre><code>## [1] 0.7941176</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We got the same predictions for every method. </span>

<span class="co"># SCAD and MCP are near in terms of coefficient estimates but Lasso has one regressor more. </span></code></pre></div>
</div>
<div id="PC" class="section level3">
<h3><span class="header-section-number">7.3.3</span> PC-simple algorithm</h3>
<blockquote>
First of all build a simulation setting as explained below:<br />
- Generate from a MVN (multivariate normal) a matrix <span class="math inline">\(\mathbf{X_{n*p}}\)</span> with <span class="math inline">\(n = 1000\)</span> and <span class="math inline">\(p = 10\)</span>. Choose the location but set the scale matrix with an autoregressive form <span class="math inline">\(\boldsymbol{\Sigma}=[\sigma_{lm}]_{l,m=1,\ldots,p}\)</span> with <span class="math inline">\(\sigma_{lm} = \rho^{\mid l - m\mid}\)</span>.  - Fix <span class="math inline">\(\rho = 0.5\)</span> and set the seed equal to 11 (i.e. set.seed(11)).<br />
- Choose the generating vector <span class="math inline">\(\boldsymbol{\beta }= [3 \; 1.5 \; 0 \; 2 \; rep(0,6)]\)</span>.<br />
- Generate <span class="math inline">\(\mathbf{\hat{y}}\)</span> thanks to the relation <span class="math inline">\(\mathbf{y} = \mathbf{X_{n*p}} \; \boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span> where <span class="math inline">\(\epsilon_{i}\)</span> is a standard normal. Suppose for simplicity that the errors are uncorrelated.<br />

</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Simulation Setting

<span class="kw">require</span>(MASS)

<span class="kw">set.seed</span>(<span class="dv">11</span>)

n =<span class="st"> </span><span class="dv">100</span>

p =<span class="st"> </span><span class="dv">10</span> 

<span class="co"># change rho as you please to inspect other correlations among the predictors</span>

rho =<span class="st"> </span><span class="fl">0.5</span> 

mu =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,p)

sigma =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,p<span class="op">^</span><span class="dv">2</span>)

sigma =<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> sigma, <span class="dt">ncol =</span> p,<span class="dt">nrow =</span> p)


<span class="co"># Autoregressive structure</span>

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>p) {
  
  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>p) {
    
    
    sigma[i,j] =<span class="st"> </span>rho<span class="op">^</span>(<span class="kw">abs</span>(i<span class="op">-</span>j))
    
    
  }
  
  
}

X =<span class="st"> </span><span class="kw">mvrnorm</span>(n,mu,sigma)

<span class="kw">dim</span>(X)</code></pre></div>
<pre><code>## [1] 100  10</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">beta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">3</span>,<span class="fl">1.5</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">6</span>))


y =<span class="st"> </span>X<span class="op">%*%</span>beta <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n,<span class="dv">0</span>,<span class="dv">1</span>) <span class="co">#for us sigma is 1</span></code></pre></div>
<blockquote>
Now perform the following passages on your simulated data:<br />
a) Find the active set <span class="math inline">\(M_{1}\)</span> using the Fisher’s Z transformation and the associated correlation coefficient test (fix <span class="math inline">\(\alpha = 0.05\)</span> for the rest of the exercise).<br />

</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part a

<span class="co"># Fix alfa at 5% for all the exercise</span>

v_cor =<span class="st"> </span><span class="kw">cor</span>(<span class="dt">x =</span> X,<span class="dt">y =</span> y)

q=<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">10</span>)


<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10</span>) {
  
  <span class="cf">if</span>(<span class="kw">sqrt</span>(n<span class="op">-</span><span class="dv">3</span>)<span class="op">*</span><span class="kw">abs</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span><span class="op">*</span><span class="kw">log</span>((<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>v_cor[i])<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>v_cor[i]))) <span class="op">&gt;</span><span class="st"> </span><span class="fl">1.96</span> ) {
    
    q[i]=<span class="dv">1</span>
    
  }
  
}

A_<span class="dv">0</span> =<span class="st"> </span>X <span class="co">#Active set zero</span>

<span class="co"># With the first step we select the first 5 variables, we reject H0 so that we have evidence that the</span>

<span class="co"># correlation is different from 0. </span>

A_<span class="dv">1</span> =<span class="st"> </span>X[,<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>] <span class="co">#new active set</span></code></pre></div>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Calculate all the partial correlations of order 1 (i.e. one variable at the time) of the active set <span class="math inline">\(M_{1}\)</span>, test them and retrieve <span class="math inline">\(M_{2} \subseteq M_1\)</span> which is the new active set.<br />
</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part b

<span class="co"># We can now create the 20x1 vector of first order partial correlation and then apply previous formula</span>


P =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">25</span>)

P =<span class="st"> </span><span class="kw">matrix</span>(P,<span class="dt">nrow =</span> <span class="dv">5</span>,<span class="dt">ncol =</span> <span class="dv">5</span>)



<span class="cf">for</span> (z <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {
  
  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {
    
    <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>z ) {
      
      
      mod_<span class="dv">1</span> =<span class="st"> </span><span class="kw">lm</span>(A_<span class="dv">1</span>[,z] <span class="op">~</span><span class="st"> </span>A_<span class="dv">1</span>[,i])
      
      e_<span class="dv">1</span> =<span class="st"> </span>A_<span class="dv">1</span>[,z] <span class="op">-</span><span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,n),A_<span class="dv">1</span>[,i])<span class="op">%*%</span>mod_<span class="dv">1</span><span class="op">$</span>coefficients
      
      mod_<span class="dv">2</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>A_<span class="dv">1</span>[,i])
      
      e_<span class="dv">2</span> =<span class="st">  </span>y <span class="op">-</span><span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,n),A_<span class="dv">1</span>[,i])<span class="op">%*%</span>mod_<span class="dv">2</span><span class="op">$</span>coefficients
      
      
      P[i,z] =<span class="st"> </span><span class="kw">cor</span>(e_<span class="dv">1</span>,e_<span class="dv">2</span>) 
      
      
    }
    
  }
  
  
}

P =<span class="st"> </span>P <span class="op">+</span><span class="st"> </span><span class="kw">diag</span>(<span class="dv">5</span>)

v_cor =<span class="st"> </span><span class="kw">c</span>(P) <span class="co">#vectorization</span>

<span class="co"># fix alfa at 5% and test the sample correlation.</span>

q=<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">25</span>)


<span class="co"># Cardinality of the active set is now 1 because we are evaluating partial correaltions of order 1</span>

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">25</span>) {
  
  <span class="cf">if</span>(<span class="kw">sqrt</span>(n<span class="op">-</span><span class="dv">4</span>)<span class="op">*</span><span class="kw">abs</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span><span class="op">*</span><span class="kw">log</span>((<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>v_cor[i])<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span>v_cor[i]))) <span class="op">&gt;</span><span class="st"> </span><span class="fl">1.96</span> ) {
    
    q[i]=<span class="dv">1</span>
    
  }
  
}

q <span class="co"># variables 3 and 5 are eliminated from the active set</span>

A_<span class="dv">2</span> =<span class="st"> </span>X[,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>)]</code></pre></div>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Find the partial correlations of higher order and test them until your reach the condition <span class="math inline">\(M_{m-1} = M_{m}\)</span> which implies the convergence of the PC-simple algorithm. Do you obtain the exact model?</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part c

<span class="co"># Now we need to check 2nd order partial correlation to discriminate.</span>

<span class="co"># Note: we could also update estimation for partial correlation in order to speed up the computations</span>

<span class="co"># but in this easy example is not necessary. </span>

<span class="co"># VARIABLE 1 ACTIVE SET A_2</span>

mod_<span class="dv">1</span> =<span class="st"> </span><span class="kw">lm</span>(A_<span class="dv">2</span>[,<span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">3</span>])

e_<span class="dv">1</span> =<span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,n),A_<span class="dv">2</span>[,<span class="dv">2</span>],A_<span class="dv">2</span>[,<span class="dv">3</span>])<span class="op">%*%</span>mod_<span class="dv">1</span><span class="op">$</span>coefficients

mod_<span class="dv">2</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">3</span>])

e_<span class="dv">2</span> =<span class="st">  </span>y <span class="op">-</span><span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,n),A_<span class="dv">2</span>[,<span class="dv">2</span>],A_<span class="dv">2</span>[,<span class="dv">3</span>])<span class="op">%*%</span>mod_<span class="dv">2</span><span class="op">$</span>coefficients

<span class="co"># Cardinality of active set now is two!</span>

<span class="kw">sqrt</span>(n<span class="op">-</span><span class="dv">5</span>)<span class="op">*</span><span class="kw">abs</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span><span class="op">*</span><span class="kw">log</span>((<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">cor</span>(e_<span class="dv">1</span>,e_<span class="dv">2</span>))<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">cor</span>(e_<span class="dv">1</span>,e_<span class="dv">2</span>)))) <span class="op">&gt;</span><span class="st"> </span><span class="fl">1.96</span>

<span class="co"># VARIABLE 2 ACTIVE SET A_2</span>

mod_<span class="dv">1</span> =<span class="st"> </span><span class="kw">lm</span>(A_<span class="dv">2</span>[,<span class="dv">2</span>] <span class="op">~</span><span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">3</span>])

e_<span class="dv">1</span> =<span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,n),A_<span class="dv">2</span>[,<span class="dv">1</span>],A_<span class="dv">2</span>[,<span class="dv">3</span>])<span class="op">%*%</span>mod_<span class="dv">1</span><span class="op">$</span>coefficients

mod_<span class="dv">2</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">3</span>])

e_<span class="dv">2</span> =<span class="st">  </span>y <span class="op">-</span><span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,n),A_<span class="dv">2</span>[,<span class="dv">1</span>],A_<span class="dv">2</span>[,<span class="dv">3</span>])<span class="op">%*%</span>mod_<span class="dv">2</span><span class="op">$</span>coefficients

<span class="co"># Cardinality of active set now is two!</span>

<span class="kw">sqrt</span>(n<span class="op">-</span><span class="dv">5</span>)<span class="op">*</span><span class="kw">abs</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span><span class="op">*</span><span class="kw">log</span>((<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">cor</span>(e_<span class="dv">1</span>,e_<span class="dv">2</span>))<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">cor</span>(e_<span class="dv">1</span>,e_<span class="dv">2</span>)))) <span class="op">&gt;</span><span class="st"> </span><span class="fl">1.96</span>

<span class="co"># VARIABLE 3 ACTIVE SET A_2</span>

mod_<span class="dv">1</span> =<span class="st"> </span><span class="kw">lm</span>(A_<span class="dv">2</span>[,<span class="dv">3</span>] <span class="op">~</span><span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">2</span>])

e_<span class="dv">1</span> =<span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">3</span>] <span class="op">-</span><span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,n),A_<span class="dv">2</span>[,<span class="dv">1</span>],A_<span class="dv">2</span>[,<span class="dv">2</span>])<span class="op">%*%</span>mod_<span class="dv">1</span><span class="op">$</span>coefficients

mod_<span class="dv">2</span> =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>A_<span class="dv">2</span>[,<span class="dv">2</span>])

e_<span class="dv">2</span> =<span class="st">  </span>y <span class="op">-</span><span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,n),A_<span class="dv">2</span>[,<span class="dv">1</span>],A_<span class="dv">2</span>[,<span class="dv">2</span>])<span class="op">%*%</span>mod_<span class="dv">2</span><span class="op">$</span>coefficients

<span class="co"># Cardinality of active set now is two!</span>

<span class="kw">sqrt</span>(n<span class="op">-</span><span class="dv">5</span>)<span class="op">*</span><span class="kw">abs</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">2</span><span class="op">*</span><span class="kw">log</span>((<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">cor</span>(e_<span class="dv">1</span>,e_<span class="dv">2</span>))<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">cor</span>(e_<span class="dv">1</span>,e_<span class="dv">2</span>)))) <span class="op">&gt;</span><span class="st"> </span><span class="fl">1.96</span>

<span class="co"># Conclusions: all conditions are true, then the algorithm will not move anymore and we have </span>

<span class="co"># our final model with the original variables c(1,2,4) which is also the exact one. </span></code></pre></div>
</div>
<div id="RT" class="section level3">
<h3><span class="header-section-number">7.3.4</span> Regression Tree</h3>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Load the <em>Zambia</em> dataset, split it randomly in a <em>train</em> and <em>test</em> sample (common choice is <span class="math inline">\(\frac{2}{3}\)</span> train and <span class="math inline">\(\frac{1}{3}\)</span> test). For simplicity, you can consider only the continuous variables.<br />
</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(rpart)

<span class="kw">load</span>(<span class="st">&quot;malnutrion_zambia_cleaned.Rda&quot;</span>)


data_zambia =<span class="st"> </span>data_zambia[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>,<span class="dv">21</span>,<span class="dv">22</span>,<span class="dv">24</span>)] <span class="co">#exclude the factors from the analysis</span>

X =<span class="st"> </span><span class="kw">as.matrix</span>(data_zambia[,<span class="dv">2</span><span class="op">:</span><span class="dv">10</span>])

y =<span class="st"> </span>data_zambia[,<span class="dv">1</span>]


## Part a

ind =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">1927</span>

index =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> ind,<span class="dt">size =</span> <span class="dv">1284</span>,<span class="dt">replace =</span> F)

data_zambia_train=<span class="st"> </span>data_zambia[index,]

data_zambia_test=<span class="st"> </span>data_zambia[<span class="op">-</span>index,]</code></pre></div>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Fit a regression tree with the function rpart() and plot the tree. Have a look at <em>rpart.plot</em> package if you want to improve the appearance of the fitted tree.  </li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part b

<span class="co"># grow tree </span>

<span class="kw">attach</span>(data_zambia_train)

fit &lt;-<span class="st"> </span><span class="kw">rpart</span>(<span class="st">`</span><span class="dt">Height for age sd</span><span class="st">`</span><span class="op">~</span><span class="st"> </span>
<span class="st">               `</span><span class="dt">Breastfeeding duration (months)</span><span class="st">`</span> <span class="op">+</span><span class="st"> `</span><span class="dt">Age of the child (months)</span><span class="st">`</span><span class="op">+</span><span class="st">`</span><span class="dt">Age of the mother (years)</span><span class="st">`</span><span class="op">+</span><span class="st">`</span><span class="dt">BMI mother</span><span class="st">`</span><span class="op">+</span><span class="st">`</span><span class="dt">Heigth mother (meter)</span><span class="st">`</span><span class="op">+</span><span class="st">`</span><span class="dt">Weight mother (kg)</span><span class="st">`</span><span class="op">+</span><span class="st">`</span><span class="dt">Wealth index factor score</span><span class="st">`</span><span class="op">+</span><span class="st">`</span><span class="dt">Child weight at birth (kg)</span><span class="st">`</span><span class="op">+</span><span class="st">`</span><span class="dt">Interval between births</span><span class="st">`</span>, 
             <span class="dt">method=</span><span class="st">&quot;anova&quot;</span>, <span class="dt">data=</span>data_zambia_train)

<span class="kw">printcp</span>(fit) <span class="co"># display the results </span></code></pre></div>
<pre><code>## 
## Regression tree:
## rpart(formula = `Height for age sd` ~ `Breastfeeding duration (months)` + 
##     `Age of the child (months)` + `Age of the mother (years)` + 
##     `BMI mother` + `Heigth mother (meter)` + `Weight mother (kg)` + 
##     `Wealth index factor score` + `Child weight at birth (kg)` + 
##     `Interval between births`, data = data_zambia_train, method = &quot;anova&quot;)
## 
## Variables actually used in tree construction:
## [1] Age of the child (months)  Child weight at birth (kg)
## [3] Heigth mother (meter)      Interval between births   
## [5] Wealth index factor score  Weight mother (kg)        
## 
## Root node error: 3949.2/1284 = 3.0757
## 
## n= 1284 
## 
##         CP nsplit rel error  xerror     xstd
## 1 0.085434      0   1.00000 1.00140 0.049312
## 2 0.027354      1   0.91457 0.92121 0.043830
## 3 0.014385      2   0.88721 0.91542 0.044182
## 4 0.013865      3   0.87283 0.94016 0.045852
## 5 0.011370      4   0.85896 0.94585 0.046185
## 6 0.010284      5   0.84759 0.94109 0.045973
## 7 0.010160      7   0.82702 0.94040 0.045758
## 8 0.010000      8   0.81686 0.94034 0.045801</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plotcp</span>(fit) <span class="co"># visualize cross-validation results </span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-66-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># summary(fit) detailed summary of splits</span>

<span class="kw">library</span>(rattle)
<span class="kw">library</span>(rpart.plot)
<span class="kw">library</span>(RColorBrewer)

<span class="kw">fancyRpartPlot</span>(fit)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-66-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">detach</span>(data_zambia_train)</code></pre></div>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>After having pruned the tree, evaluate its prediction on the <em>test</em> sample (i.e. use predict() on a tree object.</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part c

<span class="kw">attach</span>(data_zambia_train)

<span class="co"># prune the tree</span>

fit<span class="op">$</span>cptable[<span class="kw">which.min</span>(fit<span class="op">$</span>cptable[,<span class="st">&quot;xerror&quot;</span>]),<span class="st">&quot;CP&quot;</span>]</code></pre></div>
<pre><code>## [1] 0.01438482</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit<span class="op">$</span>cptable</code></pre></div>
<pre><code>##           CP nsplit rel error    xerror       xstd
## 1 0.08543400      0 1.0000000 1.0013950 0.04931249
## 2 0.02735374      1 0.9145660 0.9212097 0.04382993
## 3 0.01438482      2 0.8872123 0.9154191 0.04418249
## 4 0.01386532      3 0.8728274 0.9401599 0.04585160
## 5 0.01137031      4 0.8589621 0.9458524 0.04618497
## 6 0.01028396      5 0.8475918 0.9410920 0.04597282
## 7 0.01016050      7 0.8270239 0.9404004 0.04575772
## 8 0.01000000      8 0.8168634 0.9403443 0.04580068</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pfit&lt;-<span class="st"> </span><span class="kw">prune</span>(fit, <span class="dt">cp=</span><span class="fl">0.02406722</span>) <span class="co"># from cptable </span>

<span class="kw">fancyRpartPlot</span>(pfit) </code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-67-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Notice the different tree after pruning: Age of the child and Weight of the mother stays relevant</span>

<span class="co"># while other variables disappear. </span>

Prediction &lt;-<span class="st"> </span><span class="kw">predict</span>(pfit, data_zambia_test, <span class="dt">type =</span> <span class="st">&quot;vector&quot;</span>) 

<span class="co"># We can use the mean squared error to evaluate the predictions</span>

mse =<span class="st"> </span><span class="kw">sum</span>((Prediction <span class="op">-</span><span class="st"> </span>data_zambia_test<span class="op">$</span><span class="st">`</span><span class="dt">Height for age sd</span><span class="st">`</span>)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">length</span>(Prediction)

<span class="kw">detach</span>(data_zambia_train)

<span class="co"># Now we should find a benchmark to understand how well is doing the Regression Tree</span>

<span class="co"># In this case we will use linear regression plus model selection thanks to FDR rule.</span>

X_train =<span class="st"> </span><span class="kw">as.matrix</span>(data_zambia_train[,<span class="dv">2</span><span class="op">:</span><span class="dv">10</span>])


mod_zambia =<span class="st"> </span><span class="kw">lm</span>(data_zambia_train[,<span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>X_train, <span class="dt">data =</span> data_zambia_train)

object =<span class="st"> </span><span class="kw">summary</span>(mod_zambia)

<span class="co"># Fix a level q = 0.05 of FDR (see Hochberg&amp;Benjamini), order the p-values and we reject three H0 thus</span>

<span class="co"># selecting variables: Breastfeeding duration, Weight at birth and wealth index factor scores.</span>

mod_zambia_final =<span class="st"> </span><span class="kw">lm</span>(data_zambia_train[,<span class="dv">1</span>] <span class="op">~</span><span class="st"> </span>X_train[,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">7</span>,<span class="dv">8</span>)], <span class="dt">data =</span> data_zambia_train)

X_test =<span class="st"> </span><span class="kw">as.matrix</span>(data_zambia_test[,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">8</span>,<span class="dv">9</span>)])

predictions_linear =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">643</span>),X_test)<span class="op">%*%</span>mod_zambia_final<span class="op">$</span>coefficients

mse_linear =<span class="st"> </span><span class="kw">sum</span>((predictions_linear <span class="op">-</span><span class="st"> </span>data_zambia_test<span class="op">$</span><span class="st">`</span><span class="dt">Height for age sd</span><span class="st">`</span>)<span class="op">^</span><span class="dv">2</span>)<span class="op">/</span><span class="kw">length</span>(Prediction)

<span class="co"># Similar to the one of the Regression Tree so there is not a big improvement.</span>

mse</code></pre></div>
<pre><code>## [1] 2.401796</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mse_linear</code></pre></div>
<pre><code>## [1] 2.367926</code></pre>
</div>
<div id="CT" class="section level3">
<h3><span class="header-section-number">7.3.5</span> Classification Tree</h3>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Load the <em>Iris</em> dataset already present in R, split it randomly in a <em>train</em> and <em>test</em> sample (common choice is <span class="math inline">\(\frac{2}{3}\)</span> train and <span class="math inline">\(\frac{1}{3}\)</span> test). </li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part a

data_iris =<span class="st"> </span>iris

ind =<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">150</span>

index =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> ind,<span class="dt">size =</span> <span class="dv">100</span>,<span class="dt">replace =</span> F)

train=<span class="st"> </span>data_iris[index,]

test=<span class="st"> </span>data_iris[<span class="op">-</span>index,]</code></pre></div>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Fit a classification tree with the function rpart() and plot the tree. Have a look at <em>rpart.plot</em> package if you want to improve the appearance of the fitted tree. </li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part b

<span class="kw">attach</span>(train) 

fit_iris =<span class="st"> </span><span class="kw">rpart</span>(Species <span class="op">~</span><span class="st"> </span>Sepal.Length <span class="op">+</span><span class="st"> </span>Sepal.Width <span class="op">+</span><span class="st"> </span>Petal.Length <span class="op">+</span><span class="st"> </span>Petal.Width,
            <span class="dt">data=</span>train,
            <span class="dt">method=</span><span class="st">&quot;class&quot;</span>)


<span class="kw">printcp</span>(fit_iris) <span class="co"># display the results </span></code></pre></div>
<pre><code>## 
## Classification tree:
## rpart(formula = Species ~ Sepal.Length + Sepal.Width + Petal.Length + 
##     Petal.Width, data = train, method = &quot;class&quot;)
## 
## Variables actually used in tree construction:
## [1] Petal.Length Petal.Width 
## 
## Root node error: 63/100 = 0.63
## 
## n= 100 
## 
##        CP nsplit rel error  xerror     xstd
## 1 0.50794      0  1.000000 1.07937 0.074044
## 2 0.42857      1  0.492063 0.49206 0.073412
## 3 0.01000      2  0.063492 0.12698 0.043062</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plotcp</span>(fit_iris) <span class="co"># visualize cross-validation results </span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fancyRpartPlot</span>(fit_iris)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-69-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">detach</span>(train)</code></pre></div>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>After having pruned the tree, evaluate its prediction on the <em>test</em> sample (i.e. use predict() on a tree object). </li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part c

<span class="co"># prune the tree </span>

pfit_iris&lt;-<span class="st"> </span><span class="kw">prune</span>(fit_iris, <span class="dt">cp=</span>   fit<span class="op">$</span>cptable[<span class="kw">which.min</span>(fit<span class="op">$</span>cptable[,<span class="st">&quot;xerror&quot;</span>]),<span class="st">&quot;CP&quot;</span>])

<span class="kw">fancyRpartPlot</span>(pfit_iris)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Prediction &lt;-<span class="st"> </span><span class="kw">predict</span>(fit_iris, test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>) <span class="co">#pruned or not pruned trees are equal in this example</span>

t =<span class="st"> </span>Prediction <span class="op">==</span><span class="st"> </span>test<span class="op">$</span>Species

<span class="kw">sum</span>(t)<span class="op">/</span><span class="kw">length</span>(test<span class="op">$</span>Species)</code></pre></div>
<pre><code>## [1] 0.96</code></pre>
</div>
</div>
<div id="chapter-4" class="section level2">
<h2><span class="header-section-number">7.4</span> Chapter 4</h2>
<div id="Ridge" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Ridge Regression</h3>
<ul>
<li>Use the function <em>glmnet()</em> to perform a Ridge regression on <em>Zambia</em> dataset, plot the values as a function of <span class="math inline">\(\lambda\)</span> and comment on the results.<br />
</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(glmnet)

<span class="kw">load</span>(<span class="st">&quot;malnutrion_zambia_cleaned.Rda&quot;</span>)


data_zambia =<span class="st"> </span>data_zambia[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>,<span class="dv">21</span>,<span class="dv">22</span>,<span class="dv">24</span>)] <span class="co">#exclude the factors from the analysis</span>

X =<span class="st"> </span><span class="kw">as.matrix</span>(data_zambia[,<span class="dv">2</span><span class="op">:</span><span class="dv">10</span>])

y =<span class="st"> </span>data_zambia[,<span class="dv">1</span>]

## Part a

<span class="co"># Ridge penalty is obtained with parameter alpha = 0</span>

mod_pen_L2 =<span class="st"> </span><span class="kw">glmnet</span>(X,y,<span class="dt">family =</span> <span class="st">&quot;gaussian&quot;</span>,<span class="dt">alpha =</span> <span class="dv">0</span>)

<span class="kw">plot</span>(mod_pen_L2, <span class="dt">xvar=</span><span class="st">&quot;lambda&quot;</span>) <span class="co">#more smoothed, no model selection property just shrinkage</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># mod_pen_L2$beta shrinked but all present</span></code></pre></div>
<ul>
<li>Use the function <em>cv.glmnet()</em> to perform model selection based on 10-fold Cross Validation (i.e. method to select the <span class="math inline">\(\lambda\)</span> parameter), plot the results and comment the graph that you obtain. Which values of <span class="math inline">\(\lambda\)</span> are shown by default?<br />
</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part b

<span class="co"># Selection based on 10 fold CV</span>

cv.zero_L2 &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(X,y, <span class="dt">alpha=</span><span class="dv">0</span>) <span class="co">#a way in which you can select lambda (10k CV)</span>

cv.zero_L2<span class="op">$</span>lambda.min</code></pre></div>
<pre><code>## [1] 0.08504366</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(cv.zero_L2) </code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-72-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The two vertical lines are respectively: the value of lambda that minimizes the CV error curve and </span>

<span class="co"># the value lambda.1SE which is one standard error from lambda.min on the right.</span></code></pre></div>
<ul>
<li>Use the function <em>predict()</em> to retrieve the final model estimates and perform a simple linear model on the same covariates, what can you conclude?</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part c

<span class="co"># Easier with predict rather than check all the order based on the single value of lambda</span>

### MESSAGE FROM HASTIE AND TIBSHIRANI <span class="al">###</span>

<span class="co"># Use directly lambda.min it is prone to errors</span>

<span class="co"># </span><span class="al">WARNING</span><span class="co">: use with care. Do not supply a single value for lambda (for predictions after CV use predict() instead). </span>

<span class="co"># Supply instead a decreasing sequence of lambda values. glmnet relies on its warms starts for speed, and its often </span>

<span class="co"># faster to fit a whole path than compute a single fit.</span>


coef_L2_min =<span class="st"> </span><span class="kw">predict</span>(cv.zero_L2,<span class="dt">type=</span><span class="st">&quot;coefficient&quot;</span>,<span class="dt">s =</span> <span class="st">&quot;lambda.min&quot;</span>)

coef_L2_1SE =<span class="st"> </span><span class="kw">predict</span>(cv.zero_L2,<span class="dt">type=</span><span class="st">&quot;coefficient&quot;</span>)  <span class="co">#default is lambda.1SE</span>

model_linear =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>.,<span class="dt">data =</span> data_zambia[,<span class="kw">c</span>(<span class="dv">2</span><span class="op">:</span><span class="dv">10</span>)])

<span class="kw">summary</span>(model_linear)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ ., data = data_zambia[, c(2:10)])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.4167 -0.9869 -0.0761  0.8483  7.9208 
## 
## Coefficients:
##                                     Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                       -4.646e+00  5.439e+00  -0.854   0.3931
## `Breastfeeding duration (months)` -5.273e-02  6.866e-03  -7.681 2.50e-14
## `Age of the child (months)`       -5.574e-03  2.928e-03  -1.904   0.0571
## `Age of the mother (years)`        1.170e-02  6.485e-03   1.804   0.0715
## `BMI mother`                      -8.108e-02  1.186e-01  -0.683   0.4944
## `Heigth mother (meter)`            1.348e+00  3.423e+00   0.394   0.6938
## `Weight mother (kg)`               4.251e-02  4.674e-02   0.909   0.3632
## `Wealth index factor score`        1.261e-06  3.968e-07   3.178   0.0015
## `Child weight at birth (kg)`       2.851e-01  5.813e-02   4.904 1.02e-06
## `Interval between births`          3.250e-03  1.987e-03   1.636   0.1021
##                                      
## (Intercept)                          
## `Breastfeeding duration (months)` ***
## `Age of the child (months)`       .  
## `Age of the mother (years)`       .  
## `BMI mother`                         
## `Heigth mother (meter)`              
## `Weight mother (kg)`                 
## `Wealth index factor score`       ** 
## `Child weight at birth (kg)`      ***
## `Interval between births`            
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.592 on 1917 degrees of freedom
## Multiple R-squared:  0.1339, Adjusted R-squared:  0.1298 
## F-statistic: 32.93 on 9 and 1917 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We observe the shrinkage property of L2 norms penalties, the coefficients are shrinked to achieve a better compromise between bias and variance. For supplementary theory see Stein&#39;s estimators.</span></code></pre></div>
</div>
<div id="lasso" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Lasso</h3>
<ul>
<li>Use the function <em>glmnet()</em> to perform a lasso on <em>Zambia</em> dataset, plot the values as a function of <span class="math inline">\(\lambda\)</span> and comment on the results.<br />
</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part 1

<span class="co"># Lasso penalty is obtained by setting alpha equal to one</span>

mod_pen =<span class="st"> </span><span class="kw">glmnet</span>(X,y,<span class="dt">family =</span> <span class="st">&quot;gaussian&quot;</span>,<span class="dt">alpha =</span> <span class="dv">1</span>)


<span class="co"># mod_pen$beta # here you can see order of the lasso</span>


<span class="kw">plot</span>(mod_pen, <span class="dt">xvar=</span><span class="st">&quot;lambda&quot;</span>) <span class="co">#plot coefficients as lambda varys, use lambda found below to select. </span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-74-1.png" width="672" /></p>
<ul>
<li>Use the function <em>cv.glmnet()</em> to perform model selection based on 10-fold Cross Validation (i.e. method to select the <span class="math inline">\(\lambda\)</span> parameter), plot the results and comment the graph that you obtain. Which values of <span class="math inline">\(\lambda\)</span> are shown by default? What can you conclude on the choice of <span class="math inline">\(\lambda\)</span> in terms of model selection?<br />
</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part 2

<span class="co"># Selection based on 10 fold CV</span>

cv.zero_mod &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(X,y, <span class="dt">alpha=</span><span class="dv">1</span>) <span class="co">#a way in which you can select lambda (10k CV)</span>

cv.zero_mod<span class="op">$</span>lambda.min</code></pre></div>
<pre><code>## [1] 0.002010848</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(cv.zero_mod) </code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-75-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The two vertical lines are respectively: the value of lambda that minimizes the CV error curve and </span>

<span class="co"># the value lambda.1SE which is one standard error from lambda.min in the direction of a more </span>

<span class="co"># parsimonious model.</span></code></pre></div>
<ul>
<li>Use the function <em>predict()</em> to retrieve the final model chosen by 10-fold CV (given lasso ordering) and perform a linear model on the covariates present in the final model. What can you conclude observing the estimates?</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Part 3

mod_pen =<span class="st"> </span><span class="kw">glmnet</span>(X,y,<span class="dt">family =</span> <span class="st">&quot;gaussian&quot;</span>,<span class="dt">alpha =</span> <span class="dv">1</span>,<span class="dt">lambda =</span> <span class="fl">0.00201</span>) <span class="co">#wrong, do not use this way!</span>

<span class="co"># The right way is to use the predict function</span>

coef_L1_min =<span class="st"> </span><span class="kw">predict</span>(cv.zero_mod,<span class="dt">type=</span><span class="st">&quot;coefficient&quot;</span>,<span class="dt">s =</span> <span class="st">&quot;lambda.min&quot;</span>)

coef_L1_1SE =<span class="st"> </span><span class="kw">predict</span>(cv.zero_mod,<span class="dt">type=</span><span class="st">&quot;coefficient&quot;</span>)  <span class="co">#default is lambda.1SE</span>

<span class="co"># As you can notice, lambda min tends to select more regressors so it is less parsimonious then 1.SE</span>


model_linear =<span class="st"> </span><span class="kw">lm</span>(y <span class="op">~</span><span class="st"> </span>.,<span class="dt">data =</span> data_zambia[,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">6</span><span class="op">:</span><span class="dv">9</span>)])

<span class="kw">summary</span>(model_linear)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ ., data = data_zambia[, c(2, 3, 6:9)])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.4782 -0.9864 -0.0534  0.8430  7.8345 
## 
## Coefficients:
##                                     Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                       -7.971e+00  9.654e-01  -8.257 2.75e-16
## `Breastfeeding duration (months)` -5.196e-02  6.863e-03  -7.571 5.74e-14
## `Age of the child (months)`       -5.901e-03  2.924e-03  -2.018 0.043726
## `Heigth mother (meter)`            3.675e+00  6.483e-01   5.668 1.66e-08
## `Weight mother (kg)`               1.203e-02  3.970e-03   3.031 0.002469
## `Wealth index factor score`        1.311e-06  3.916e-07   3.349 0.000827
## `Child weight at birth (kg)`       2.822e-01  5.811e-02   4.856 1.30e-06
##                                      
## (Intercept)                       ***
## `Breastfeeding duration (months)` ***
## `Age of the child (months)`       *  
## `Heigth mother (meter)`           ***
## `Weight mother (kg)`              ** 
## `Wealth index factor score`       ***
## `Child weight at birth (kg)`      ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.594 on 1920 degrees of freedom
## Multiple R-squared:  0.1302, Adjusted R-squared:  0.1275 
## F-statistic: 47.91 on 6 and 1920 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Lasso estimates are known to be biased and we are not even sure about the signs if dimensions of predictor space </span>

<span class="co"># is greater than 2. There are methods which are called &quot;debiasing the lasso&quot; that you can search if you are interested by this topic. Of course here we are using real data so we have always to be aware of the assumptions of each model that we are fitting. </span></code></pre></div>
</div>
<div id="nonconvex" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Non Convex Penalties</h3>
<ul>
<li>Fix the generating vector <span class="math inline">\(\boldsymbol{\beta}=(4,2,-4,-2,0,0,\ldots,0)\)</span> and set the seed equal to 11 (i.e. set.seed(11)).</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(MASS)

<span class="kw">set.seed</span>(<span class="dv">11</span>)

beta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">2</span>,<span class="op">-</span><span class="dv">4</span>,<span class="op">-</span><span class="dv">2</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">996</span>))</code></pre></div>
<ul>
<li>Generate from a MVN (multivariate normal) a matrix <span class="math inline">\(\mathbf{X_{n*p}}\)</span> with <span class="math inline">\(n = 200\)</span> and <span class="math inline">\(p = 1000\)</span>. You can choose the location vector as you wish but set the scale matrix with an autoregressive form <span class="math inline">\(\boldsymbol{\Sigma}=[\sigma_{lm}]_{l,m=1,\ldots,p}\)</span> with <span class="math inline">\(\sigma_{lm} = \rho^{\mid l - m\mid}\)</span>.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n =<span class="st"> </span><span class="dv">200</span> 

p =<span class="st"> </span><span class="dv">1000</span> 

<span class="co"># Three values needed for rho = (0 0.2 0.5)</span>

rho =<span class="st"> </span><span class="dv">0</span> 

mu =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>,p)

sigma =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,p<span class="op">^</span><span class="dv">2</span>)

sigma =<span class="st"> </span><span class="kw">matrix</span>(<span class="dt">data =</span> sigma, <span class="dt">ncol =</span> p,<span class="dt">nrow =</span> p)


<span class="co"># Autoregressive structure</span>

<span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>p) {
  
  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>p) {
    
    
    sigma[i,j] =<span class="st"> </span>rho<span class="op">^</span>(<span class="kw">abs</span>(i<span class="op">-</span>j))
    
    
  }
  
  
}

X =<span class="st"> </span><span class="kw">mvrnorm</span>(n,mu,sigma)</code></pre></div>
<ul>
<li>For each <span class="math inline">\(\boldsymbol{\rho} = [0 \; 0.2 \; 0.5]\)</span> generate <span class="math inline">\(\mathbf{\hat{y}}\)</span> thanks to the relation <span class="math inline">\(\mathbf{y} = \mathbf{X_{n*p}} \; \boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span> where <span class="math inline">\(\epsilon_{i}\)</span> is a standard normal. Suppose for simplicity that the errors are uncorrelated.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">y =<span class="st"> </span>X<span class="op">%*%</span>beta <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n,<span class="dv">0</span>,<span class="dv">1</span>) <span class="co">#for us sigma is 1</span></code></pre></div>
<ul>
<li>Compare the solution paths (graphically as a function of <span class="math inline">\(\lambda\)</span>) for the lasso, SCAD and MCP by fixing several values for <span class="math inline">\(\gamma\)</span> (choose e.g. <span class="math inline">\(\gamma=(1.5, 2, 3, 3.7, 5)\)</span>) for each value of <span class="math inline">\(\rho\)</span> indicated at previous point.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Paths will be presented for gamma = (2, default values, 5)</span>

<span class="co"># The value 1.5 for gamma is not possible for SCAD while 3 and 3.7 are the default value for MCP and Scad respectively</span>

<span class="kw">require</span>(ncvreg)

<span class="co"># Default gamma</span>

model_scad =<span class="st"> </span><span class="kw">ncvreg</span>(<span class="dt">X =</span> X,<span class="dt">y =</span> y,<span class="dt">family =</span> <span class="st">&quot;gaussian&quot;</span>,<span class="dt">penalty =</span> <span class="st">&quot;SCAD&quot;</span>) <span class="co">#gamma default is 3.7</span>

model_mcp =<span class="st"> </span><span class="kw">ncvreg</span>(<span class="dt">X =</span> X,<span class="dt">y =</span> y,<span class="dt">family =</span> <span class="st">&quot;gaussian&quot;</span>,<span class="dt">penalty =</span> <span class="st">&quot;MCP&quot;</span>) <span class="co">#gamma default is 3</span>

model_lasso =<span class="st"> </span><span class="kw">ncvreg</span>(<span class="dt">X =</span> X,<span class="dt">y =</span> y,<span class="dt">family =</span> <span class="st">&quot;gaussian&quot;</span>,<span class="dt">penalty =</span> <span class="st">&quot;lasso&quot;</span>)


<span class="co"># Gamma = 2</span>

model_scad_<span class="dv">2</span> =<span class="st"> </span><span class="kw">ncvreg</span>(<span class="dt">X =</span> X,<span class="dt">y =</span> y,<span class="dt">family =</span> <span class="st">&quot;gaussian&quot;</span>,<span class="dt">penalty =</span> <span class="st">&quot;SCAD&quot;</span>,<span class="dt">gamma =</span> <span class="fl">2.1</span>) <span class="co">#no scad for gamma lower than 2</span>

model_mcp_<span class="dv">2</span> =<span class="st"> </span><span class="kw">ncvreg</span>(<span class="dt">X =</span> X,<span class="dt">y =</span> y,<span class="dt">family =</span> <span class="st">&quot;gaussian&quot;</span>,<span class="dt">penalty =</span> <span class="st">&quot;MCP&quot;</span>, <span class="dt">gamma =</span> <span class="dv">2</span>)


<span class="co"># Gamma = 5</span>

model_scad_<span class="dv">5</span> =<span class="st"> </span><span class="kw">ncvreg</span>(<span class="dt">X =</span> X,<span class="dt">y =</span> y,<span class="dt">family =</span> <span class="st">&quot;gaussian&quot;</span>,<span class="dt">penalty =</span> <span class="st">&quot;SCAD&quot;</span>, <span class="dt">gamma =</span> <span class="dv">5</span>)

model_mcp_<span class="dv">5</span> =<span class="st"> </span><span class="kw">ncvreg</span>(<span class="dt">X =</span> X,<span class="dt">y =</span> y,<span class="dt">family =</span> <span class="st">&quot;gaussian&quot;</span>,<span class="dt">penalty =</span> <span class="st">&quot;MCP&quot;</span>, <span class="dt">gamma =</span> <span class="dv">5</span>)




<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))

<span class="co"># gamma = 2</span>

<span class="kw">plot</span>(model_lasso,<span class="dt">main=</span><span class="st">&quot;Lasso&quot;</span>)
<span class="kw">plot</span>(model_scad_<span class="dv">2</span>, <span class="dt">main=</span><span class="st">&quot;Scad - Gamma 2.1&quot;</span>)
<span class="kw">plot</span>(model_mcp_<span class="dv">2</span>, <span class="dt">main=</span><span class="st">&quot;MCP - Gamma 2&quot;</span>)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))

<span class="co">#default gamma</span>

<span class="kw">plot</span>(model_lasso,<span class="dt">main=</span><span class="st">&quot;Lasso&quot;</span>)
<span class="kw">plot</span>(model_scad,<span class="dt">main=</span><span class="st">&quot;Scad - Gamma 3.7&quot;</span>)
<span class="kw">plot</span>(model_mcp, <span class="dt">main=</span><span class="st">&quot;MCP - Gamma 3&quot;</span>)

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>))

<span class="co"># gamma = 5</span>

<span class="kw">plot</span>(model_lasso,<span class="dt">main=</span><span class="st">&quot;Lasso&quot;</span>)
<span class="kw">plot</span>(model_scad_<span class="dv">5</span>,<span class="dt">main=</span><span class="st">&quot;Scad - Gamma 5&quot;</span>)
<span class="kw">plot</span>(model_mcp_<span class="dv">5</span>, <span class="dt">main=</span><span class="st">&quot;MCP - Gamma 5&quot;</span>)</code></pre></div>
<p><span class="math inline">\(\rho = 0\)</span></p>
<div class="figure">
<img src="Figures/gamma_2_rho_0.png" />

</div>
<div class="figure">
<img src="Figures/gamma_default_rho_0.png" />

</div>
<div class="figure">
<img src="Figures/gamma_5_rho_0.png" />

</div>
<p><span class="math inline">\(\rho = 0.2\)</span></p>
<div class="figure">
<img src="Figures/gamma_2_rho_0.2.png" />

</div>
<div class="figure">
<img src="Figures/gamma_default_rho_0.2.png" />

</div>
<div class="figure">
<img src="Figures/gamma_5_rho_0.2.png" />

</div>
<p><span class="math inline">\(\rho = 0.5\)</span></p>
<div class="figure">
<img src="Figures/gamma_2_rho_0.5.png" />

</div>
<div class="figure">
<img src="Figures/gamma_default_rho_0.5.png" />

</div>
<div class="figure">
<img src="Figures/gamma_5_rho_0.5.png" />

</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Comments: </span>

<span class="co"># 1) We can already see that there is a non convex behavior of Scad and MCP while Lasso path is more </span>

<span class="co"># smoothed.</span>

<span class="co"># 2) As gamma goes to plus infinity we notice that we go back to lasso path (smoothing procedure). </span>

<span class="co"># 3) The bias is minimized as gamma approaches his minimum value.</span>

<span class="co"># 4) Correlation creates a distorsion in the paths of all the three penalties.</span></code></pre></div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="post-selection-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
