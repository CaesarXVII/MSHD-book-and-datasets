[
["index.html", "Model Selection in High Dimensions 1 Introduction 1.1 Read this part first 1.2 Content choice and structure 1.3 Using R 1.4 Writing reports 1.5 Examples 1.6 Fundamental statistical concepts", " Model Selection in High Dimensions Maria-Pia Victoria-Feser (professor), Cesare Miglioli and Guillaume Blanc (teaching assistants) 2019-02-27 1 Introduction 1.1 Read this part first Anyone is invited to use any part of this eBook as long as credit is given. To cite this book, please use: Victoria-Feser, M.-P. (2018). A Lecture in Model Selection in High Dimensions, Research Center for Statistics, GSEM, University of Geneva, Switzerland. If you use this eBook as a reference for a course, please inform the author. (maria-pia.victoriafeser@unige.ch). The content of this eBook is dynamic and changes as the lectures take place. Students participating to the classes can contribute to the content, with for example the analysis of real data sets, the resolution of exercises, simulations to explore methods in particular settings, etc. Their contribution is acknowledge where it is due. The first acknowledgements go to Cesare Miglioli and Guillaume Blanc, Ph. D. Students and the Research Center for Statistics, University of Geneva, for their invaluable contribution in setting up the first version of this eBook. 1.2 Content choice and structure The content of this e-book is intended for graduate and doctoral students in statistics and related fields interested in the statistical approach of model selection in high dimensions. Model selection in high dimensions is an active subject of research, ranging from machine learning and/or artificial intelligence algorithms, to statistical inference, and sometimes a mix of the two. We focus on the frequentist approach to model selection in view of presenting methods that have the necessary properties for out-of-sample (or population) validity, within an as large as possible theoretical framework that enables the measurement of different aspects of the validity concept. We therefore anchor the content into an inferential statistics approach, essentially for causal models. More specifically, the focus of model selection in high dimensions is presented into two main headings, one on statistical methods or criteria for measuring the statistical validity, and the other one on fast algorithms in high dimensional settings, both in the number of observation and in the number of inputs, that avoid the simultaneous comparison of all possible models. Even within this focus, the set of available methods is still very rich, so that only a selection of the available methods is presented. Each presentation is accompanied with practical exercises using R. We highly recommend downloading RStudio’s IDE which is an ideal working environment for statistical analyses. 1.2.1 Bibliography (to be completed) Computer Age Statistical Inference: Algorithms, Evidence, and Data Science. Bradley Efron &amp; Trevor Hastie, Cambridge University Press, 2016. https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf An Introduction to Statistical Learning: with Applications in R. Gareth James, Daniela Witten, Trevor Hastie &amp; Robert Tibshirani, Springer, 2013. http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Trevor Hastie, Robert Tibshirani &amp; Jerome Friedman, Springer, 2009. https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf Model selection and model averaging. Gerda Claeskens and Nils Lid Hjort, Cambridge University Press, 2008. Regression and Time Series Model Selection. Allan D R McQuarrie and Chih-Ling Tsai, World Scientific, 1998. 1.2.2 Useful links (to be completed) Chamilo (Search for Model Selection in High Dimensions to register) Github repository of the course R project R Studio An Introduction to Statistical Programming Methods with R GitHub UCI repository for datasets Database definition Data Wrangling cheatsheet Malnutrition in Zambia, p. 64 Course Datasets - Malnutrition in Zambia American Cancer Association on Leukemia prognostic factors Data on Leukemia in Children quantmod package in R Theil inequality index Deep Learning Alzeihmer data 1.3 Using R There are many available classes, textbooks, e-books, etc. on how to get acquainted with a quite sophisticated usage of the most commonly used statistical software R. The choice of the R editor depends on how R is used and for this class, we propose the open source editor RStudio. We also highly recommend the introduction proposed in SMAC, which will constitute the basic knowledge from which this class starts. 1.3.1 Useful R packages The R packages that will be used throughout this class are the following (to be completed): rmarkdown quantmod plotly tidyr dplyr RODBC pool RMySQL foreign leaps glmulti MuMIn caret mvtnorm MASS tictoc pROC selectiveInference SIS rattle rpart rpart.plot glmnet devtools To install a package, use the R command install.packages(&quot;chosen.package.name&quot;) (see also https://cran.r-project.org/web/packages/). Visit the CRAN R project to get useful information about all the available R packages (https://cran.r-project.org/web/packages/available_packages_by_name.html) To install all packages at once, run the following code with administrator rights. packages.required &lt;- c(&quot;rmarkdown&quot;, &quot;quantmod&quot;, &quot;plotly&quot;, &quot;tidyr&quot;, &quot;dplyr&quot;, &quot;RODBC&quot;, &quot;pool&quot;, &quot;RMySQL&quot;, &quot;foreign&quot;, &quot;leaps&quot;, &quot;glmulti&quot;, &quot;MuMIn&quot;, &quot;caret&quot;,&quot;mvtnorm&quot;, &quot;MASS&quot;, &quot;tictoc&quot;, &quot;pROC&quot;, &quot;selectiveInference&quot;, &quot;SIS&quot;, &quot;rpart&quot;, &quot;rpart.plot&quot;, &quot;glmnet&quot;, &quot;devtools&quot;,&quot;rattle&quot;) packages.missing &lt;- packages.required[!(packages.required %in% rownames(installed.packages()))] if(length(packages.missing)!=0) install.packages(packages.missing) 1.3.2 Managing Data (by G. Blanc) Data are nowadays continuously produced and readily available from internet platforms. This has become necessary since very often personal computer memory is not sufficient to store (high dimensional) data locally. It is therefore important to be able to import data into R for data analysis in an (almost) automatic fashion. Colloquially, `loading’ a dataset means storing it into your computer’s Random-Access-Memory (RAM), which allows for a fast access of the CPU to the data. The RAM is extremely fast1, but is typically of limited amount compared to what can be stored in a hard-drive or a SSD (usually 4 to 16 Gbs in a typical consumer-grade computer). The data stored in your computer’s RAM is called volatile: the information it contains will disappear once the computer is shut down. This is the reason why the datasets must be loaded into R at the beginning of each session. In this section, you will learn different ways to load the data that you may encounter in the future, depending on the context and the size of the databases: from an R package, from a local data file, from an online data file, and from an online database. In this latter case, datasets are obtained by way of SQL queries via a remote connection. Follow along and load all the datasets as in the text: they will be used for the following exercises. 1.3.3 Loading data from an R package (by G. Blanc) Some packages have their own data included, and R-base indeed includes a well known selection, for instance the iris dataset. You can load it using: # Load the iris dataset iris &lt;- iris and display its structure with: # Display iris&#39; structure str(iris) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... The dataset iris is now ready for analysis. 1.3.4 Loading data from a local file (by G. Blanc) Typical file formats include .txt and .csv, or .data. Download the file wine.data from http://archive.ics.uci.edu/ml/machine-learning-databases/wine/ to a folder on your computer, say in ./datasets/. You can load the data using the read.table function, which takes as further input sep = &quot;,&quot; to indicate that the variables in each line are separated by a comma. These variables will be organized as columns in the dataset.Every line of the file becomes a row in the dataset. wine &lt;- read.table(&quot;./datasets/wine.data&quot;, sep = &quot;,&quot;) 1.3.5 Loading data from an online file (by G. Blanc) The end result will be the same as above, but the process is less tedious, provided you have an internet connection. Simply load the data using the complete url as an input for read.table: wine &lt;- read.table(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;, sep = &quot;,&quot;) A quick look at the structure of the dataset shows that the columns do not have names that describe their content. Let’s change that: # format the data.frame: colnames(wine) &lt;- c(&quot;Class&quot;, &quot;Alcohol&quot;, &quot;Malic_acid&quot;, &quot;Ash&quot;, &quot;Alcalinity_of_ash&quot;, &quot;Magnesium&quot;, &quot;Total_phenols&quot;, &quot;Flavanoids&quot;, &quot;Nonflavanoid_phenols&quot;, &quot;Proanthocynins&quot;, &quot;Color_intensity&quot;, &quot;Hue&quot;, &quot;OD280vsOD315&quot;, &quot;Proline&quot;) wine$Class &lt;- as.factor(wine$Class) The dataset wine is now ready for analysis. 1.3.6 Loading data from an online database using a mySQL query (Optional) (by G. Blanc) The most common way to store massive, related data from different sources is to use relational databases. These consists of multiple datasets (tables), that may be related in some specific way. For instance, an online shop may have a table listing all of its registered clients, and another table listing all the orders made by the clients. In this example, the clients will have a unique identifier that will establish the relation between the two databases. A database is an organized collection of data. A relational database (see also https://en.wikipedia.org/wiki/Database), more restrictively, is a collection of schema, tables, queries, reports, views, and other elements. A database-management system (DBMS) is a computer-software application that interacts with end-users (you), other applications that you may develop (e.g., an Rmarkdown document), and the database itself to capture and analyze data. Relational databases organize data into one or more tables of columns and rows, with a unique key identifying each row. Generally, each table/relation represents one “entity type” (such as customer or product). The rows represent instances of that type of entity (such as “Lee” or “chair”) and the columns represent values attributed to that instance (such as “address” or “price”). The databases can be stored - offline, on a non-volatile memory (for instance your hard drive or SSD drive), or - online, which require an internet connection to access. You will usually need credentials (a username and a password) to access either databases. As we will see, there are many advantages of relational databases which explain their almost universal use when it comes to storing massive amount of data online. The main advantages are that they: avoid data duplication, avoid inconsistent records, allow easily to change or add/remove the data, are more secure. To access the data store on the database, you will need to establish a connection, for which we will need RStudio’s pool package. library(pool) # Establish a connection to the external database con &lt;- dbPool( drv = RMySQL::MySQL(), dbname = &quot;shinydemo&quot;, host = &quot;shiny-demo.csa7qlmguqrf.us-east-1.rds.amazonaws.com&quot;, username = &quot;guest&quot;, password = &quot;guest&quot; ) We can explore the database and list the tables that it contains using dbListTables: # Explore the tables available in the database: dbListTables(con) ## [1] &quot;City&quot; &quot;Country&quot; &quot;CountryLanguage&quot; Remember that each table is a different dataset. Some databases are very big, in the BigData sense: think millions, even billions of entries. SQL queries allow to cherry pick the data we need, without having to download the whole dataset (which would be in some cases unfeasible). In a typical use, we would then query the data that we need, and no more. Since this is not a mySQL course and our datasets are of reasonable size, we will simply download the three datasets entirely using the SQLquery SELECT * FROM (Table). # Download the three datasets City &lt;- DBI::dbGetQuery(con, &quot;select * from City&quot;) Country &lt;- DBI::dbGetQuery(con, &quot;select * from Country&quot;) CountryLanguage &lt;- DBI::dbGetQuery(con, &quot;select * from CountryLanguage&quot;) poolClose(con) Exercise: - Load the iris dataset from the R-base package - Load the wine dataset from a local file and from an online file using the URL directly - Load the datasets City, Country, and CountryLanguage by connecting to an online database 1.3.7 Data Wrangling (by G. Blanc) Data wrangling means to manipulate and prepare a dataset in such a way, that it becomes amenable to analysis. Minimally, a numerical dataset is stored as a Matrix object, a type optimized for computations. Preferably, however, a dataset is stored as a dataframe object. A dataframe is technically a list of columns, each containing data of a given type (e.g., integer, numerical, character, factor). Many packages, and indeed R-base itself, are optimized to have the data organized in the following way: each row represent one observation each column represents a variable (or `feature’) There are two packages dedicated to data wrangling in R: dplyr is a grammar of data wrangling, which focuses on efficient and elegant coding data.table is computationally extremely fast to manipulate very large datasets Both create objects that are extensions of dataframes: dplyr uses a tibble, and data.table uses a data.table. We do not recommend to use data.table in this course, as it has a steeper learning curve. Using dplyr is not necessary either to complete the course; however, it will save you time in the end and is a worthwhile investment. For a quick reference, download the official cheatsheet. Exercise: We will assume that the datasets iris, wine, City, Country, and CountryLanguage of the above section have all been loaded in R, as well as the package dplyr. library(dplyr) Comment on the type of variables of the iris dataset. Write a code that computes the mean of each variable, grouped by Species. Do it first using R-base only, and optionally, do it using dplyr. variables.to.average &lt;- c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;) species.types &lt;- c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;) # levels(iris$Species) # # Method 1A: without dplyr using a for loop; outputs a matrix species.means1 &lt;- NULL for(type in species.types){ is.species &lt;- iris$Species==type species.subset &lt;- iris[is.species, variables.to.average] species.means1 &lt;- rbind(species.means1, colMeans(species.subset)) } rownames(species.means1) &lt;- species.types # # Method 1B: more compact and efficient; outputs a matrix species.means2 &lt;- t(sapply(species.types, function(species){ colMeans(iris[iris$Species==species, variables.to.average]) })) # # Method 2: using dplyr; outputs a dataframe species.means3 &lt;- iris %&gt;% group_by(Species) %&gt;% summarise_all(mean) species.means3 ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 0.246 ## 2 versicolor 5.94 2.77 4.26 1.33 ## 3 virginica 6.59 2.97 5.55 2.03 Using the method of your choice, plot the boxplots of alcohol levels of the three classes of wine from the wine dataset. boxplot(Alcohol ~ Class, data=wine, xlab=&quot;Class of wine&quot;, ylab=&quot;Alcohol concentration&quot;) Create 6 boxplots displaying the alcohol level of wine by class (3 levels) and by degree of color intensity (2 levels: above or below 5.0). Optional: compute the corresponding means using dplyr. # Create a new binary variable, called Color_intensity_factor to denote whether the color is mild or intense. wine$Color_intensity_factor &lt;- factor(wine$Color_intensity&gt;5, levels=c(FALSE, TRUE), labels=c(&quot;mild&quot;, &quot;intense&quot;)) # Create the boxplots using the standard formulae boxplot(Alcohol ~ Color_intensity_factor + Class, data=wine, xlab=&quot;Class of wine&quot;, ylab=&quot;Alcohol concentration&quot;) # # Compute the alcohol means of each subgroup using dplyr wine %&gt;% group_by(Class, Color_intensity_factor) %&gt;% summarise_at(&quot;Alcohol&quot;, mean) ## # A tibble: 6 x 3 ## # Groups: Class [3] ## Class Color_intensity_factor Alcohol ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 mild 13.4 ## 2 1 intense 13.9 ## 3 2 mild 12.3 ## 4 2 intense 12.4 ## 5 3 mild 13.0 ## 6 3 intense 13.2 Consider the three datasets City, Country, and CountryLanguage. Merge the three datasets into a single one using the method of your choice, such that no information is lost. Compare the total number of entries in the three relational datasets, to that of the unique dataset. What do you notice? [Hint: notice that “country” is the common relation among the three datasets] # We observe that &quot;country&quot; is the common relation among all datasets. # Let&#39;s try to merge all datasets into a single one using this key # # Merge all cities with country attributes data.merged &lt;- left_join(City, Country, by=c(&quot;CountryCode&quot; = &quot;Code&quot;)) %&gt;% left_join(CountryLanguage, by=&quot;CountryCode&quot;) # we observe that all the data pertaining to the countries appear multiple times. #The total number of data &quot;cells&quot; of the merged data set is 27031*22 = 594682, as compared to the original 24656 data &quot;cells&quot;, that is about 24 times bigger to store the same information. 1.4 Writing reports In this Section, information is provided about one convenient way to produce reports when working in teams. To be able to participate in the construction of this eBook, only text (including R chunks) in RMarkdown will be accepted. 1.4.1 R Markdown RMarkdown is a framework that provides a literate programming format for data science. It can be used to save and execute R code within R Studio and also as a simple formatting syntax for authoring HTML, PDF, ODT, RTF, and MS Word documents as well as seamless transitions between available formats. For example this eBook is written using R Markdown. We recommend the introduction proposed in https://smac-group.github.io/ds/rmarkdown.html to rapidly get acquainted with the use of R Markdown. Exercises with Iris dataset (see Loading data from an R package) Create an .rmd file from R Studio classic interface and look at the basic notions explained in the new document Create an histogram of the sepal width of Iris Setosa without showing both the code and the graph. Then, in another code chunk, show only the graph (without the code) and change the height or width of the histogram as you prefer Write the formula, both inline and with a Latex environment (e.g. equation), of the conditional probability of observing an Iris Virginica given that the sepal width is greater than 3. Display both the code and the conditional probability 1.4.2 GitHub GitHub (https://github.com/) is a development platform designed to host and review code, manage projects, and build software alongside millions of other developers. An introduction to the use of Github for managing projects (e.g. a data analysis project), we recommend https://smac-group.github.io/ds/github.html. Students following this course will be stimulated to complete the exercises and practicals and provide their solutions that will be published in the eBook. GitHub provides a platform for team work that is strongly encouraged. Exercises Create a free GitHub account on https://github.com/ Read chapter 3 of the GitHub Guide https://smac-group.github.io/ds/github.html Install a version of Git (from https://git-scm.com/downloads) which is compatible with the OS of your computer (e.g. Windows/Mac/Linux/Solaris). Once you have downloaded and installed Git, the first thing you should do is to configure it by setting your username and email address (see first point). Watch the video in Section 3.3 of the GitHub Guide on the workflow within R Studio Create a new R Studio project, following the steps highlighted in the video, and take the URL from the GitHub repository of the course https://github.com/CaesarXVII/Model-Selection-in-High-Dimensions Modify the file (add the name of practical1.rmd file) as you like (e.g. try to solve an exercise). Then commit the changes and push it to the remote repository of the course. Do not forget to click on pull every time you access to your R Studio project to retrieve the updated version of all the files of the course repository In order to properly execute your commits, you need to be added as a collaborator of the project. It is sufficient to send an email to cesare.miglioli@unige.ch with your GitHub name in it from your unige mail account and you will be set as a collaborator. 1.5 Examples 1.5.1 Data on Malnutrition in Zambia Childhood malnutrition is considered to be one of the worst health problems in developing countries (United Nations Children’s Fund 1998). Both a manifestation and a cause of poverty, malnutrition is thought to contribute to over a third of death in children under five years old globally (United Nations Children’s Fund 2012). Moreover, it is well established in the medical literature that maternal and child under nutrition have considerable consequences for adult health and human capital (see e.g. Victora et al. (2008) and the references therein). Such conditions are, for example, associated with less schooling, reduced economic productivity, and for women lower offspring birth weight. It has also been reported that lower birth weight and under nutrition in childhood have an influence on cancer occurrence and are risk factors for high glucose concentrations, blood pressure, and harmful lipid profiles. See also https://archive-ouverte.unige.ch/unige:29628, p. 64. Under nutrition is generally assessed by comparing anthropometric indicators such as height or weight at a certain age to a reference population. A well established measurement for the study of acute malnutrition is given by (see cite {who1995physical} for details): \\[\\begin{equation} Y_i = \\frac{H_{i,j} - \\mu_j}{\\sigma_{j}} \\label{eq:Zscore} \\end{equation}\\] where \\(H_{i,j}\\), \\(\\mu_j\\) and \\(\\sigma_j\\) denote, respectively, the height of the \\(i^{\\text{th}}\\) child at age \\(j\\), the median height of a child of the same age in the reference population and the associated standard deviation. Several factors are assumed to have a determinant influence on under nutrition. Consider the dataset Zambia.SAV available at Course Datasets - Malnutrition in Zambia containing variables assumed to be potential causes for childhood malnutrition, i.e. breastfeeding duration (month); age of the child (month); age of the mother (years); Body Mass Index (BMI) of the mother (kg/meter\\(^2\\)); height of the mother (meter); weight of the mother (kg); region of residence (9 levels: Central, Copperbelt, Eastern, Luapula, Lusaka, Northern, Northwestern, Southern and Western); mother’s highest education level attended (4 levels: No education, Primary, Secondary and Higher); wealth index factor score; weight of child at birth (kg) ; sex of the child; interval between the current birth and the previous birth (month); and main source of drinking water (8 levels: Piped into dwelling, Piped to yard/plot, Public tap/standpipe, Protected well, Unprotected well, River/dam/lake/ponds/stream/canal/ irrigation channel, Bottled water, Other). Exercise (7.1.1) Load the dataset and build the variables so that they can be used for a regression analysis. Associate proper names to each variable (hint: look at the previous comments in the r chunk). Perform a linear regression on all the available variables. Reduce the number of covariates (e.g. using the t-test) and add some interactions. Perform a linear regression on the new dataset. Analyse your chosen estimated model with a residual analysis (e.g. residuals vs fitted plot, normal QQ plot etc.). 1.5.2 Prognostic Factors in Childhood Leukemia (by C. Miglioli) Factors that can affect a child’s outlook (prognosis) suffering e.g. from Leukemia are called prognostic factors. They help doctors decide whether a child with leukemia should receive standard treatment or more intensive treatment. Prognostic factors seem to be more determinant in acute lymphocytic leukemia (ALL) than in acute myelogenous leukemia (AML). See https://www.cancer.org/cancer/leukemia-in-children/detection-diagnosis-staging/prognostic-factors.html for a detailed explanation. The leukemia_big.csv dataset contains gene expression measurements on 72 leukemia patients: 47 ALL (i.e. acute lymphocytic leukemia) and 25 AML (i.e. acute myelogenous leukemia). These data arise from the landmark of Golub et al. (1999) Science paper and exhibit an important statistical challenge because \\(p &gt;&gt; n\\) as we deal with 72 patients and 7128 measurements. Exercises (7.1.2) Load the package SIS in your R environment. Type both data(“leukemia.train”) and data(“leukemia.test”) in the console window to load the data. Create the response variable y according to the number of ALL and AML patients. In the same fashion create the matrix X of independent variables by merging the two datasets. Choose the correct exponential family for this situation and perform a GLM on the data. Comment on the results that you obtain. 1.5.3 Gene Expression in Prostate Cancer (by C. Miglioli) Prostate tumors are among the most heterogeneous of cancers, both histologically and clinically. Microarray expression analysis can be used to determine whether global biological differences underlie common pathological features of prostate cancer and to identify genes that might anticipate the clinical behavior of this disease. The prostate.train dataset contains 12600 gene expression measurements on 102 patients: 52 with cancer and 50 healthy. These data originate from Singh et al. (2002) Cancer cell paper and support the notion that “the clinical behavior of prostate cancer is linked to underlying gene expression differences that are detectable at the time of diagnosis”. Exercises Load the package SIS in your R environment. Type data(“prostate.train”) in the console window to load the data. Define the response variable y knowing that it is the last column of the dataset. In the same fashion create the matrix X of independent variables. Choose one of the model selection procedures explained during the course to select the most important genes related to prostate cancer. Validate the prediction accuracy of your selected model on the test dataset data(“prostate.test”), available in the package SIS. 1.5.4 Gene Expression Ratios in Lung Cancer and Mesothelioma (by C. Miglioli) The pathological distinction between malignant pleural mesothelioma (MPM) and adenocarcinoma (ADCA) of the lung can be cumbersome using established methods. That is why doctors are moving towards gene expression ratios as an accurate and inexpensive technique with direct clinical applicability for distinguishing between MPM and lung cancer. The gordon dataset contains 12533 gene expression measurements on 181 tissue samples: 31 MPM and 150 ADCA. These data derive from Gordon et al. (2002) Cancer Research paper which provides evidence that this technique can be accurate in this and other clinical scenarios. Exercises Install the R package devtools and load it. Type install_github(‘ramhiser/datamicroarray’) in the console window to install the updated version of the datamicroarray package. After loading the datamicroarray package, type data(‘gordon’, package = ‘datamicroarray’) and define the second element of the list as the response y. In the same fashion create the matrix X of independent variables with the remaining first element of the list. Try different model selection procedures, explained during the course, to find the most important genes to distinguish between Lung Cancer and Mesothelioma. Plot the ROC curve relative to each different selected model and compare the results in terms of AUC. 1.5.5 R package quantmod The quantmod package for R (https://www.quantmod.com/) is designed to assist the quantitative trader in the development, testing, and deployment of statistically based trading models. We are here interested in the easy and rapid access to data. It is possible with one quantmod function to load data from a variety of sources, including Yahoo! Finance (OHLC data), Federal Reserve Bank of St. Louis FRED® (11,000 economic series), Google Finance (OHLC data), Oanda, The Currency Site (FX and Metals), etc. Below are some examples on how to load financial data and perform some simple data analysis. A getting started guide to quantmod can be found at https://www.quantmod.com/examples/intro/. The first step is to install the quantmod package (only once), using the install.packages in R. Also install the plotly package for nice plots. Then try the following: # Load quantmod # library(quantmod) # Download data # today &lt;- Sys.Date() # three_month_ago &lt;- seq(today, length = 2, by = &quot;-3 month&quot;)[2] # getSymbols(&quot;AAPL&quot;, from = three_month_ago, to = today) # getSymbols(&quot;NFLX&quot;, from = three_month_ago, to = today) # Produce a # candleChart(NFLX, theme=&#39;white&#39;) # candleChart(AAPL, theme=&#39;white&#39;) 1.6 Fundamental statistical concepts 1.6.1 Sample and population If data are collected, stored, analysed, it is because the are supposed to provide information that cannot be otherwise available. Most of the information that is sought concerns sufficiently general phenomena that, in fact, nobody know (or will ever know) exactly. As an illustration, take the example of a teacher that computes the average score of the last math test in his class, what he gets is the exact information about the average score for that particular class and particular test, at the particular moment when the test took place and when the teacher marked the copies. Any other inference from the available information (the sample) to another context is subject to sampling variability and hence is not exact. If the teacher uses the average score to somehow evaluate the difficulty of his math test, then what he has observed within his class is only a part of the truth. For that purpose (evaluating the difficulty of the math test), he should let all the possible students (the population) pass the test and compute the average of the resulting scores of all of them. This is of course not possible, but statistical methodology can help in targetting the question of interest summarized here by the scores average, by providing, for example, a finite set of possible values for the true average, the one computed virtually on the population, also called a parameter. 1.6.2 Models and risk When the sample per se is not the target (i.e. in most of the cases), then one enters into the process of inference: what can we say about what happens in the population, given a sample of data, supposedly carrying enough information for that purpose? A fundamental aspect of statistical inference is the ability of constructing (manageable) measures of variability to any data treatment operated in order to produce information that is used, in its context, to e.g.: understand the phenomenon under investigation, to predict, to evaluate research hypotheses, etc. The inference concept implies two subsequent questions that are at the core of statistics. On the one hand, one has to define what is the population information of interest, and, on the second hand, one has to provide an inferential risk, i.e. a measure of risk associated to any inference made from the sample to the population. The first concept can be associated, very broadly, to the model, i.e. a set of input (a priori) information that serves to formalize the information of interest. The second concept which is a direct consequence of a function of the sampling variability, can be associated to a (set of) propability, a fundamental measure in statistics. Sometimes, and even more and more often, the two concepts are untangled, in the sense that the model can be very flexible (it is actually a set of models) and a risk measure is used to somehow define a (or a drastically reduced set of) model. This vas-et-vient process could be used to define model selection in statistics. Finally, while the model (or the set of models) is, in general, set a priori, the inferential risk needs to be estimated from the available information, i.e. the sample itself. For that purpose, the fundamental instrument is probability theory. 1.6.3 Estimators and associated variability Consider the simplest decisional setting, i.e. confidence intervals for population parameters. We adopt here a frequentist approach. Population parameters can be quantities of interest, e.g. the population mean, the population proportion (for something specific), the population probability (e.g. of being bankrupt or of surviving a given treatment), or more elaborate quantities such as inequality or poverty measures (see e.g. Cowell (2011)). Very generally, consider an estimator \\(\\hat{\\theta}\\) from a population parameter \\(\\theta\\in\\Theta\\subseteq\\mathbb{R}^p\\) that is computed on a sample \\(F^{(n)}\\)2 generated for a (family of) model \\(F\\). The latter can be parametric, non parametric or semi-parametric. We can write \\(\\hat{\\theta}(F)\\), i.e. the estimator as a functional (or function of a distribution) of \\(F\\); in particular, we can write \\(\\hat{\\theta}(F^{(n)})\\). For example, the sample mean, an estimator of the population mean \\(\\mu\\), can be written as: \\[\\begin{equation} \\hat{\\mu}\\left(F^{(n)}\\right)=\\int x dF^{(n)}(x)=\\frac{1}{n}\\sum_{i=1}^n x_i \\end{equation}\\] An estimator is first chosen for the population parameter and then a confidence interval is built (estimated from the data) that depends on some underlying assumptions about the data generating process. This requires calculating the properties of estimators \\(\\hat{\\theta}(F^{(n)})\\) at distribution \\(F\\) (which is unknown, only assumed). To do so, there are several strategies which include (see Efron and Hastie (2016), chapter 2): The plug-in principle: The variance (or any other moment of the distribution) of \\(\\hat{\\theta}(F^{(n)})\\) is expressed (theoretically) as a function of population parameters (e.g. the population mean, variance, higher moments) and the population parameters in the formula are replaced by estimators computed from the sample. General results on classes of estimators such as the maximum likelihood estimator (MLE) or \\(M\\)-estimators (Huber and Ronchetti 2009) can be used for the plug-in principle. Taylor-series approximations: Let \\(T(\\hat{\\theta})\\) be a function of interest of \\(\\hat{\\theta}(F^{(n)})\\), one can use local linear approximations, method that is also sometimes known as the delta method. One considers the linear expansion of \\(T(\\hat{\\theta})\\) around \\(T(\\theta)\\) and uses the approximation \\(T(\\hat{\\theta})=T(\\theta)+\\partial/\\partial\\theta^T T(\\theta)\\left(\\hat{\\theta}-\\theta\\right)\\) together with the plug-in principle to get an estimator of \\(T(\\hat{\\theta})\\). For example, the variance of \\(T(\\hat{\\theta})\\) can be estimated by \\(\\left(\\partial/\\partial\\theta^T T(\\theta)\\right)\\text{var}(\\hat{\\theta})\\left(\\partial/\\partial\\theta^T T(\\theta)\\right)^T\\) in which the unknown \\(\\theta\\) is replaced by its estimated value. Using the functional notation, approximations can be found using von Mises expansions, together with Gâteaux differentials for multidimensional functionals (see e.g. Fernholz (2001)). Simulation and the bootstrap: The basic idea is to implement the infinite sequence of future trials using simulated samples in almost infinite quantities. An estimator \\(\\hat{F}\\) of \\(F\\) is first chosen, then samples \\(F_k^{(n)},k=1,\\ldots,B\\) (\\(B\\) is the almost infinite quantity) are simulated from \\(\\hat{F}\\) to compute estimates \\(\\hat{\\theta}^{(k)}:=\\hat{\\theta}(F_k^{(n)})\\). This produces an estimate for the distribution of \\(\\hat{\\theta}\\) that can be used to compute the required quantities such mean, variance, quantiles, etc. Simulation based inference is quite different to traditional methods based on the plug-in principle in that an estimator is sought for \\(F\\) instead of estimators for population parameters. The natural question to ask at this point is what is the best approach to measure risk (sampling error) associated to a statistic (a functional of the sample distribution)? In a frequentist paradigm, one can rely on concepts such as the minimum variance or mean squared error of the resulting estimator (an asymptotic concept also called efficiency) and/or the rate of convergence of the resulting estimator (related to the asymptotic concept of consistency), i.e. how does the estimator converge to the corresponding population quantity as a function of the sample size \\(n\\). There is no unifying theory providing an optimality result for all settings, rather general results for classes of models, estimators and/or simulation-based methods. 1.6.4 Simulating the population using resampling techniques (See Efron and Hastie (2016), chapter 10) Generally speaking, resampling techniques allow to simulate the population, or more precisely, the sampling mechanism of an infinte number of trials (samples). The unspoiled and unique proxy for the population (model) is the sample \\(F^{(n)}\\). For finite populations of size \\(N\\) (that can be huge), a sample of independently drawn observations (i.e. the iid case) can be seen as one realization of \\(n\\) draws from a multinomial distribution with \\(N\\) equally probable (\\(1/N\\)) outcomes corresponding to the \\(N\\) population values. This suggests that a suitable proxy to this data generating mechanism (in the iid case) is to proceed with \\(n\\) draws from a multinomial distribution with \\(n\\) equally probable (\\(1/n\\)) outcomes corresponding to the \\(n\\) sample values. This is what the non parametric Bootstrap (Efron 1979) does. Exercise (optional): The aim here is to reproduce the non parametric Bootstrap and compare it with Monte Carlo simulations. Use as an example the Theil Inequality Index (see e.g. https://en.wikipedia.org/wiki/Theil_index) together with the Generalized Beta Distribution of the second kind. For the sampling part, control the seed (see help(set.seed) in R). Use both the sample command in R and the Uniform distribution to produce the random draws. - Produce (with both methods) the Bootstrap estimate of the sampling distribution of the Theil Inequality Index (of size 1000) for one chosen set of values for the parameters of the Generalized Beta Distribution of the second kind. Check that both methods produce exactly the same distribution’s estimate. - Do the same using a Monte Carlo experiment and compare the outcome with the Bootstrap estimate (control the seed). The jackknife (Quenouille (1956), Tukey (1958)) was a first step toward simulation-based inference, developed to compute standard errors. It actually produces \\(n\\) systematic (not randomly drawn) samples \\(F_{(i)}^{(n)},i=1,\\ldots,n\\), each one of size \\(n-1\\), obtained by successively removing one observation at the time. For the functional \\(\\hat{\\theta}(F^{(n)})\\), the (unbiased) Jackknife standard error (SE), say \\(T\\), is given by (see Efron and Stein (1981)) \\[\\begin{equation} T(\\hat{\\theta})=\\left[\\frac{n-1}{n}\\sum_{i=1}^n\\left(\\hat{\\theta}\\left(F_{(i)}^{(n)}\\right)-\\bar{\\hat{\\theta}}\\right)\\right]^{1/2} \\end{equation}\\] with \\(\\bar{\\hat{\\theta}}=1/n\\sum_{i=1}^n\\hat{\\theta}\\left(F_{(i)}^{(n)}\\right)\\). The jackknife can be seen as a linear approximation of the bootstrap, hence less appropriate for unsmooth estimators (e.g. quantiles). Exercise (optional) - Use the sample experimental setting as in the exercise for the Boostrap and compute the sampling distribution estimate of of the Theil Inequality Index using the jacknife. Compare the results with the ones based on the Bootsrap and Monte Carlo simulations. An alternative and natural estimator for \\(F\\) is to assume that \\(F\\) belongs to a family of (parametric) distributions (models), indexed by a parameter vector \\(\\theta\\), i.e. the set \\(\\{F_{\\theta}, \\theta\\in\\Theta\\subseteq\\mathbb{R}^p \\}\\) and, using the plug-in principle, one gets \\(\\hat{F}=F_{\\hat{\\theta}}\\). This requires some attention for the choice of \\(\\hat{\\theta}\\) which ad minima should be consistent. The MLE (computed on the original sample \\(F^{(n)}\\)) is a suitable candidate for consistency, but also for efficiency. To produce \\(B\\) samples of size \\(n\\), there exists various (implemented) random generators, which are in principle based on a random generation of Uniform(0,1) realizations \\(u_i,\\ldots,u_n\\), from which a sample is obtained via \\(x_i=F_{\\hat{\\theta}}^{-1}(u_i), i=1,\\ldots,n\\). Parametric families act as regularizers, smoothing out the raw data and de-emphasizing extreme observations. They are particularly appreciated when studying rare events, like probabilities of extremes, in finance, insurance and with natural phenomena (tides, temperatures, earthquakes, etc.). The obvious drawback is that they need to be specified a priori, but the family can be sufficiently large. In this case, model selection becomes an important step into model building, with an obvious impact on inference. Exercise (optional): There are different ways to simulate samples in the linear regression model case. In the non parametric case, the design matrix is kept fixed or, alternatively, the raws are drawn together with the response. It is not yet clear which method is the most suitable in therms of statistical properties of resulting procedures like significance testing. In the parametric case, only the random part is simulated, i.e. the residuals (with \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}\\)). There is also a semi-parametric version which consists in resampling the residuals. - With the Malnutrition in Zambia dataset, considering the (complete) linear model without interactions, compute 95% confidence intervals (percentile method) for the slope parameter of the breastfeeding duration variable, using the four different resampling schemes for the linear regression model. Compare. Hint: Read chapter 11 of Efron and Hastie (2016). 1.6.5 Model Selection Model selection is a broad concept. For example, for a family of models \\(F_{\\mathbf{\\theta}}\\), there exists an (almost) infinite number of different ones according to the value of \\(\\mathbf{\\theta}\\). Hence, estimation (parametric or non parametric) is also a form of model selection since it allows, from the sample, to reduce the set of potential models. Another form of model selection involves, simultaneously, the specification of the parameter’s set \\(\\mathbf{\\theta}\\) (e.g. what is \\(p\\)) and, within this specification, a reduced set of potential values. This process, like estimation, is by nature inferential, since the only available information is the sample. A trade-off needs then to be made between the model complexity (e.g. \\(p\\)) and the model adequacy (e.g. its fit to the data). The choice of the measure associated to the trade-off is also important. While model adequacy is an obvious objective to achieve, reducing model complexity is a more subtle, but also an important feature. The reasons include (see also James et al. (2013), Section 2.1.3): When we are mainly interested in inference, then restrictive models are much more interpretable. When the objective is prediction only and the interpretability of the predictive model is simply not of interest, we might expect that it will be best to use the most flexible model available. Surprisingly, this is not always the case, since more accurate predictions are obtained using a less complex models (see Section 2.1) One can separate model selection procedures in three broad categories which are: - Subset Selection: This approach involves identifying a subset of the \\(p\\) predictors (i.e. a non zero subset of \\(\\mathbf{\\theta}\\)) that we believe to be related to the response. - Shrinkage: This approach involves fitting a model involving all \\(p\\) predictors (or parameters in \\(\\mathbf{\\theta}\\)). However, the estimated parameters are shrunk towards zero relative in the estimation procedure. This shrinkage is also known as regularization and has the effect of reducing sampling error and, depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero, leading to a form of subset selection. - Dimension Reduction: This approach involves reducing the \\(p\\) predictors into a \\(q\\)-dimensional subspace, where \\(q &lt;p\\), using a trade-off between information loss due to the dimension reduction and model complexity. The \\(q\\) (orthogonal) axes of the subspace are then used as predictors to fit the model. Obviously, subset selection and shrinkage are methods that target the model interpretability objective, while dimension reduction might be more appropriate for pure prediction. It is however not clear that, in terms of out-of-sample prediction error, one set of approaches is better than the others. In this course we will mainly focus on subset selection while also presenting shrinkage methods. References "],
["assessing-the-validity-of-a-model.html", "2 Assessing the validity of a model 2.1 Introduction 2.2 Cross-Validation 2.3 Covariance Penalties Criteria 2.4 Information Theory and Bayesian Criteria 2.5 Mean Squared Error Based Criteria 2.6 Classification measures", " 2 Assessing the validity of a model 2.1 Introduction Broadly speaking, model selection is about choosing a model that best fits (on some sense) the available sample so that it can be used to understand the phenomenon under investigation. This also includes being able to predict future outcomes and to assess research hypotheses within the population. Therefore, to reach these objectives, the available information should be used for two important tasks: building the model (learning phase) assess its out-of-sample validity The two tasks are interdependent since building the model, i.e. finding the most suitable one, is made by optimizing, in some sense, its out-of-sample validity. Out-of-sample validity is a crucial concept in model selection. Indeed, it allows to assess how the results of a statistical analysis (e.g. the selection of a model) will generalize to other outcomes, equivalently, to the population. Without out-of-sample validation, a selection procedure will necessarily tend to choose models that overfit the data, since they would be the best within the sample. In other words, when overfitting, a model describes random error or noise instead of the underlying relationship, leading to an excessively complex model (too many parameters relative to the number of observations) and consequently a poor predictive performance and a wrong representation of what happens in the population. Example of overfitting situation # Taken from https://en.wikipedia.org/wiki/Overfitting. # Noisy (roughly linear) data is fitted to a linear function and a polynomial function. # Although the polynomial function is a perfect fit, the linear function can be expected # to generalize better: if the two functions were used to extrapolate beyond the fit data, # the linear function would make better predictions. This is because the polynomial # function it is too dependent on the data which contains sampling error. # knitr::include_graphics(&quot;Figures/OverfitEx1.png&quot;) More formally, consider a random variable \\(Y\\) distributed according to model \\(F_{\\boldsymbol{\\theta}}\\), possibly conditionally on a set of fixed covariates \\(\\mathbf{X}=[x_1 \\ldots \\, x_p]\\). We observe a random sample \\(\\mathbf{y} = (Y_i)_{i = 1, \\ldots , n}\\) supposedly generated from \\(F_{\\boldsymbol{\\theta}}\\) together with a non-random \\(n\\times p\\) full rank matrix of predictors \\(\\mathbf{X}\\). Define the prediction function \\(\\hat{\\mathbf{y}}\\) that depends on the considered model, for example \\(\\hat{\\mathbf{y}}=\\hat{\\boldsymbol{\\beta}}\\mathbf{x}\\) for the linear regression model. The inferential task is to assess the accuracy of the prediction function, so that it can be compared to alternative prediction functions. Quantifying the prediction error of a prediction function requires specification of a discrepancy \\(D\\) between a prediction \\(\\hat{\\mathbf{y}}\\) and the actual response \\(\\mathbf{y}\\). A natural choice is the mean squared error (mean residual squared error) \\[\\begin{equation} D\\left(\\hat{\\mathbf{y}},\\mathbf{y}\\right)=\\frac{1}{n}\\sum_{i=1}^nd\\left(\\hat{y}_i,y_i\\right)=\\frac{1}{n}\\sum_{i=1}^n\\left(\\hat{y}_i-y_i\\right)^2 \\end{equation}\\] However, in order to avoid overfitting situations for out-of-sample validity, what is actually sought is the true prediction error, i.e. the (mathematically) expected discrepancy between prediction function out-of-sample \\(\\hat{y}_0\\) and the corresponding out-of-sample realization \\(y_0\\), namely \\[\\begin{equation} \\text{Err}=\\mathbb{E}_{0}\\left[d\\left(\\hat{Y}_0,Y_0\\right)\\right] \\end{equation}\\] where \\(\\mathbb{E}_{0}\\) denotes the expectation at the out-of-sample distribution. This quantity needs to be estimated in some manner with the sample at hand, hence the real challenge in model selection is - the specification of an out-of-sample validity measure, called a model selection criterion - estimators for model selection criteria - performance measures to compare different model selection criteria In the following sections we present several methods for model selection and measures for out-of-sample validity. Exercice (optional): - Consider the Malnutrition in Zambia dataset (without interections). As model fit assesment consider a) the residual variance and b) the R^2. Starting from the full model (without interactions), increase progressively model size by including polynomials (on the continuous variables) and observe the behavior of the model fit criteria (using e.g. a plot). Additional material: Controlling for out-of-sample validity follows the lines of the scientific approach, in particular Occam’s razor problem-solving principle (https://en.wikipedia.org/wiki/Occam's_razor). One can intuitively understand overfitting from the fact that information from all past experience can be divided into two groups: information that is relevant for the future (“signal”) and irrelevant information (“noise”). Everything else being equal, the more difficult a criterion is to predict (i.e., the higher its uncertainty), the more noise existing in past information needs to be ignored. The problem is to determine which part to ignore. See https://en.wikipedia.org/wiki/Overfitting. 2.2 Cross-Validation The challenge is now to find a suitable estimator for \\(\\text{Err}\\) with the help of the sample and the (assumed) model \\(F_{\\boldsymbol{\\theta}}\\). A first guess is the apparent error \\[\\begin{equation} \\text{err}= \\frac{1}{n}\\sum_{i=1}^nd\\left(\\hat{y}_i,y_i\\right) \\end{equation}\\] Unfortunately err usually underestimates Err since \\(\\hat{y}_i\\) has been adjusted to fit the observed responses \\(y_i\\) (trainig set). Ideally, one should have an independent validation set (or test set) of say \\(n^{\\star}\\) additional observations \\(y_{0i}, i=1,\\ldots,n^{\\star}\\), so that one could estimate Err using \\[\\begin{equation} \\widehat{\\text{Err}}_{\\text{val}}= \\frac{1}{n^{\\star}}\\sum_{i=1}^{n^{\\star}}d\\left(\\hat{y}_{0i},y_{0i}\\right) \\end{equation}\\] Cross-validation attempts to mimic \\(\\widehat{\\text{Err}}_{\\text{val}}\\) without the need for a validation set. Define \\(\\hat{y}_{(i)}\\) to be the predicted value computed on the reduced training set in which the \\(i\\)th observation has been omitted. The the leave one out cross-validation estimate of prediction error is \\[\\begin{equation} \\widehat{\\text{Err}}_{\\text{cv1}}= \\frac{1}{n}\\sum_{i=1}^{n}d\\left(\\hat{y}_{(i)},y_{i}\\right) \\end{equation}\\] Additional material: Actually, \\(\\hat{y}_{0i}\\) and \\(\\hat{y}_{(i)}\\) have to be understood as functions applied respectively on \\(y_{0i}\\) or \\(y_{i}\\). The functions are the prediction rules built during the learning phase, i.e. respectively on \\(y_i,i=1,\\ldots,n\\) or \\(y_i,i=1,\\ldots,i-1,i+1,n\\). A more common practice is to leave out several observations at a time: the sample is randomly partitioned into \\(J\\) groups of size about \\(\\lfloor n/J\\rfloor=n_J\\) each, then for each \\(j=1,\\ldots,J\\) groups, the training set is the sample without the \\(j\\)th group on which the prediction \\(\\hat{y}_{(j)}\\) is computed and then compared to the observations in the \\(j\\)th group \\(y_j\\) using the chosen discrepancy measure \\(D\\). A common choice is \\(J=10\\) leading to the ten-fold cross-validation procedure. Reducing from \\(n\\) to \\(J\\) the number of training and validation sets reduces the necessary number of prediction rules constructions (estimation). The optimal value for \\(J\\) is however not known. Exercise: (solutions (7.2.1) provided by Alexander Maslev, Hanxiong Wang and Minyoung Lee). Program k-fold Cross-Validation (with k=2) and do model selection in a specific simulation setting with an exhaustive search. Follow these steps: Generate from a MVN (multivariate normal) a matrix \\(\\mathbf{X_{n*p}}\\) with \\(n = 1000\\) and \\(p = 5\\). You can choose the location vector as you wish but set the scale matrix as the identity. Choose the generating vector \\(\\boldsymbol{\\beta }= [3 \\; 1.5 \\; 0 \\; 2 \\; 0]\\) and retrieve the signal to noise ratio of this setting. Generate \\(\\hat{\\mathbf{y}}\\) thanks to the relation \\(\\mathbf{y} = \\mathbf{X_{n*p}} \\; \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) where \\(\\epsilon_{i}\\) is a standard normal, \\(n = 1000\\) and \\(p = 5\\). Suppose for simplicity that the errors are uncorrelated. Split the data randomly in two halves (k=2) and use the training set to determine \\(\\hat{\\boldsymbol{\\beta}}\\). Then, compute the squared loss function as prediction error measure for each possible model. Observe which model is the best model. Suppose now that we increase the size of \\(\\boldsymbol{\\beta}\\) to 100 (i.e. \\(p = 100\\) ). Calculate the number of possible models to evaluate together with an estimate of the time needed for an exhaustive search (hint: use previous results). Conclude on the feasibility of this task. 2.3 Covariance Penalties Criteria 2.3.1 Introduction Originally, the covariance penalty approach treats prediction error estimation in a regression framework, with the predictors \\(\\mathbf{X}\\) considered as fixed. Moreover, supposing for the moment that the discrepancy measure is the squared difference (or \\(L_2\\) norm), the true prediction error (conditionally on \\(\\mathbf{x}_i\\)) is defined as \\[\\begin{equation} \\text{Err}_i=\\mathbb{E}_{0}\\left[\\left(\\hat{Y}_i-Y_0\\right)^2\\right] \\end{equation}\\] The overall prediction error is \\(\\text{Err}=1/n\\sum_{i=1}^n\\text{Err}_i\\). Note: again, \\(\\hat{Y}_i\\) is understood as a function of the sample. The question is how to estimate this quantity, given a sample and a data generating model? (Efron 2004) uses \\(\\mathbb{E}\\left[\\text{Err}_i\\right]\\) (where \\(\\mathbb{E}\\) denotes the expectation at the insample distribution) and shows that \\[\\begin{equation} \\mathbb{E}\\left[\\text{Err}_i\\right]= \\mathbb{E}\\left[\\text{err}_i\\right]+2\\text{cov}\\left(\\hat{Y}_i;Y_i\\right) \\end{equation}\\] with \\(\\text{err}_i= (Y_i-\\hat{Y}_i)^2\\), the apparent (or in-sample) error and \\(\\text{cov}\\left(\\hat{Y}_i;Y_i\\right)=\\mathbb{E}\\left[(Y_i-\\mathbb{E}[Y_i])(\\hat{Y}_i-\\mathbb{E}[Y_i])\\right]\\). Proof (see Efron and Hastie (2016), p. 220): Let \\(\\delta_{1i}=Y_i-\\mathbb{E}[Y_i]\\) and \\(\\delta_{2i}=\\hat{Y}_i-\\mathbb{E}[Y_i]\\), we can write \\[\\begin{eqnarray} (Y_i-\\hat{Y}_i)^2&amp;=&amp;(\\delta_{1i}-\\delta_{2i})^2=\\delta^2_{1i}-2\\delta_{1i}\\delta_{2i}+\\delta_{2i}^2 \\\\ &amp;=&amp;(Y_i-\\mathbb{E}[Y_i])^2-2(Y_i-\\mathbb{E}[Y_i])(\\hat{Y}_i-\\mathbb{E}[Y_i])+(\\hat{Y}_i-\\mathbb{E}[Y_i])^2 \\end{eqnarray}\\] We then have \\[\\begin{eqnarray} \\mathbb{E}\\left[(Y_i-\\hat{Y}_i)^2\\right]&amp;=&amp;\\mathbb{E}\\left[(Y_i-\\mathbb{E}[Y_i])^2\\right]-2\\mathbb{E}\\left[(Y_i-\\mathbb{E}[Y_i])(\\hat{Y}_i-\\mathbb{E}[Y_i])\\right]+\\mathbb{E}\\left[(\\hat{Y}_i-\\mathbb{E}[Y_i])^2\\right] \\\\ &amp;=&amp; \\sigma^2 -2\\mbox{cov}\\left(Y_i,\\hat{Y}_i\\right)+\\mathbb{E}\\left[(\\hat{Y}_i-\\mathbb{E}[Y_i])^2\\right] \\end{eqnarray}\\] from which we deduce that \\(\\mathbb{E}\\left[(\\hat{Y}_i-\\mathbb{E}[Y_i])^2\\right]=\\mathbb{E}[\\text{err}_i]+2\\mbox{cov}\\left(Y_i,\\hat{Y}_i\\right)-\\sigma^2\\). Likewise \\[\\begin{eqnarray} \\mathbb{E}\\left[(Y_{0i}-\\hat{Y}_i)^2\\right]&amp;=&amp;\\mathbb{E}\\left[(Y_{0i}-\\mathbb{E}[Y_i])^2\\right]-2\\mathbb{E}\\left[(Y_{0i}-\\mathbb{E}[Y_i])(\\hat{Y}_i-\\mathbb{E}[Y_i])\\right]+\\mathbb{E}\\left[(\\hat{Y}_i-\\mathbb{E}[Y_i])^2\\right] \\\\ \\end{eqnarray}\\] and because \\(\\hat{Y}_i\\) and \\(Y_{0i}\\) are independent, we get \\[\\begin{eqnarray} \\mathbb{E}\\left[(Y_{0i}-\\hat{Y}_i)^2\\right]&amp;=&amp; \\sigma^2 +\\mathbb{E}\\left[(\\hat{Y}_i-\\mathbb{E}[Y_i])^2\\right] \\end{eqnarray}\\] Note that \\(\\mathbb{E}\\left[\\mathbb{E}_{0}\\left[(Y_{0i}-\\hat{Y}_i)^2\\right]\\right]=\\mathbb{E}_0\\left[\\mathbb{E}\\left[(Y_{0i}-\\hat{Y}_i)^2\\right]\\right]\\), so that \\[\\begin{eqnarray} \\mathbb{E}\\left[\\text{Err}_i\\right]&amp;=&amp;\\sigma^2 +\\mathbb{E}\\left[(\\hat{Y}_i-\\mathbb{E}[Y_i])^2\\right] \\\\ &amp;=&amp; \\mathbb{E}[\\text{err}_i]+2\\mbox{cov}\\left(Y_i,\\hat{Y}_i\\right) \\end{eqnarray}\\] This result says that, on average, the apparent error \\(\\text{err}_i\\) underestimates the true prediction error \\(\\text{Err}_i\\) by the covariance penalty \\(2\\text{cov}\\left(\\hat{Y}_i;Y_i\\right)\\). This makes intuitive sense since \\(\\text{cov}\\left(\\hat{Y}_i;Y_i\\right)\\) measures the amount by which \\(Y_i\\) influences its own prediction \\(\\hat{Y}_i\\). An natural estimator for the overall prediction error is then given by \\[\\begin{equation} \\widehat{\\text{Err}}= \\frac{1}{n}\\sum_{i=1}^n\\left(\\hat{y}_i-y_i\\right)^2+\\frac{2}{n}\\sum_{i=1}^n\\widehat{\\text{cov}}\\left(\\hat{Y}_i;Y_i\\right) \\tag{2.1} \\end{equation}\\] Depending on the assumed model, \\(\\widehat{\\text{cov}}\\left(\\hat{Y}_i;Y_i\\right)\\) is obtained analytically up to a value of \\(\\boldsymbol{\\theta}\\), the model’s parameters, which is then replaced by \\(\\hat{\\boldsymbol{\\theta}}\\) (plug-in method), or by resampling methods such as the parametric bootstrap. For the later, considering the model \\(F_{\\boldsymbol{\\theta}}\\), one uses the following steps: Estimate \\(\\boldsymbol{\\theta}\\) from \\(F^{(n)}\\) to get \\(\\hat{\\boldsymbol{\\theta}}\\). Set the seed For \\(j=1,\\ldots,B\\), do Simulate \\(n\\) values \\(y_i^{(j)}, i=1,\\ldots,n\\) from \\(F_{\\hat{\\boldsymbol{\\theta}}}\\), Compute \\(\\hat{y}_i^{(j)}, i=1,\\ldots,n\\) (i.e. the prediction rule), possibly conditionally on the matrix of predictors \\(\\mathbf{X}\\) Compute \\(\\forall i\\) \\[\\begin{equation} \\widehat{\\text{cov}}\\left(\\hat{Y}_i;Y_i\\right)=1/B\\sum_{j=1}^B\\left(\\hat{y}_i^{(j)}-\\hat{y}_i^{(j\\cdot)}\\right)\\left(y_i^{(j)}-y_i^{(j\\cdot)}\\right) \\end{equation}\\] with \\(\\hat{y}_i^{(j\\cdot)}=1/n\\sum_{j=1}^B\\hat{y}_i^{(j)}\\) and \\(y_i^{(j\\cdot)}=1/n\\sum_{j=1}^By_i^{(j)}\\) Exercise (optional): Consider the simulation setting of the exercise in the previous Section. - Instead of splitting the sample in two halves, compute the covariance prenalized predition error, with covariance penalty estimated via simulations (using the proposed algorithm). - Compare the analysis (decision) with the one obtained by means of cross-validation (in the previous exercise). 2.3.2 Mallows \\(C_p\\) Consider the linear regression model \\(Y_i|\\mathbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu}\\left(\\mathbf{x}_i\\right),\\sigma^2), 0&lt;\\sigma^2&lt;\\infty\\), with \\[\\begin{equation} \\boldsymbol{\\mu}\\left(\\mathbf{x}_i\\right)=\\mathbf{x}_i^T \\boldsymbol{\\beta}, \\end{equation}\\] where \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\) and \\(\\mathbf{x}_i^T\\) is the ith row of \\(\\mathbf{X}\\) (that includes a column of ones for the intercept). One notable result (see exercise below) that can be deduced from the covariance penalty formula, for the linear regression model using the least squares estimator (LSE) \\(\\hat{\\beta}=\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}\\), \\(\\mathbf{y}=[y_1,\\ldots,y_n]^T\\), is Mallow’s \\(C_p\\)3 (Mallows 1973): \\[\\begin{equation} C_p=\\frac{1}{n}\\sum_{i=1}^n\\left(\\hat{y}_i-y_i\\right)^2+\\frac{2}{n}p\\sigma^2 \\tag{2.2} \\end{equation}\\] Exercise: derive Mallow’s \\(C_p\\) from \\(\\text{Err}\\). (2.2) is a special case of (2.1) if \\(\\sum_{i=1}^n\\text{cov}\\left(\\hat{Y}_i;Y_i\\right)=p\\sigma^2\\). We can make use of the trace operator (see trace) to write \\[\\begin{equation} \\sum_{i=1}^n\\text{cov}\\left(\\hat{Y}_i;Y_i\\right) = \\mbox{tr}\\left(\\mbox{cov}(\\hat{\\mathbf{Y}},\\mathbf{Y})\\right) \\end{equation}\\] Since \\(\\hat{\\mathbf{Y}} = \\mathbf{H}\\boldsymbol{Y}\\), with \\(\\mathbf{H} = \\mathbf{X}(\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\) is the least squares projection matrix (or hat matrix) of trace \\(p\\), we can then write \\[\\begin{eqnarray} \\mbox{tr}\\left(\\mbox{cov}(\\hat{\\mathbf{Y}},\\mathbf{Y})\\right) &amp;=&amp; \\mbox{tr}\\left( \\mbox{cov}(\\mathbf{HY},\\mathbf{Y})\\right)\\\\ &amp;=&amp; \\mbox{tr}\\left( \\mathbf{H}\\mbox{cov}(\\mathbf{Y},\\mathbf{Y})\\right) = \\mbox{tr}\\left(\\mathbf{H} \\sigma^2 \\mathbf{I}\\right) \\\\ &amp;=&amp; \\sigma^2 \\mbox{tr}\\left(\\mathbf{H}\\right) = \\sigma^2 p \\end{eqnarray}\\] q.e.d. Note that (2.2) cannot be used in a real situation since the value of \\(\\sigma^2\\) is unknown. In practice, \\(\\sigma^2\\) is replaced by \\(\\hat{\\sigma}^2\\) suitabily chosen. For that, there is no optimal choice and what is usually preferred is the residual variance estimator at the full or maximal model. When \\(n&lt;p\\), the choice is still an open question… Exercise (optional): Consider a Linear Mixed Model (LMM), for example the electrode resistance data, estimated using the generalized least squares (GLS) estimator. - Derive and/or estimate \\(\\widehat{\\text{Err}}\\). Hint: write the model with a vector of stacked responses and, consequently, a non-diagonal residual error (co)variance. 2.3.3 Efron’s \\(q\\)-class Covariance penalties can be applied to measures of prediction error other than squared error, like the Kullback - Leibler divergence. Then, to derive \\(\\widehat{\\text{Err}}\\), one needs a more general expression for \\(\\mathbb{E}\\left[\\text{Err}_i\\right]\\). B. Efron (1986a) uses a function \\(Q(\\cdot,\\cdot)\\) based on the \\(q\\)-class error measure between two scalar functions \\(u\\) and \\(v\\), given by \\[\\begin{equation} Q(u,v) = q(v) + \\dot{q}(v) (u - v) - q(u) \\tag{2.3} \\end{equation}\\] where \\(\\dot{q}(v)\\) is the derivative of \\(q( \\cdot )\\) evaluated at \\(v\\). For example \\(q(v) = v(1-v)\\) gives the squared loss function \\(Q(u,v) = (u - v)^2\\) and \\(q(v)=\\min\\{v,(1-v)\\}\\) leads to the missclassification loss \\(Q(u,v)=I\\{u\\neq I(u&gt;1/2)\\}\\), where \\(I(\\cdot)\\) denotes the indicator function. Efron’s optimism Theorem (Efron 2004) demonstrates that \\[\\begin{equation*} \\text{Err}_i = \\mathbb{E} \\left[ \\mathbb{E}_0 \\left[ Q(Y^0_i,\\hat{Y}_i) | \\mathbf{y}\\right] \\right] =\\mathbb{E} \\left[ Q(Y_i,\\hat{Y}_i) + \\Omega_i \\right] \\label{eq:optimismTHM} \\end{equation*}\\] with \\(\\Omega_i = \\text{cov} \\left( \\dot{q}(\\hat{Y}_i),Y_i \\right)\\). Hence, an estimator of \\(\\text{Err}\\) is obtained as \\[\\begin{equation} \\widehat{\\text{Err}} = \\frac{1}{n} \\sum_{i = 1}^{n}\\left( Q(y_i,\\hat{y}_i) + \\widehat{\\text{cov}} \\left( \\dot{q}(\\hat{Y}_i),Y_i \\right)\\right) \\end{equation}\\] Exercise: - Verify that by setting \\(q(v)=\\min\\{v,(1-v)\\}\\) one gets \\(Q(u,v)=I\\{u\\neq I(u&gt;1/2)\\}\\). 2.4 Information Theory and Bayesian Criteria 2.4.1 AIC: Akaike Information Criterion The AIC is derived from Information Theory which concerns the quantification, storage, and communication of information (Shannon (1948a),Shannon (1948b)). Associated measures are applied to distributions of random variables and include the entropy measure for a single random variable. A derived measure for two random variables is the Kullback-Leibler divergence (or information divergence, information gain, or relative entropy). Consider two densities \\(f_0\\) and \\(f_1\\), the Kullback–Leibler divergence is \\[\\begin{equation} D\\left(f_0,f_1\\right)=2\\int f_0(y) \\log\\left(\\frac{f_0(y)}{f_1(y)}\\right)dy = 2\\mathbb{E}_0\\left[\\log\\left(\\frac{f_0(y)}{f_1(y)}\\right)\\right] \\end{equation}\\] The Kullback-Leibler divergence can be used to evaluate the adequacy of a model, by considering e.g. \\(f_1:=f(y;\\hat{\\boldsymbol{\\theta}})\\), the fitted density corresponding to model \\(F_{\\boldsymbol{\\theta}}\\). In that case, the true prediction error becomes a deviance given by \\[\\begin{equation} \\text{Err}_i=2\\mathbb{E}_0\\left[\\log\\left(\\frac{f_0(y)}{f(y_i;\\hat{\\boldsymbol{\\theta}})}\\right)\\right] \\end{equation}\\] with total deviance \\(1/n\\sum_{i=1}^n \\text{Err}_i\\). Akaike (Akaike 1974) proposed to consider, as a model adequacy measure, an estimator of \\[\\begin{equation} 2\\mathbb{E}_0\\left[\\mathbb{E}\\left[\\log\\left(f_0(y)\\right)-\\log\\left(f(y;\\hat{\\boldsymbol{\\theta}})\\right)\\right]\\right] \\end{equation}\\] where \\(\\mathbb{E}\\) denotes the expectation at the insample distribution. Akaike derived the estimator \\[\\begin{equation} -2\\sum_{i=1}^n \\log f(\\hat{y}_i;\\hat{\\boldsymbol{\\theta}})+2p + \\text{const.} \\end{equation}\\] where \\(\\text{const.}\\) does not depend on the model and hence can be omitted when comparing models. For the linear regression model with \\(\\hat{\\boldsymbol{\\mu}}=\\mathbf{x}^T \\hat{\\boldsymbol{\\beta}}\\), supposing \\(\\sigma^2\\) known (and omitting the constant term), we have \\[\\begin{equation} \\frac{1}{\\sigma^2}\\sum_{i=1}^n \\left(y_i-\\mathbf{x}_i^T \\hat{\\boldsymbol{\\beta}}\\right)^2+2p \\end{equation}\\] There exist several expressions for the AIC for the linear regression model, one of them being \\[\\begin{equation} \\text{AIC}= \\frac{1}{n\\sigma^2} \\text{RSS}+\\frac{2}{n}p \\end{equation}\\] where \\(\\text{RSS}=\\sum_{i=1}^n\\left(y_i-\\mathbf{x}_i^T \\hat{\\boldsymbol{\\beta}}\\right)^2\\) is the residual sum-of-squares. We can see that \\(C_p=\\sigma^2\\text{AIC}\\). Exercise (solutions (7.2.2) provided by Alexander Maslev, Hanxiong Wang and Minyoung Lee). Derive the AIC for the regression model from its more general definition. (optional) Program AIC and do model selection in a specific simulation setting with an exhaustive search (follow the passages listed in the CV exercise section). Compare the performance of your programmed CV and AIC by replicating 100 times the tasks. In particular you should evaluate three specific criteria: the proportion of times the correct model is selected (Exact), the proportion of times the selected model contains the correct one (Correct) and the average number of selected regressors (Average \\(\\sharp\\)) In the same simulation setting outlined in the CV exercise section, generate from a MVN (multivariate normal) a matrix \\(\\mathbf{X_{n*p}}\\) with \\(n = 1000\\) and \\(p = 5\\) but now fix the scale matrix with an autoregressive form \\(\\boldsymbol{\\Sigma}=[\\sigma_{lm}]_{l,m=1,\\ldots,p}\\) with \\(\\sigma_{lm} = \\rho^{\\mid l - m\\mid}\\). Compare the performance of CV and AIC for \\(\\boldsymbol{\\rho} = [0.2 \\; 0.5\\; 0.7]\\) (\\(\\rho = 0\\) corresponds to the identity case that you have already treated). Upload the Zambia dataset and perform an exhaustive search on the continuous covariates (i.e. avoiding factors) based on CV and AIC in order to find the best model. You can either employ your codes derived in previous exercises or make use of the existing R packages: leaps, glmulti, MuMIn and caret. Exercise (optional): Using the general result on covariance penalty measures based on Efron’s \\(q\\)-class, show that the AIC is a suitable estimator of the prediction error. 2.4.2 BIC: Bayesian Information Criterion (Schwarz 1978) derived the Bayesian information criterion as \\[\\begin{equation} \\text{BIC} = -\\sum_{i=1}^n \\log f(\\hat{y}_i;\\hat{\\boldsymbol{\\mu}},\\hat{\\sigma}^2)+p\\log(n) + \\text{const.} \\end{equation}\\] where \\(\\log(n)\\) is the natural logarithm of n and \\(\\text{const.}\\) does not depend on the model; hence can be omitted when comparing models. The BIC is derived from Bayesian inference arguments, but is not related to information theory. Compared to the AIC (or indeed the \\(C_p\\)), the BIC uses \\(p\\log(n)\\) instead of \\(2p\\) as an estimated penalty and since \\(\\log(n)&gt; 2\\) for any \\(n &gt; 7\\), the BIC statistic generally places a heavier penalty on models with many variables, and hence results in the selection of smaller models than AIC. 2.5 Mean Squared Error Based Criteria 2.5.1 Stein’s unbiased risk estimator (SURE) Stein (1981) derived an unbiased estimator of the mean-squared error of “a nearly arbitrary, nonlinear biased estimator”, hence providing an indication of the accuracy of a given estimator. Namely, consider the normal setting with \\(Y\\vert \\mathbf{X} \\sim \\cal{N}\\left(\\boldsymbol{\\mu}(\\mathbf{X}),\\sigma^2\\right)\\) with a differentiable estimator \\(\\hat{\\boldsymbol{\\mu}}:=\\hat{\\boldsymbol{\\mu}}(Y,\\mathbf{X})\\), Stein (1981) proposed an unbiased estimator of \\(\\mathbb{E}\\left[\\vert\\vert \\boldsymbol{\\mu}-\\hat{\\boldsymbol{\\mu}}\\vert\\vert_2^2\\right]\\) (MSE), given by \\[\\begin{equation} \\text{SURE}=\\sum_{i=1}^n\\left(\\hat{y}_i-y_i\\right)^2+2\\sigma^2\\sum_{i=1}^n \\frac{\\partial\\hat{\\mu}_i}{\\partial y_i}-n\\sigma^2 \\end{equation}\\] Actually, for \\(Z\\sim N(0,\\sigma^2)\\), it can be shown that \\(\\mathbb{E}\\left[Zf(Z)\\right]=\\sigma^2\\mathbb{E}\\left[f^{&#39;}(Z)\\right]\\) (Stein 1981), so that \\(\\sigma^2\\sum_{i=1}^n\\mathbb{E}\\left[\\partial\\hat{\\mu}_i/\\partial Y_i\\right]=\\sum_{i=1}^n\\text{cov}\\left(Y_i,\\hat{Y}_i\\right)\\). This implies that, in the covariance penalty framework of Section 2.3, for the normal model, \\(\\sigma^2\\sum_{i=1}^n \\partial\\hat{\\mu}_i/\\partial y_i\\) is a suitable estimator of \\(\\sum_{i=1}^n\\text{cov}\\left(Y_i,\\hat{Y}_i\\right)\\). For the linear model with \\(\\boldsymbol{\\mu}=\\mathbf{X}\\boldsymbol{\\beta}\\) and considering the OLS, \\(\\hat{\\boldsymbol{\\mu}}=\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{^-1}\\mathbf{X}^T\\mathbf{y}\\), we have \\(\\partial\\hat{\\mu}_i/\\partial y_i=\\mathbf{x}_i(\\mathbf{X}^T\\mathbf{X})^{^-1}\\mathbf{x}_i^T\\), and hence the SURE is not equal to the \\(C_p\\). It is however more general, i.e. for any differentiable estimator \\(\\hat{\\boldsymbol{\\mu}}\\) of \\(Y\\), and hence useful in nonparametric regression (see e.g. Donoho and Johnstone (1995)) Exercise: show that for \\(Z\\sim N(0,\\sigma^2)\\), \\(\\mathbb{E}\\left[Zf(Z)\\right]=\\sigma^2\\mathbb{E}\\left[f^{&#39;}(Z)\\right]\\) 2.5.2 The Focused Information Criterion (FIC) The FIC in its original format (see Claeskens and Hjort (2003)) interprets best model in the sense of minimizing the mean squared error (MSE) of the estimator of the quantity of interest. The FIC philosophy puts less emphasis on which variables are in the model but rather on the accuracy of the estimator of a focus. To build the FIC, one considers a model of the form \\(F_{\\boldsymbol{\\nu}, \\boldsymbol{\\gamma}}\\), with density \\(f(\\cdot;\\boldsymbol{\\nu},\\boldsymbol{\\gamma})\\), with \\(\\boldsymbol{\\nu} \\in \\mathbb{R}^p\\) not subject to model selection (i.e. included in all considered models), \\(\\boldsymbol{\\gamma} \\in \\mathbb{R}^q\\) the parameters on which model selection is performed. \\(\\boldsymbol{\\gamma}\\) and \\(q\\) are allowed to depend on the sample size \\(n\\), hence \\(\\boldsymbol{\\gamma}:= \\boldsymbol{\\gamma}_{n}\\) and \\(q:= q_n\\). For the linear regression model \\(Y_i|\\mathbf{x}_i \\sim \\mathcal{N}(\\beta_0+\\mathbf{x}_i\\boldsymbol{\\beta},\\sigma^2)\\)(with \\(\\mathbf{x}_i\\) not containing the one in the first column), a natural choice is \\(\\boldsymbol{\\nu} = (\\beta_0,\\sigma^2)\\) and \\(\\boldsymbol{\\gamma}_n = \\boldsymbol{\\beta}\\). The focus, or quantity of interest, is \\(\\boldsymbol{\\mu}:=\\boldsymbol{\\mu}(\\boldsymbol{\\nu},\\boldsymbol{\\gamma}_n)\\), which can be, but not necessarily, the prediction for one particular new observation. Given a (chosen) estimator for the focus, \\(\\hat{\\boldsymbol{\\mu}}:=\\boldsymbol{\\mu}(\\hat{\\boldsymbol{\\nu}},\\hat{\\boldsymbol{\\gamma}}_n)\\), assuming \\(\\boldsymbol{\\gamma}_n= \\boldsymbol{\\gamma} + \\boldsymbol{\\delta}/\\sqrt{n}\\) and considering a fixed value \\(q\\), Claeskens and Hjort (2003) use a Taylor expansion of \\(\\sqrt{n}\\left(\\hat{\\boldsymbol{\\mu}}-\\boldsymbol{\\mu}\\right)\\) to derive the bias and variance to build the (first order) MSE. More specifically, consider the set of indices \\(S\\subseteq \\left\\{1,\\ldots,q\\right\\}\\) such that \\(\\boldsymbol{\\gamma}_S\\subseteq \\boldsymbol{\\gamma}\\) is the corresponding subset of parameters, hence forming a submodel \\(S\\) of \\(F_{\\boldsymbol{\\nu},\\boldsymbol{\\gamma}}\\), one gets \\[\\begin{equation} \\sqrt{n}\\left(\\hat{\\boldsymbol{\\mu}}_S-\\boldsymbol{\\mu}\\right)\\approx \\left[\\frac{\\partial\\boldsymbol{\\mu}(\\boldsymbol{\\nu},\\boldsymbol{\\gamma})}{\\partial\\boldsymbol{\\nu}}\\right]^T\\sqrt{n}\\left(\\hat{\\boldsymbol{\\nu}}-\\boldsymbol{\\nu}\\right)+ \\left[ \\frac{\\partial\\boldsymbol{\\mu}(\\boldsymbol{\\nu},\\boldsymbol{\\gamma})}{\\partial\\boldsymbol{\\gamma}_S} \\right]^T\\sqrt{n}\\left(\\hat{\\boldsymbol{\\gamma}}_S-\\boldsymbol{\\gamma}\\right)-\\frac{\\partial\\boldsymbol{\\mu}(\\boldsymbol{\\nu},\\boldsymbol{\\gamma})}{\\partial\\boldsymbol{\\gamma}}\\boldsymbol{\\delta} \\end{equation}\\] To derive the MSE (of \\(\\hat{\\boldsymbol{\\mu}}_S\\)), Claeskens and Hjort (2003) use the asymptotic distribution of \\(\\sqrt{n}\\left(\\hat{\\boldsymbol{\\mu}}_S-\\boldsymbol{\\mu}\\right)\\). Project: - For the Malnutrition in Zambia dataset, considering the breastfeeding duration as the most important variable, develop an estimator for the FIC, build a model that optimizes the FIC and compare the resulting model with the one obtained with other methods. 2.6 Classification measures When model building is about predicting binary outcome, like for example the success (or not) of a treatment in medical research, one enters in the world of classification. We are concerned here in supervised classification, i.e. the exercise of building a classifier from a dataset containing the outcome (response) variable. When the outcome variable is not available, the exercise is referred to as unsupervised classification (or clustering). In a model building framework, the obvious model for binary classification is the logistic regression, a member of the family of generalized linear models (GLM). In this setting, prediction error becomes classification error and as it will be seeing below, there is no unique and natural measure, which imply that assessing the validity of the model (i.e. the classification rule) is not a straightforward task. Indeed, the nature of the problem can make classical prediction error measures rather useless. For example, suppose that the objective is to build a logistic model to classify patients into two groups, the ones with a serious disease and the other healthy ones. Most often, the collected data will contain a large majority of healthy patients (representing what happens in the population), say 95%, so that a good classifier (that is difficult to beat), is the one that predicts ALL patients as healthy. In that case, the classification error as measured by a prediction criterion would be of only 5%. Hence, other criteria need to be considered when assessing the validity of a classifier (model). 2.6.1 The logistic model The logistic model links, through the Bernoulli distribution, the outcome variable (response) \\(Y\\) and the linear predictor \\(\\mathbf{X}\\boldsymbol{\\beta}\\), \\(\\mathbf{X}\\) being an \\(n\\times p\\) matrix of fixed covariates with row \\(\\mathbf{x}_i,i=1,\\ldots,n\\), by means of \\(Y_i\\vert \\mathbf{x}_i \\sim_{iid} B(\\pi_i)\\), with \\(\\pi_i=\\mathbb{E}[Y_i]=P(Y_i=1)=\\exp(\\mathbf{x}_i\\boldsymbol{\\beta})/(1+\\exp(\\mathbf{x}_i\\boldsymbol{\\beta}))\\) (logistic link). An estimator for \\(\\boldsymbol{\\beta}\\) is the maximum likelihood estimator (MLE) with score functions \\[\\begin{equation} \\mathbf{S}(\\boldsymbol{\\beta}\\vert \\mathbf{X},\\mathbf{y})=\\sum_{i=1}^n\\mathbf{x}^T_iy_i- \\sum_{i=1}^n\\frac{\\exp(\\mathbf{x}_i\\boldsymbol{\\beta})}{1+\\exp(\\mathbf{x}_i\\boldsymbol{\\beta})}\\mathbf{x}^T_i \\end{equation}\\] so that \\(\\hat{\\boldsymbol{\\beta}}=\\mbox{argzero}_{\\boldsymbol{\\beta}}\\mathbf{S}(\\boldsymbol{\\beta}\\vert \\mathbf{X},\\mathbf{y})\\) which has no closed form. In high dimensions, this feature adds numerical difficulties compared to the linear regression model. Indeed, a popular algorithm for finding \\(\\hat{\\boldsymbol{\\beta}}\\) is the iteratively reweighted least squares with iterative \\(k\\)th step given by \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}^{k}\\equiv \\hat{\\boldsymbol{\\beta}}^{k-1}+\\mathbf{J}^{-1}\\left(\\hat{\\boldsymbol{\\beta}}^{k-1}\\right)\\mathbf{S}(\\boldsymbol{\\beta}\\vert \\mathbf{X},\\mathbf{y}) \\end{equation}\\] with \\(\\mathbf{J}\\left(\\hat{\\boldsymbol{\\beta}}\\right)\\) being the negative of the Hessian matrix (partial derivative of \\(\\mathbf{S}(\\boldsymbol{\\beta}\\vert \\mathbf{X},\\mathbf{y})\\) with respect to \\(\\boldsymbol{\\beta}\\)) that depends on the current value of \\(\\hat{\\boldsymbol{\\beta}}\\), hence requiring numerous inversions of high dimensional matrices. 2.6.2 Prediction error measures for Binary classification Before considering a prediction error measure, one should define a prediction measure for the logistic model. There are two common measures: - The fitted probabilities: \\(\\hat{\\pi}_i=\\widehat{\\mathbb{E}[Y_i]}=\\exp(\\mathbf{x}_i\\hat{\\boldsymbol{\\beta}})/(1+\\exp(\\mathbf{x}_i\\hat{\\boldsymbol{\\beta}}))\\) - The predicted outcome: \\(I\\left(\\hat{\\pi}_i\\geq c\\right)\\) with \\(I\\left(\\right)\\) being the indicator function and \\(c\\) a chosen threshold with a priori \\(c=0.5\\). A classification error then builds, in general, upon \\(\\left(y_i-\\hat{\\pi}_i\\right)\\) or \\(\\left(y_i-I\\left(\\hat{\\pi}_i\\geq c\\right)\\right)\\). Popular measures include: The quadratic error: \\(\\Vert \\mathbf{y}-\\hat{\\boldsymbol{\\pi}}(\\mathbf{X})\\Vert_2\\) with \\(\\mathbf{y}=[y_i]_{i=1,\\ldots,n}\\), \\(\\hat{\\boldsymbol{\\pi}}(\\mathbf{X})=[\\hat{\\pi}_i(\\mathbf{x}_i)]_{i=1,\\ldots,n}\\) and \\(\\Vert \\cdot \\Vert_2\\) denoting the \\(L2\\)-norm. The misclassification error or Hamming distance: \\(\\sum_{i=1}^n(y_i-I\\left(\\hat{\\pi}_i(\\mathbf{x}_i)\\geq c\\right))^2\\). The Binomial deviance (or cross-entropy): \\(\\sum_{i=1}^n\\left(y_i\\log(\\hat{\\pi}_i(\\mathbf{x}_i))-(1-y_i)\\log(1-\\hat{\\pi}_i(\\mathbf{x}_i))\\right)\\). The hinge loss (or support vector machines, Vapnik (1996)): \\(\\sum_{i=1}^n \\max\\left(1−(2y_i −1)\\mbox{sign}(\\hat{\\pi}_i(\\mathbf{x}_i) −c), 0\\right)\\), with \\(c=0.5\\). The exponential loss (from the AdaBoost algorithm in generalized additive models, Hastie, Tibshirani, and Friedman (2009)): \\(\\sum_{i=1}^n\\exp\\left(-(2y_i-1)\\mathbf{x}_i\\hat{\\boldsymbol{\\beta}}\\right)\\). The Hamming distance does not really predict above (below) the threshold \\(c\\), since they are the same for say \\(\\hat{\\pi}_i(\\mathbf{x}_i)=0.6\\) and \\(\\hat{\\pi}_i(\\mathbf{x}_i)=0.9\\) with \\(c=0.5\\). There is therefore an information loss. In general, the (total) deviance for a model with estimates \\(\\hat{\\boldsymbol{\\mu}}(\\mathbf{X}) =\\widehat{\\mathbb{E}}\\left[ \\mathbf{Y} \\vert \\mathbf{X},\\hat{\\boldsymbol{\\beta}}\\right]\\) may be constructed by its likelihood function as \\[\\begin{equation} D \\left( \\mathbf{Y} , \\hat{\\boldsymbol{\\mu}}(\\mathbf{X}) \\right) = 2 \\left( \\log P ( \\mathbf{Y} \\vert \\mathbf{X},\\hat{\\boldsymbol{\\beta}}^s ) − \\log (P (\\mathbf{Y} \\vert \\mathbf{X},\\hat{\\boldsymbol{\\beta}} ) \\right) \\end{equation}\\] where \\(\\hat{\\boldsymbol{\\beta}}^s\\) denotes the fitted parameters for the saturated model, i.e. with a parameter for every observation so that the data are fitted exactly. Hence, \\(P ( \\mathbf{Y} \\vert \\mathbf{X},\\hat{\\boldsymbol{\\beta}}^s )=\\mathbf{1}\\) so that \\(\\log P ( \\mathbf{Y} \\vert \\mathbf{X},\\hat{\\boldsymbol{\\beta}}^s )=\\mathbf{0}\\) and for the logistic model with \\(\\hat{\\boldsymbol{\\mu}}(\\mathbf{X})=\\hat{\\boldsymbol{\\pi}}(\\mathbf{X})\\), \\(D \\left( \\mathbf{Y} , \\hat{\\boldsymbol{\\mu}} (\\mathbf{X})\\right)\\) becomes the Binomial deviance. The hinge and exponential loss functions are particular cases of a general loss function used in machine learning: for binary outcomes \\(y_i^{\\star}=2y_i-1\\) taking the values \\(-1\\) or \\(1\\) and a prediction function \\(\\hat{\\mu}_i(\\mathbf{x}_i)\\), a general loss function is \\(V\\left(y_i^{\\star},\\hat{\\mu}_i(\\mathbf{x}_i)\\right)=\\phi(-y_i^{\\star}\\hat{\\mu}_i(\\mathbf{x}_i))\\). The hinge loss uses \\(\\hat{\\mu}_i(\\mathbf{x}_i)=\\mbox{sign}(\\hat{\\pi}_i(\\mathbf{x}_i) −c)\\), hence is based on the fitted probabilities, while the exponential loss uses \\(\\hat{\\mu}_i(\\mathbf{x}_i)=\\mathbf{x}_i\\hat{\\boldsymbol{\\beta}}\\). Note that \\(\\mathbf{x}_i\\hat{\\boldsymbol{\\beta}}=\\log\\left(\\hat{\\pi}_i(\\mathbf{x}_i)/(1-\\hat{\\pi}_i(\\mathbf{x}_i))\\right)\\) A comparison between different classification error measures (loss functions) can be found in Hastie, Tibshirani, and Friedman (2009), Section 10.6. 2.6.3 Classification error estimation Efron’s optimism Theorem (Efron 2004) can also be used to obtain an estimator of the out-of-sample classification error, at least up to \\(\\widehat{\\text{cov}} \\left( \\dot{q}(\\hat{\\mu}_i),Y_i \\right)\\), with the appropriately chosen prediction function \\(\\hat{\\mu}_i\\). Exercise: - Show that when \\(q(v) = v(1-v)\\) in (2.3), one obtains for the loss function the quadratic error. - Show that when \\(q(v)=\\min\\{v,(1-v)\\}\\) in (2.3), one obtains for the loss function the misclassification error. - Show that when \\(q(v)= -2[v \\log(v) + (1-v)\\log(1-v)]\\) in (2.3), one obtains for the loss function the binomial deviance or twice the Kullback-Leibler divergence. - Show that when \\(q(v)=2\\min\\{v,(1-v)\\}\\) in (2.3), one obtains for the loss function the hinge loss. - Show that when \\(q(v)=2\\sqrt{v,(1-v)}\\) in (2.3), one obtains for the loss function the exponential loss. Deriving \\(\\text{cov} \\left( \\dot{q}(\\hat{\\mu}_i),Y_i \\right)\\) analytically and estimating it by then plugging in parameter’s estimates is not a straightforward task. Approximations provided in Efron (1978) and B. Efron (1986b) can be used. Alternatively, one can rely on simulation based (bootstrap) estimation of the covariance, as is done in Efron (2004), but in high dimensions, the computational cost could be prohibitive. Ecercise (optional): - Derive \\(\\text{cov} \\left( \\dot{q}(\\hat{\\mu}_i),Y_i \\right)\\) for the quadratic error, using a Efron’s approximation \\(\\hat{\\boldsymbol{\\beta}} = \\Sigma^{-1} \\hat{\\boldsymbol{\\theta}}\\), with \\(\\hat{\\boldsymbol{\\beta}}=\\sum_{i=1}^n \\mathbf{x}_{i}y_{i}\\) and \\(\\Sigma=\\sum_{i=1}^n \\pi_{i} (1-\\pi_{i})\\), \\(\\mathbf{x}_{i} \\mathbf{x}_{i}^{T}\\) and also using first order Taylor expansions. 2.6.4 The ROC curve The receiver operating characteristic curve, i.e. ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. Indeed, with binary classification, supposing two possible outcomes denoted as positive, respectively, negative, there are four possible classification outcomes: - True positive (TP): the outcome is positive and correctly predicted as positive. - False positive (FP) occurs when the outcome is negative and predicted as positive. - True negative (TN): the outcome is negative and predicted as negative. - False negative (FN) occurs when the outcome is positive and predicted as negative. The corresponding rates are the probability (proportion) of occurrences that are correctly identified as such. For example, the TPR is the probability of positive prediction given that true state is positive, or in other words, the proportion of positive predictions among all positive outcomes. The rates are estimated from the sample for a selected model (classifier). The TPR is also known as sensitivity and the TNR as the specificity (the probability of negative prediction given that the true state is negative). Sensitivity therefore quantifies the avoiding of FN, and specificity does the same for FP. There is actually a trade-off between the measures like with hypothesis testing and the associated two error-types. This trade-off can be represented graphically using the ROC curve which plots 1 - specificity versus sensitivity. These measures and representations are widely used in medical sciences, where, ideally, a perfect model (classifier) would be described as 100% sensitive, meaning e.g. all sick individuals are correctly identified as sick, and 100% specific, meaning no healthy individuals are incorrectly identified as sick. Example: ROC curve and associated trade-off measures # Taken from https://en.wikipedia.org/wiki/Receiver_operating_characteristic. # # The ideal model (classifier) would reach the (0,100)% point. # knitr::include_graphics(&quot;Figures/ROC_curves.png&quot;) The ROC curve is mostly used as a diagnostic tool to evaluate a selected model (classifier). Indeed, in subset selection, a set of predictors (say \\(\\mathbf{X}_S\\)) are chosen and from which, using the logistic regression, fitted probabilities \\(\\hat{\\pi}_i(\\mathbf{x}_{iS})\\) can be computed. The ROC curve is then computed by varying the threshold \\(c\\) in predicting outcomes using \\(\\hat{y}_i=I(\\hat{\\pi}_i(\\mathbf{x}_{iS})\\geq c)\\). Indeed, for different values of \\(c\\), the predictions \\(\\hat{y}_i\\) are different, hence leading to different measures for the sensitivity and the specificity. Conventionally, the sensitivity and specificity are computed at \\(c=0.5\\). Exercise (solutions presented in 7.2.3) Read the information on the Leukemia Dataset in the first chapter of the book. Then load the Leukemia dataset reduced which contains a subset of 11 eleven predictors among the 3571 present in the leukemia_small.csv. These variables have been selected, because of their importance, by the binary lasso which is a shrinkage method that will be discussed later on in the course. Now perform the following steps: (a) Fit the appropriate GLM for the situation using only one of the available predictors (e.g. V457) (b) Read the ROC curve section of the e-book. Then find the TPR (i.e. true positive rate), FPR (i.e. false positive rate), TNR (i.e. true negative rate), FNR (i.e. false negative rate) of the fitted values found at point (a) with a cut-off value \\(c = 0.5\\). (c) For a given cut-off grid of values, that you can choose as you wish, plot the ROC curve relative to the estimated model at point (a). (d) Check the quality of your result at point (c) with the R package pROC. Exercise (optional): - For a given selected model (classifier), using the logistic model and estimator \\(\\hat{\\boldsymbol{\\beta}}\\), express (formula) the estimated sensitivity and specificity as a function of the threshold \\(c\\). The area under the ROC is a scalar summary measure that is sometimes used as model validity. It is sometimes called the c-statistic. Alternative summary statistics include the Gini coefficient which corresponds to the area between the ROC curve and the no-discrimination line. However, for evaluating the contribution of an additional predictor when added to a standard model, any summary statistic associated to the ROC curve, like the the c-statistic, may not be an informative measure. The new predictor can be very significant in terms of the change in e.g. model deviance, but show only a small increase in the c-statistic (see Hastie, Tibshirani, and Friedman (2009)). References "],
["ordering-the-variables.html", "3 Ordering the variables 3.1 Introduction 3.2 Stepwise forward regression 3.3 Streamwise regression 3.4 Classification And Regression Tree (CART)", " 3 Ordering the variables 3.1 Introduction In high dimensional settings, for causal models, the available set of potential predictors can be very large and an exhaustive building of potential models (to be compared in terms of model validity) is practically impossible. One hence needs to find suitable methods for reducing the model size (number of predictors) by either: (a) ordering the predictors that enter the model sequentially, to constitute, at most, \\(p\\) potential models to be assessed and compared, (b) averaging groups of predictors. For (a), the ordering consists in a sequence of steps in which, starting from a null model (usually just the intercept), a covariate is chosen for addition to the current model based on some prespecified criterion. The sequence then stops when an overall criterion is met. For (b) averaging predictors is very popular in machine learning (supervised learning) and in this chapter we only review Classification And Regression Tree (CART). Obviously, the two approaches do not lead to the same type of chosen model. While stepwise methods lead to models that only include a subset of the available covariates, averaging methods use all the available covariates and group them in weighted averages. If the model is used to understand the underlying phenomenon, then stepwise methods are usually preferred. Exercise: Compare the number of models to be considered in an exhaustive approach to the ones considered in a stepwise forward approach. Choose \\(p=\\{5,10,15,20\\}\\) and suppose that the true (best) model has \\(p\\) and \\(p/2\\) predictors. 3.2 Stepwise forward regression Very generally, a forward stepwise regression follows the steps: 1. Let \\(\\cal{M}_0\\) denote the null model, which typically contains no predictors. 2. For \\(k = 1,\\ldots,p-1\\), do : (a) Consider all \\(p-k\\) models that augment the predictors in \\(\\mathcal{M}_k\\) with one additional predictor. (b) Choose the best among these \\(p-k\\) models, and call it \\(\\mathcal{M}_{k+1}\\). 3. Stop the algorithm if \\(\\mathcal{M}_{k+1}\\) is not better than \\(\\mathcal{M}_{k}\\) and provide \\(\\mathcal{M}_{k}\\) as output. The algorithms differ in the definition of best in 2(b) and in the stopping rule criteria in 3. For the latter, it can be chosen among the criteria presented in Chapter 2. As an example, consider the \\(C_p\\) given in (2.2) as model selection criterion. We note that for fixed \\(p\\), i.e. when comparing models of the same size \\(p\\), the difference among the possible models that can be constructed is measured by the residual sum of squares \\(\\sum_{i=1}^n\\left(\\hat{y}_i-y_i\\right)^2\\). Hence, choosing the best model in 2(b) amounts at choosing the added covariate that most reduces the residual sum of squares of the augmented model. In a sequential framework, optimizing the choice of the covariate to add to the current model does not necessary lead to the best model of the current size since the optimization is done on the added covariate only. Additional properties need therefore to be considered. 3.2.1 Partial correlations The partial correlation between \\(X_j\\) and \\(Y\\) conditional of a set of \\(q\\) variables \\(\\mathbf{X} = \\{X_1, X_2, \\ldots, X_q\\}, X_j\\notin \\mathbf{X}\\), written \\(\\hat{\\rho}_{X_jY,\\mathbf{X}}\\), is the correlation between the residuals resulting from the linear regression of \\(X_j\\) on \\(\\mathbf{X}\\) and \\(Y\\) on \\(\\mathbf{X}\\). Namely, let \\(\\mathbf{X}_k\\), of size \\(n\\times q\\), and \\(\\mathbf{X}_{k+1}=[\\mathbf{x}_j]_{j=1,\\ldots,p-q}\\) be the matrices formed by the columns of \\(\\mathbf{X}\\) (containing all the potential predictors) that are a) present at the current stepwise forward step for \\(\\mathbf{X}_k\\) and b) not present at the current stepwise forward step for \\(\\mathbf{X}_{k+1}\\). The partial correlations between each covariate \\(X_j\\) (corresponding to the columns of \\(\\mathbf{X}_{k+1}\\)) and the response vector \\(Y\\), given the set of \\(q\\) covariates corresponding to \\(\\mathbf{X}_k\\), can be written as \\[\\begin{equation} \\hat{\\rho}_{X_jY,\\mathbf{X}_k}=\\frac{\\mathbf{e}^T(\\mathbf{I}-\\mathbf{H})\\mathbf{x}_j}{\\sqrt{\\left(\\mathbf{e}^T\\mathbf{e}\\right)\\left(\\mathbf{x}_j^T(\\mathbf{I}-\\mathbf{H})\\mathbf{x}_j\\right)}} \\end{equation}\\] with \\(\\mathbf{e}=\\mathbf{y}-\\mathbf{X}_k\\hat{\\boldsymbol{\\beta}}\\), \\(\\hat{\\boldsymbol{\\beta}}=\\left(\\mathbf{X}_k^T\\mathbf{X}_k\\right)^{-1}\\mathbf{X}_k^T\\mathbf{y}\\), so that \\(\\mathbf{e}=\\left(\\mathbf{I}-\\mathbf{X}_k\\left(\\mathbf{X}_k^T\\mathbf{X}_k\\right)^{-1}\\mathbf{X}_k^T\\right)\\mathbf{y}=\\left(\\mathbf{I}-\\mathbf{H}\\right)\\mathbf{y}\\). The predictor \\(\\mathbf{X}_j\\) with the largest partial correlation offers the greatest reduction in the residual sum of squares; see e.g. Foster and Stine (2004). In high dimensional settings, the sweep operator (Goodnight 1979) can be used to obtain the partial correlations in a computationally fast manner. Exercise (solution provided by Talia Béatrice Kimber): - Prove that optimization problem (3.1) (i.e. min RSS at a given step) and (3.2) (i.e. max partial correlations) are equivalent: \\[\\begin{equation} \\begin{aligned} &amp; \\underset{j \\in P\\backslash S}{\\text{min}} &amp; &amp; \\| \\mathbf{y} - \\mathbf{X}^{\\star} \\hat{\\beta}^{\\star} - \\mathbf{x}_j \\beta_{j} \\|_{2}^{2} \\\\ \\end{aligned}\\tag{3.1} \\end{equation}\\] \\[\\begin{equation} \\begin{aligned} &amp; \\underset{j \\in P\\backslash S}{\\text{max}} &amp; &amp; \\mid \\mathbf{x}_{j}^{T} (\\mathbf{y} - \\mathbf{X}^{\\star} \\hat{\\beta}^{\\star}) \\mid \\\\ \\end{aligned}\\tag{3.2} \\end{equation}\\] where: a) \\(P = \\{1,..,p\\}\\) is the set of all available predictors. b) \\(S = \\{a \\in P : \\; \\mid S \\mid = q\\}\\) is the solution set at the current step of the procedure. c) \\(\\mathbf{X}^{\\star}\\) is the \\(n \\;x\\; q\\) matrix of predictors selected at previous steps. d) \\(\\hat{\\beta}^{\\star}\\) is the OLS estimator of the model \\(\\mathbf{y} = \\mathbf{X}^{\\star} {\\beta}^{\\star} + \\boldsymbol{\\epsilon}\\). e) \\(\\mathbf{x}_{j}\\) is one of the \\(p - q\\) predictors left at the current step. Assume also that all the predictors have unitary norm (i.e. \\(\\mathbf{x}_{l}^{T}\\mathbf{x}_{l} = 1 \\; \\forall \\;l \\in P\\)). Exercise (optional): - Consider the Malnutrition in Zambia dataset and order the covariates according to their partial correlations using the R function fs of the Selective Inference R Package (https://cran.r-project.org/web/packages/selectiveInference/index.html). - Compare the ordering with the one obtained by the lasso (LARS) and discuss why they are different. 3.2.2 Selection by hypothesis testing Partial correlations have been used in several instances to order the covariates in stepwise forward regression, combined with a stopping criterion based on a test statistic, like the \\(t\\)-test for significance testing. That is, the covariates are entered in the model using the partial correlation criterion (that is maximized) and the procedure stops when the \\(t\\)-test is not significant. However, with testing multiple hypothesis, one has to pay attention to the familywise error rate (FWER), i.e. the probability of making one or more false discoveries, or type I errors (rejecting the null hypothesis when it is true). This probability increases very rapidly with the number of hypothesis to be tested. Another concept is the false discovery rate (FDR) which is a measure that provides less stringent control of Type I errors compared to familywise error rate (Benjamini and Hochberg 1995). FDR actually controls the expected proportion of discoveries (rejected null hypotheses) that are false (incorrect rejections). When all hypotheses are true, the FDR is equal to the FWER, and smaller otherwise. Hence, substantial power can be gained when testing multiple hypothesis, while controlling the FDR. Basically, one can see model selection (subset selection) as a series of hypothesis to be tested, namely significance testing associated to each potential slope \\(\\beta_j, j=1,\\ldots,p\\) at the full model. Because one cannot consider each test separately (FWER effect), what one actually seeks is to set the threshold \\(\\alpha\\) so that the proportion of false discoveries, i.e. the number of rejected hypothesis when they are true, is below this threshold. The proportion of false discovery is relative to the (unknown) number of \\(\\beta_j\\neq 0\\), it is hence given by the ratio \\[\\begin{equation} \\mbox{frd}=\\frac{V_p}{R_p(\\alpha)} \\end{equation}\\] with \\(V_p\\) the (unknown) number of false discoveries and \\(R_p(\\alpha)\\) the number of rejected hypothesis, among \\(p\\) hypothesis. frd is set to 0 when \\(R_p(\\alpha)=0\\). For the later, given a level \\(\\alpha\\), one can compute a \\(p\\)-value associated to each null hypothesis \\(\\mathbb{H}_{j0}:\\beta_j=0,j=1,\\ldots,p\\), say \\(p_j\\), so that \\(R_p(\\alpha)=\\sum_{j=1}^pI(p_j&gt;1-\\alpha)\\). The FRD is defined as the expected value of frd (over the \\(p\\)-values distribution), i.e. \\[\\begin{equation} \\mbox{FRD}=\\mathbb{E}\\left[\\frac{V_p}{R_p(\\alpha)}\\right] \\end{equation}\\] If all \\(\\mathbb{H}_{j0}\\) are true, FDR is equivalent to FWER and in general controlling the FRD controls as well the FWER. In a sequential testing procedure in which the \\(p\\)-values \\(p_k,k=1,\\ldots p\\) (and associated null hypotheses) have been ordered in an ascending order, \\(\\mathbb{H}_{0j}, j=k,\\ldots,p\\) are rejected if \\(p_k&lt;\\alpha_k\\) with \\[\\alpha_k=\\alpha(k+1)/p.\\] With this rejection rule, the FDR (\\(\\leq\\alpha\\)) is controlled (Benjamini and Hochberg 1995). Hence, in a stepwise procedure for variable selection, the search stops at step \\(k-1\\) if \\(p_k&gt;\\alpha_k\\), \\(p_k\\) being associated to the variable to enter at step \\(k\\). G’Sell et al. (2016) propose two other rules to find the cutoff when stopping the stepwise forward search, namely when stopping to reject the null hypothesis. Let \\(\\hat{k}\\) denote the last rejected null hypothesis (in the sequence), the rules are: - ForwardStop: \\(\\hat{k}_F=\\max\\left\\{k\\in\\{1,\\ldots,p\\}\\vert-\\frac{1}{k}\\sum_{j=1}^k\\log\\left(1-p_j\\right)\\leq\\alpha\\right\\}\\) - StrongStop: \\(\\hat{k}_S=\\max\\left\\{k\\in\\{1,\\ldots,p\\}\\vert \\exp\\left\\{\\sum_{j=k}^p\\frac{\\log\\left(p_j\\right)}{j}\\right\\}\\leq\\frac{k\\alpha}{p}\\right\\}\\) with \\(p_j\\) the \\(p\\)-value associated to \\(\\mathbb{H}_j\\) and by convention \\(\\hat{k}=0\\) whenever no rejections can be made. Both rules control the FDR at level \\(\\alpha\\). To order the covariates (hence the null hypotheses), one can use partial correlations, or, alternatively, as proposed in Lin, Foster, and Ungar (2011), to use \\(\\hat{\\gamma}_j\\), the least squares estimator of the model \\[\\begin{equation} \\mathbf{e}=\\gamma_j\\mathbf{x}_j+\\tilde{\\varepsilon} \\end{equation}\\] with \\(\\mathbf{e}=\\mathbf{y}-\\mathbf{X}_k\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{\\boldsymbol{\\beta}}\\) is the least squares estimator of the model at step \\(k\\) (i.e. with \\(\\mathbf{X}_k\\)). Lin, Foster, and Ungar (2011) show that \\(\\hat{\\gamma}_j=\\rho^2\\hat{\\beta}_j\\), where \\(\\hat{\\beta}_j\\) is the least squares estimator of \\(\\beta_j\\) in the model with covariates matrix \\(\\left[\\mathbf{X}_k \\; \\mathbf{x}_j\\right]\\) and with \\[\\begin{equation} \\rho^2=\\mathbf{x}_j^T(\\mathbf{I}-\\mathbf{H})\\mathbf{x}_j \\end{equation}\\] and \\(\\mathbf{H}=\\mathbf{X}_k\\left(\\mathbf{X}_k^T\\mathbf{X}_k\\right)^{-1}\\mathbf{X}_k^T\\). \\(p\\)-values associated to \\(\\beta_j\\) (\\(\\forall j\\) corresponding to the columns of \\(\\mathbf{X}_{k+1}\\)) can be calculated without re-estimating each potential model at a given step and selection is made on the basis their size (in absolute value). Exercise (solutions presented in 7.3.1): - First of all we retrieve the simulation setting used in Practical 3. Now, after having read the documentation of the R package selectiveInference and installed it, perform the following steps: a) Use the functions fs(), fsInf() and forwardStop() to do a stepwise regression based on partial correlations and a model selection phase with the ForwardStop rule on your generated data. Try different values for the type one error: how does the choice of \\(\\alpha\\) impact the model selection technique? b) Given the order of variables produced by fs(), use AIC and BIC criteria for model selection to retrieve your final model (Hint: you do not need to program them, use an existing function of the selectiveInference package). c) Calculate how many models are needed for an exhaustive search in this simulation setting. Use your previous results obtained in Practical 3 to understand the computational time gained by stepwise regression with respect to exhaustive search. Use the package tictoc for this comparison. d) (Optional) Change the simulation setting outlined above to an high dimensional one: generate from a MVN (multivariate normal) a matrix \\(\\mathbf{X_{nxp}}\\) with \\(n = 100\\) and \\(p = 150\\). Evaluate the performance of the ForwardStop rule in this high dimensional setting (i.e. by replicating the model selection task 100 times) thanks to the usual three specific criteria: the proportion of times the correct model is selected (Exact), the proportion of times the selected model contains the correct one (Correct) and the average number of selected regressors (Average \\(\\sharp\\)}). What do you observe? What is the role of \\(\\alpha\\) in this case? Consider the Malnutrition in Zambia dataset. For simplicity work only on the continuous covariates (i.e. avoiding factors) and order them according to their partial correlations using the R function fs of the Selective Inference R Package (https://cran.r-project.org/web/packages/selectiveInference/index.html). Compare the selected models when using: the ForwardStop the StrongStop \\(\\alpha_k=\\alpha(k+1)/p\\) (Benjamini and Hochberg 1995) the \\(C_p\\) or AIC (equal in linear case) the BIC 3.2.3 Orthogonal matching pursuit The Orthogonal Matching pursuit algorithm (OMP) is a greedy algorithms for least squares regression, often called forward greedy selection in the machine learning literature. Zhang (2009) establishes the conditions for the algorithm (see below) to find the correct feature set \\(\\{j\\;\\vert \\; \\beta_j\\neq 0\\}\\) (when \\(n\\rightarrow \\infty\\)). Orthogonal matching pursuit (OMP) algorithm: 1. Inputs: \\(\\mathbf{X}=[\\mathbf{x}_{1},\\dots,\\mathbf{x}_{p}]\\), of size \\(n\\times p\\), \\(\\mathbf{y}\\) and \\(\\epsilon&gt;0\\) 2. Outputs: \\(S^{(\\ell)}\\) and \\(\\boldsymbol{\\beta}_{S^{(\\ell)}}\\) 3. Initial steps: - \\(\\mathbf{\\tilde{x}}_j=\\mathbf{x}_{j}/\\|\\mathbf{x}_{j}\\|_2\\), \\(\\forall j=1,\\dots,p\\), - \\(S^{(0)}=\\emptyset\\), - \\(\\mathcal{P}=\\{1,\\ldots,p\\}\\), - \\(\\hat{\\boldsymbol{\\beta}}_{S^{(0)}}=\\mathbf{0}\\) 4. For\\(\\ell = 1, 2, \\ldots, p\\), do: - Set \\(j^{(\\ell)}=\\arg\\max_{j\\in \\mathcal{P}\\setminus S^{(l-1)}}|\\mathbf{\\tilde{x}}_j^T(\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{S^{(\\ell-1)}})|\\), - \\(|\\mathbf{\\tilde{x}}_{j^{(\\ell)}}^T(\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{S^{(\\ell-1)}})|\\leq \\epsilon\\) break, – Set \\(S^{(\\ell)}=\\{j^{(\\ell)}\\}\\cup S^{(\\ell-1)}\\) and \\(\\mathbf{X}_{S^{(\\ell)}}\\) the column subset of \\(\\mathbf{X}\\) with all columns \\(j\\in S^{(\\ell)}\\), – Set \\(\\hat{\\boldsymbol{\\beta}}_{S^{(\\ell)}}=\\left(\\mathbf{X}_{S^{(\\ell)}}^T\\mathbf{X}_{S^{(\\ell)}}\\right)^{-1}\\mathbf{X}_{S^{(\\ell)}}^T\\mathbf{y}\\). One can see that in the search loop (stage no 4), the variable to enter is chosen as the one to maximize the partial correlation with the response, conditioned on the current variables (selected until the previous step). Indeed \\(\\mathbf{\\tilde{x}}_{j^{(\\ell)}}^T(\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{S^{(\\ell-1)}})=\\) \\(\\mathbf{\\tilde{x}}_{j^{(\\ell)}}^T\\left(\\mathbf{y}-\\mathbf{X}_{S^{(\\ell-1)}}\\left(\\mathbf{X}_{S^{(\\ell-1)}}^T\\mathbf{X}_{S^{(\\ell-1)}}\\right)^{-1}\\mathbf{X}_{S^{(\\ell-1)}}^T\\mathbf{y}\\right)=\\) \\(\\mathbf{\\tilde{x}}_{j^{(\\ell)}}^T\\left(\\mathbf{I}-\\mathbf{H}_{S^{(\\ell-1)}}\\right)\\mathbf{y}\\), and hence \\(\\mathbf{\\tilde{x}}_{j^{(\\ell)}}\\) is selected to maximize the drop in the residual sum of squares based on \\(S^{(\\ell)}\\). Therefore, the OMP can be used in conjunction with a model validity estimator based on the residual sum of squares (L2 norm loss function) such as the \\(C_p\\), i.e. instead of of \\(\\epsilon\\) in the OMP. There is therefore no difference between a stepwise forward regression and the OMP in terms of the variables that are chosen at each step. The only difference is that forward stepwise regression is associated with testing or optimizing a model validity criterion, while the OMP is associated to another criterion \\(\\epsilon\\). 3.2.4 Forward stagewise regression Forward stagewise regression is a smoother variation of stepwise regression that has been overlooked for a while but is coming back in high dimensional settings. It is also based on partial correlations, but at each step the chosen variable is not entered fully. It starts with all coefficients \\(\\beta_j\\) equal to zero, and iteratively updates the \\(\\beta_j\\) by a small amount \\(\\epsilon\\) of the variable \\(X_j\\) that achieves the maximal absolute correlation with the current residuals. Forward Stagewise Regression algorithm: 1. Inputs: \\(\\mathbf{X}=[\\mathbf{x}_{1},\\dots,\\mathbf{x}_{p}]\\), of size \\(n\\times p\\), \\(\\mathbf{y}\\), \\(\\epsilon&gt;0\\), \\(\\Delta\\) 2. Outputs: \\(\\hat{\\boldsymbol{\\beta}}\\) 3. Initial steps: - standardize all covariates \\(\\rightarrow \\mathbf{\\tilde{x}}_{j},j=1,\\ldots,p\\) - Set \\(\\hat{\\boldsymbol{\\beta}}^{(0)}=\\mathbf{0}\\) - Set \\(\\mathbf{e}_0=\\mathbf{y}-\\bar{y}\\), \\(\\bar{y}=1/n\\sum_{i=1}^n y_i\\) 4. For \\(k =1, 2, \\ldots\\), do: - \\(j=\\arg\\max_{j}|\\mathbf{\\tilde{x}}_j^T\\mathbf{e}_{k-1}|\\), - if \\(|\\mathbf{\\tilde{x}}_j^T\\mathbf{e}_{k-1}|\\leq \\Delta\\) then stop - \\(\\delta_j=\\epsilon\\;\\mbox{sign}\\left(\\mathbf{\\tilde{x}}_j^T\\mathbf{e}_{k-1}\\right)\\) - Set \\(\\hat{\\beta}_l^{(k+1)}\\leftarrow \\beta_l^{(k)}+\\delta_jI(l=j), l=1,\\ldots,p\\) - Set \\(\\mathbf{e}_k\\leftarrow \\mathbf{e}_{k-1}-\\delta_j\\mathbf{\\tilde{x}}_j\\) There is no clear rule for setting \\(\\epsilon\\) and \\(\\Delta\\), which makes the forward stagewise regression algorithm more an exploratory method. Obviously, \\(\\Delta\\) should be small (e.g. \\(0.0001\\)), while the smaller \\(\\epsilon\\), the more steps are needed to reach the end of the algorithm. There is actually a strong connection between the sequence of forward stagewise estimates and the solution path of the lasso; see Efron et al. (2004), and also Hastie et al. (2007), Tibshirani (2015). Exercise (optional): - Show that an ordering procedure based on stagewise regression is biased and calculate this bias for the OLS coefficient of the \\(x_j\\) predictor. Hint: Use the proof of Proposition I of Lin, Foster, and Ungar (2011). 3.3 Streamwise regression 3.3.1 Introduction Stepwise forward selection is a procedure that finds a good approximation of the the model (formed by a subset of the available covariates) that minimizes a chosen criterion (e.g. the \\(C_p\\)). It involves however, at each step \\(k\\), the comparison of \\(p-k\\) models which can be computationally very challenging. Streamwise regression (see e.g. Zhou et al. (2006) and the references therein), is a variation of stepwise regression in which covariates (predictive features) are tested sequentially for addition to the current model (or not). Because it considers each potential covariate only once, it is extremely fast. The question that arises then is, if stepwise regression is already an approximation to the optimal model in terms of model validity, how can streamwise regression, which is by construction less accurate (in terms of approximating the optimum) still be valid in some sense. There is trade-off between usable or workable procedures and reaching an optimum in high dimensions (where any, even reasonable, procedure can fail). This discussion, still widely open, goes beyond the scope of the focus of this eBook. Streamwise regression can be bound to hypothesis testing, in that the focus is placed on selecting the largest set of covariates so that their significance (rejecting their associated null hypothesis) is valid up to a global risk. These procedures mimic somehow the situation in which a global model could be estimated (not possible, so far, when \\(n&lt;p\\)) and the useful covariates, in terms of model validity, could be identified as the ones for which \\(\\mathbb{H}_{0j}=0\\) can be rejected. Care needs to be taken about not only the FWER (bound to multiple hypothesis testing) and the fact that any theoretical result (bound to the hypothesis to be tested) needs to be implemented using estimated quantities from the sample at hand, hence introducing bias and overfitting. 3.3.2 Sure Independence Screening Fan and Lv (2008) introduce the concept of sure screening and propose a sure screening method based on correlation learning which filters out the features that have weak correlation with the response. The resulting method is called sure independence screening (SIS). Sure screening is a property that insures all the important variables survive after variable screening with probability tending to 1. An important feature in high dimensions is that spurious correlations (sample correlations) between predictors can be very large. This implies that using the (sample) correlations (partial or marginal) induces a form of decisional bias because the sample (observed) quantities are misleading. For example, important predictors can be found to be highly correlated with some unimportant ones, which then makes selection tend to include more predictors than necessary. Fan and Lv (2008) provide a simulations study in which \\(p\\) independent standard normal predictors are simulated and the maximum absolute sample correlation coefficient between predictors is considered. In the Figure below are represented these quantities computed on \\(500\\) datasets with \\(n=60\\) and \\(p=1000\\) and \\(p=5000\\). Reported from Fan and Lv (2008) In the SIS framework, all covariates \\(X_j,j=1,\\ldots,p\\) are standardized (mean 0, variance 1). It is also assumed that the true model is sparse, i.e. the true dimension \\(s\\) is such that \\(s&lt;&lt;p\\). Let \\(\\mathcal{M}_s = \\{j=1,\\ldots,p \\; \\vert \\; \\beta_j \\neq 0 \\}\\) be the true sparse model with non-sparsity size \\(s=\\vert \\mathcal{M}_s\\vert\\). Let also \\(\\mathcal{M}_\\gamma\\) denote a model selected on the basis of a (streamwise) selection procedure, the sure screening property ensures that \\[\\begin{equation} P\\left(\\mathcal{M}_s\\ \\subseteq \\mathcal{M}_\\gamma\\right)\\rightarrow 1 \\end{equation}\\] The SIS procedure proposed by Fan and Lv (2008) ranks the importance of covariates (features) according to their marginal correlation with the response variable and filters out those that have weak marginal correlations with the response variable. Let \\[\\begin{equation} \\boldsymbol{\\omega}=(\\omega_1,\\ldots,\\omega_p) = \\mathbf{X}^T\\mathbf{y} \\end{equation}\\] hence, the vector of marginal correlations of predictors with the response variable (rescaled by the standard deviation of the response \\(\\mathbf{y}\\)). For any given \\(\\gamma\\in (0,1)\\), the SIS selected model is \\[\\begin{equation} \\mathcal{M}_\\gamma=\\{j=1,\\ldots,p \\;\\vert \\; \\vert \\omega_j\\;\\vert \\; \\mbox{is among the first} \\lfloor \\gamma n \\rfloor \\; \\mbox{largest}\\} \\end{equation}\\] where \\(\\lfloor \\gamma n \\rfloor\\) denotes the integer part of \\(\\gamma n\\). The SIS is actually used to shrink the full model of size \\(p\\) down to a submodel \\(\\mathcal{M}_\\gamma\\) with size \\(d =\\lfloor \\gamma n \\rfloor&lt;n\\). There is a link between SIS and ridge regression (Tikhonov regularization), a shrinkage method that will be presented in Chapter 4. Fan and Lv (2008) set up the conditions (Theorem 1) for the SIS to have the sure screening property, for suitable (technical) choices of \\(\\gamma\\). In particular, \\(\\boldsymbol{\\Sigma}^{^1/2}\\mathbf{X}, \\boldsymbol{\\Sigma}=\\mbox{cov}\\left(\\mathbf{X}\\right), \\mathbf{X}=\\left[X_j\\right]_{j=1,\\ldots,p}\\) has a spherically symmetric distribution. The SIS procedure can be combined with another (subset) selection method: apply the SIS to reduce the dimension, apply a selection method to the reduced dimension to get, say, \\(\\mathcal{M}_{k_1}\\) of size of order e.g. \\(n/log(n)\\). The residuals from a regression of \\(Y\\) on \\(\\mathbf{X}=[X_j],j\\in \\mathcal{M}_{k_1}\\) are then used as a new response vector and the SIS is applied to the left-aside \\(p-k_1\\) covariates to get \\(\\mathcal{M}_{k_2}\\). The rationale behind is that using the residuals from the previous step, amounts to considering partial correlations which set aside the unimportant covariates that are highly correlated with the response through their associations with the covariates present in the set \\(\\mathcal{M}_{k_1}\\) and also makes those important predictors that are missed in the previous step possible to survive. The procedure is iterated until the \\(L\\) disjoint subsets \\(\\mathcal{M}_{k_l},l=1,\\ldots,L\\) whose union \\(\\mathcal{M}_d\\) has maximal size satisfying e.g. \\(\\vert \\mathcal{M}_d\\vert&lt;n\\). This procedure is called the iterative sure independence screening (ISIS). Exercise (solutions presented in 7.3.2): After having read the documentation of the R package SIS and installed it, perform the following steps: a) Load the Leukemia dataset. b) Split the dataset randomly and create a train and test sample. c) The functions SIS() performs first a screening procedure based on marginal correlations and then applies a penalized method (Chapter 4 of the e-book) to obtain the final model. Choose among all the available options (i.e. in terms both of penalized methods and tuning constants) three candidates and evaluate the predictions of the selected models on the test sample. Which penalized method performs best in this specific example after the SIS? (optional) Explain why stepwise regression based on partial correlations (for the inclusion criterion) is a special case of ISIS. 3.3.3 PC-simple algorithm Bühlmann, Kalisch, and Maathuis (2010) use partial correlations to stream through the covariates and develop an algorithm that is a simplified version of the PC algorithm (Spirtes, Glymour, and Scheines 2000) used in Bayesian network modeling (Causal Bayesian Network). Moreover, they provide a criterion based on hypothesis testing to decide if a covariate is selected or not. The PC-simple algorithm is given by the following steps: 1. Define the active set at step m as \\(\\mathcal{M}_m\\) and set m = 1. 2. Do correlation screening, and build the active set \\(\\mathcal{M}_1 = \\{j=1,\\ldots,p \\; \\vert \\; \\rho_{X_jY}\\neq 0 \\}\\). 3. m = m + 1. 4. Construct the \\(m\\) active set \\(\\mathcal{M}_m =\\{j \\in \\mathcal{M}_{m-1}\\; \\vert \\;\\rho_{X_jY,\\mathbf{X}_{C}}\\neq 0, \\; \\mathbf{X}_{C}=[X_l],l\\in\\mathcal{M}_{m-1}\\setminus\\{j\\}\\}\\). 5. Repeat from step 3 as long as \\(\\vert \\mathcal{M}_m\\vert \\leq m\\). 6. Output: \\(m_S=\\{m\\;\\vert\\; \\vert \\mathcal{M}_m\\vert \\leq m\\}\\). The PC-simple algorithm hence starts (i.e. m=1) with a streamwise regression based on marginal correlations which are a particular case of partial correlations with empty conditioning set. In the next step (i.e. m=2) all the possible first order partial correlation, of the variables present in \\(\\mathcal{M}_1\\), are evaluated to exclude all the covariates for with at least one first order partial correlation that is equal to zero. Then a new active set is obtained such that \\(\\mathcal{M}_2\\subseteq \\mathcal{M}_1\\). Thus, at a given step m, there is an evaluation of the (m-1)-th order partial correlations, of the variables present in \\(\\mathcal{M}_{m-1}\\), to reduce the active set to \\(\\mathcal{M}_m\\subseteq \\mathcal{M}_{m-1}\\). The procedure stops when an equilibrium is reached, namely when the number of covariates in the active set at step \\(m\\) is the closest (and inferior) to \\(m\\). Since partial correlations need to be estimated from the available data, they are subject to sampling error, hence even if \\(\\rho_{\\cdot\\;\\cdot,\\cdot}= 0\\), \\(\\hat{\\rho}_{\\cdot\\;\\cdot,\\cdot}\\neq 0\\). Bühlmann, Kalisch, and Maathuis (2010) propose to apply Fisher’s \\(Z\\)-transformation (Fisher 1915) \\[\\begin{equation} Z_{X_jY,\\mathbf{X}_C}=\\frac{1}{2}\\log\\left(\\frac{1+\\hat{\\rho}_{X_jY,\\mathbf{X}_{C}}}{1-\\hat{\\rho}_{X_jY,\\mathbf{X}_{C}}}\\right) \\end{equation}\\] When testing (two-sided) \\(\\mathbb{H}_0:\\rho_{X_jY,\\mathbf{X}_{C}}= 0\\), \\(\\mathbb{H}_0\\) is rejected when (Fisher 1921) \\(\\sqrt{n-(m-1) -3} \\vert Z_{X_jY,\\mathbf{X}_C}\\vert &gt; \\Phi^{-1}\\left(1-\\alpha/2\\right)\\), where \\(\\alpha\\) is the significance level, \\(\\Phi\\) is the standard Normal cumulative distribution function and m the current step of the procedure. Partial correlations can be calculated recursively (Bühlmann, Kalisch, and Maathuis 2010) using \\[\\begin{equation} \\hat{\\rho}_{X_jY,\\mathbf{X}_{C}}=\\frac{\\hat{\\rho}_{X_jY,\\mathbf{X}_{C\\setminus\\{k\\}}}-\\hat{\\rho}_{X_kY,\\mathbf{X}_{C\\setminus\\{k\\}}}\\hat{\\rho}_{X_jX_k,\\mathbf{X}_{C\\setminus\\{k\\}}}}{\\sqrt{\\left(1-\\hat{\\rho}_{X_kY,\\mathbf{X}_{C\\setminus\\{k\\}}}^2\\right)\\left(1-\\hat{\\rho}_{X_jX_k,\\mathbf{X}_{C\\setminus\\{k\\}}}^2\\right)}} \\end{equation}\\] To assess that the (population version) of the PC-simple algorithm identifies the true underlying active set, i.e. \\(\\mathcal{M}_{m_S} = \\{j = 1,\\ldots,p \\; \\vert\\; \\beta_j\\neq 0 \\}\\), Bühlmann, Kalisch, and Maathuis (2010) make use of the partial faithfulness concept. It applies to models that are such that if \\(\\forall j\\) (with \\(j=1,\\ldots,p\\)) \\(\\rho_{X_jY,\\mathbf{X}_{C}}= 0\\), for some \\(\\mathcal{M}_C\\subseteq \\mathcal{M}_p\\setminus \\{j\\}\\), implies that \\(\\beta_j=0\\) (with \\(\\mathcal{M}_p=\\{j=1,\\ldots,p\\}\\)). Partial faithfulness can only be verified when assuming a (joint) distribution for the covariates. Exercise (solutions presented in 7.3.3): First of all build a simulation setting as explained below: - Generate from a MVN (multivariate normal) a matrix \\(\\mathbf{X_{n*p}}\\) with \\(n = 1000\\) and \\(p = 10\\). Choose the location but set the scale matrix with an autoregressive form \\(\\boldsymbol{\\Sigma}=[\\sigma_{lm}]_{l,m=1,\\ldots,p}\\) with \\(\\sigma_{lm} = \\rho^{\\mid l - m\\mid}\\). - Fix \\(\\rho = 0.5\\) and set the seed equal to 11 (i.e. set.seed(11)). - Choose the generating vector \\(\\boldsymbol{\\beta }= [3 \\; 1.5 \\; 0 \\; 2 \\; rep(0,6)]\\). - Generate \\(\\mathbf{\\hat{y}}\\) thanks to the relation \\(\\mathbf{y} = \\mathbf{X_{n*p}} \\; \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) where \\(\\epsilon_{i}\\) is a standard normal. Suppose for simplicity that the errors are uncorrelated. Now perform the following passages on your simulated data: a) Find the active set \\(M_{1}\\) using the Fisher’s Z transformation and the associated correlation coefficient test (fix \\(\\alpha = 0.05\\) for the rest of the exercise). b) Calculate all the partial correlations of order 1 (i.e. one variable at the time) of the active set \\(M_{1}\\), test them and retrieve \\(M_{2} \\subseteq M_1\\) which is the new active set. c) Find the partial correlations of higher order and test them until your reach the condition \\(M_{m-1} = M_{m}\\) which implies the convergence of the PC-simple algorithm. Do you obtain the exact model? Exercise (project): - Write the PC-simple algorithm in R using the recursive formula for partial correlations. - Perform a simulation study to evaluate the performance (model selection criteria) both when the covariates are normally distributed and when they are not (consider e.g. factors, i.e. dummies). - Study the sensitivity of the PC-simple as a function of \\(\\alpha\\). - Apply the algorithm for variable selection in a chosen dataset. - Compare with another variable selection method (implemented). 3.4 Classification And Regression Tree (CART) Note: The Section is highly inspired from Hastie, Tibshirani, and Friedman (2009), chapter 9. The regression model involves estimating the conditional mean of the response variable \\(Y\\) given a set of predictors \\(X_j, j=1,\\ldots,p\\) (collected in the \\(n\\times p\\) design matrix \\(\\mathbf{X}\\)), i.e. \\(\\mu(\\mathbf{X})=\\mathbf{X}\\beta, \\dim(\\beta)=p\\). The parameters \\(\\beta\\) represent the slopes of the linear regression of \\(Y\\) on \\(\\mathbf{X}\\). The basis of CART (Breiman 1984) is to approximate these slopes by partitioning the observed predictors realizations into consecutive sets, for which the observed mean response is computed. More precisely, tree-based methods partition the predictors’ space (feature space) into a set of rectangles, and then fit a simple model (i.e. computes the mean) in each one. The partitioning is done sequentially, with usually a binary partitioning (i.e. in two consecutive parts), one predictor at the time. The Figure below shows such a sequence in the case of two predictors \\(X_1, X_2\\), with splitting values \\(\\theta_m\\) and (split) regions \\(R_m, m=1,\\ldots,M, M=5\\). Illustration of the recursive binary partitioning A regression tree algorithm has the following characteristics that need to be set: - The choice of the splitting variable \\(X_j\\) (from which the split is done)in each region) - criterion for the splitting value \\(t_m:=\\theta_m\\) - the model (predictor) that is fitted in each region \\(R_m\\) - the size of the tree \\(M\\) 3.4.1 Regression tree In the linear regression model, a constant model is fitted in each region \\(R_m\\), i.e. the mth stage of the algorithm is \\[\\begin{equation} \\hat{\\mu}(\\mathbf{X})= \\sum_{m=1}^M c_m(y_i\\vert \\mathbf{x}_i\\in R_m) \\end{equation}\\] with \\[\\begin{equation} c_m(y_i\\vert \\mathbf{x}_i\\in R_m)=\\frac{1}{n_m}\\sum_i^n y_i I(\\mathbf{x}_i\\in R_m), \\\\ n_m=\\sum_i^n I(\\mathbf{x}_i\\in R_m) \\end{equation}\\] This amounts to consider, as predictor for the response, the sum of \\(m\\) local predictors computed as the mean responses \\(c_m(y_i\\vert \\mathbf{x}_i\\in R_m)\\) in each region \\(R_m\\). The slopes \\(\\beta_j\\) are then never estimated (even locally) which results in a important gain in computational time. At stage of the algorithm, two new regions are created, namely one of the former region is split into two parts. This is done for each region. A choice for the covariate \\(X_j\\) to consider for the spiting as well as the splitting value \\(t_{mj}\\) needs to be done, and the criterion is the minimization of the residual sum of squares (RSS) \\[\\begin{equation} \\sum_{k=1}^m\\sum_{i \\in R_k}\\left(y_i-c_k(y_i\\vert \\mathbf{x}_i\\in R_k)\\right)^2 \\end{equation}\\] Since the contributions to the RSS of each region are computed separately (the \\(c_k(y_i\\vert \\mathbf{x}_i\\in R_k)\\)), the search for an optimum can be conducted region by region, in which both the splitting variable \\(X_j\\) and value \\(t_{mj}\\) are found that minimize the RSS in a given region. More precisely, let, at stage \\(m\\), \\(R_{m_1}(j,t_{mj})\\) and \\(R_{m_2}(j,t_{mj})\\) the regions resulting from the split of one region on the basis of one of the covariates \\(X_j\\) and according to a splitting scalar \\(t_{mj}\\), then \\(j\\) and \\(t_{mj}\\) are found using \\[\\begin{eqnarray} \\min_{j,t_{mj}}&amp; \\left\\{\\sum_{i \\in R_{m_1}(j,t_{mj})} \\left(y_i-c_m(y_i\\vert \\mathbf{x}_i\\in R_{m_1}(j,t_{mj}))\\right)^2 \\right. \\\\ &amp; \\left. + \\sum_{i \\in R_{m_2}(j,t_{mj})} \\left(y_i-c_m(y_i\\vert \\mathbf{x}_i \\in R_{m_2}(j,t_{mj}))\\right)^2\\right\\} \\end{eqnarray}\\] For the size of the tree, one approach is to grow the tree until each region is very small, say of size \\(n_m\\approx 5\\), and this large tree, say \\(T_0\\), is then prunned using cost-complexity pruning based on a criterion \\(C_{\\alpha}(T)\\). Pruning generates a series of trees \\(T_{k}\\subset T_{k-1}\\subset \\ldots\\subset T_{1}\\subset T_0\\), obtained (sequentially) removing a subtree from the previous tree. A subtree is removed by removing a splitting (and the following ones) to create a collapsed region, also called leaf. The choice for the subtree to split, hence the resulting tree, say \\(T\\) of size \\(\\vert T\\vert\\), is done using a criterion based on the RSS, i.e. \\[\\begin{equation} C(T)=\\sum_{m=1}^{\\vert T\\vert}\\sum_{i\\in R_m(T)}\\left(y_i-c_m(T)\\right)^2 \\end{equation}\\] where \\(R_m(T)\\) is the \\(m\\)th region (leaf) of tree \\(T\\) and \\(c_m(T)=\\frac{1}{n_m}\\sum_i^n y_i I(\\mathbf{x}_i\\in R_m(T))\\) is the average response in region \\(R_m(T)\\). \\(C(T)\\) needs however to be constrained to avoid overfitting (since the largest possible tree is always the one that minimizes \\(C(T)\\)), so that one uses a penalized criterion given by (Hastie, Tibshirani, and Friedman 2009) \\[\\begin{equation} C_{\\alpha}(T)=C(T) + \\alpha \\vert T\\vert \\end{equation}\\] Hence, the idea is to find, for each \\(\\alpha_k&gt;\\alpha_{k-1}&gt;\\ldots &gt; \\alpha_1&gt; \\alpha_0=0\\), the (unique) subtree \\(T_{\\alpha_k} \\subset T_{\\alpha_{k-1}}\\) that minimizes \\(C_{\\alpha}(T)\\). The tuning parameters \\(\\alpha_k\\) govern the trade off between tree size (to avoid overfitting) and its goodness of fit to the data. The sequence of \\(\\alpha_k\\) can be constructed starting from \\(\\alpha_1\\), and the sequence stops when a criterion such as tenfold cross-validation is optimized. One major problem with trees is their high variance. Often a small change in the data can result in a very different series of splits, making interpretation somewhat precarious. Averaging methods such as bagging, i.e. bootstrap aggregating, can be used to reduce the variability of the tree estimator (Breiman 1996). See Sutton (2005) for a survey. Exercise (solutions presented in 7.3.4): After having read the documentation of the rpart package and having installed it, perform the following steps: Load the Zambia dataset, split it randomly in a train and test sample (common choice is \\(\\frac{2}{3}\\) train and \\(\\frac{1}{3}\\) test). For simplicity, you can consider only the continuous variables. Fit a regression tree with the function rpart() and plot the tree. Have a look at rpart.plot package if you want to improve the appearance of the fitted tree. After having pruned the tree, evaluate its prediction on the test sample (i.e. use predict() on a tree object. 3.4.2 Classification Trees Like for subset selection, if the target is a classification outcome taking values \\(q=1, 2,\\ldots,Q\\), the criterion for splitting (growing) and pruning the tree, , i.e. the RSS, needs to be adapted. Let \\(p_{mq}=1/n_m\\sum_{\\mathbf{x}_i\\in R_m}I(y_i= q)\\) be the proportion of class \\(q\\) observations in region \\(R_m\\) and let \\(q_m=\\mbox{argmax}_q p_{mq}\\), i.e. the most frequent class in region \\(R_m\\). Different alternatives to the RSS (i.e. risk measures) exist and include the following ones, specified for the region \\(R_m\\): - Misclassification error: \\(\\mbox{err}_m=\\frac{1}{n_m}\\sum_{i\\in R_m}I(y_i\\neq q_m)=1-p_{mq_m}\\) - Gini index: \\(\\mbox{err}_m=\\sum_{q=1}^Qp_{mq}(1-p_{mq})= \\sum_{q=1}^Q\\sum_{q&#39;\\neq q}p_{mq}p_{mq&#39;}\\) - Cross-entropy or deviance: \\(\\mbox{err}_m=- \\sum_{q=1}^Qp_{mq}\\log(p_{mq})\\) The total risk or error is obtained as \\(\\sum_{m=1}^R\\mbox{err}_m\\). The deviance and the Gini index have the advantage of being differentiable functions, and hence more amenable to numerical optimization. Exercise (solutions presented in 7.3.5): - After having read the documentation of the rpart package and having installed it, perform the following steps: a) Load the Iris dataset already present in R, split it randomly in a train and test sample (common choice is \\(\\frac{2}{3}\\) train and \\(\\frac{1}{3}\\) test). Fit a classification tree with the function rpart() and plot the tree. Have a look at rpart.plot package if you want to improve the appearance of the fitted tree. After having pruned the tree, evaluate its prediction on the test sample (i.e. use predict() on a tree object). (Optional) Analyse the Leukemia dataset by means of the regression tree method. Use the rpart package in R (see also https://www.statmethods.net/advstats/cart.html). References "],
["shrinkage-methods.html", "4 Shrinkage Methods 4.1 Introduction 4.2 Ridge regression 4.3 The lasso estimator 4.4 Alternative penalized regression methods", " 4 Shrinkage Methods 4.1 Introduction Shrinkage methods denote estimation methods under constraints that shrink estimators toward a given value. The origin goes back to the James–Stein estimator (James and Stein 1961). The rationale is that one can sacrifice bias for variance, i.e. optimize in some sense the Mean Squared Error (MSE), which, for an estimator \\(\\hat{\\boldsymbol{\\beta}}\\) is \\[\\begin{equation} \\mbox{MSE}\\left(\\hat{\\boldsymbol{\\beta}}\\right)=\\mathbb{E}\\left[\\left\\Vert\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}\\right\\Vert^2\\right]=\\mathbb{E}\\left[\\left(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}\\right)^T\\left(\\hat{\\boldsymbol{\\beta}}-\\boldsymbol{\\beta}\\right)\\right] \\end{equation}\\] (see Efron and Hastie (2016), Section 7.1, for details on the James–Stein estimator) For the least squares (LS) estimator of the linear model \\(\\mathbf{y}=\\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\\), \\(\\boldsymbol{\\varepsilon}\\sim N\\left(\\mathbf{0},\\sigma^2\\mathbf{I}\\right)\\), and supposing without loss of generality that \\(\\mathbf{y}\\) is centered and the covariates (columns in \\(\\mathbf{X}\\)) are standardized, we have \\(\\mbox{MSE}\\left(\\hat{\\boldsymbol{\\beta}}_{LS}\\right)=\\sigma^2\\mbox{tr}\\left[\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\right]\\), which obviously grows with the dimension \\(p\\). In terms of out-of-sample prediction error (PE), we have the following result \\[\\begin{eqnarray} \\mbox{PE}\\left(\\mathbf{x}_0,\\hat{\\boldsymbol{\\beta}}\\right) &amp;=&amp;\\sigma^2+\\mathbb{E}_{Y\\vert\\mathbf{X}=\\mathbf{x}_0}\\left[\\left(Y-\\mathbf{x}_0\\hat{\\boldsymbol{\\beta}}\\right)\\right]^2 +\\mathbb{E}_{Y\\vert \\mathbf{X}=\\mathbf{x}_0}\\left[\\left(Y-\\mathbf{x}_0\\hat{\\boldsymbol{\\beta}}\\right)^2\\right] \\\\ &amp;=&amp; \\sigma^2+\\mbox{Bias}^2\\left(\\mathbf{x}_0,\\hat{\\boldsymbol{\\beta}}\\right)+ \\mbox{var}\\left(\\mathbf{x}_0,\\hat{\\boldsymbol{\\beta}}\\right) \\end{eqnarray}\\] As model becomes more complex (more terms included), on one hand, local structure/curvature can be picked up, but on the other hand, coefficient estimates suffer from high variance as more terms are included in the model (see overfitting phenomenon). Reported from http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf Hence, introducing a little bias in the estimators might lead to a substantial decrease in variance, and hence to a substantial decrease in PE. The idea is then not to let the \\(\\hat{\\beta}_j\\) take too large values in order to avoid too large variances. To control the variance, the slope coefficients are regularized in that a constraint is added to the optimization problem (e.g. LS) that controls how large the coefficients can be. 4.2 Ridge regression The first proposed regularization method is the ridge constraint (or Tikhonov regularization https://en.wikipedia.org/wiki/Tikhonov_regularization), which amounts to set the constraint \\(\\sum_{i=1}^p\\beta_j^2&lt;t_{\\lambda}\\). In terms of optimization, for a given value of \\(\\lambda\\), the ridge estimator \\(\\hat{\\boldsymbol{\\beta}}(\\lambda)\\) is given by \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}(\\lambda)=\\mbox{argmin}_{\\boldsymbol{\\beta}}\\left\\Vert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\right\\Vert^2-\\lambda\\Vert\\boldsymbol{\\beta}\\Vert^2 \\end{equation}\\] The solution is \\[\\begin{eqnarray} \\hat{\\boldsymbol{\\beta}}(\\lambda)&amp;=&amp;\\left(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}\\\\ &amp;=&amp; \\left(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I}\\right)^{-1}\\left(\\mathbf{X}^T\\mathbf{X}\\right)\\hat{\\boldsymbol{\\beta}}_{LS} \\end{eqnarray}\\] Hence, the variance of \\(\\hat{\\boldsymbol{\\beta}}_{LS}\\) is shrunk by an amount of \\(\\left(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I}\\right)^{-1}\\left(\\mathbf{X}^T\\mathbf{X}\\right)\\) which depends on \\(\\lambda\\). With \\(\\lambda=0\\), there is no shrinkage and \\(\\hat{\\boldsymbol{\\beta}}(0)=\\hat{\\boldsymbol{\\beta}}_{LS}\\), while when \\(\\lambda\\rightarrow\\infty\\), \\(\\hat{\\boldsymbol{\\beta}}(\\lambda)\\rightarrow\\mbox{0}\\), the maximal shinkage. Reported from Efron and Hastie (2016), Section 7.2: Ridge regression applied to the diabetes data Ridge estimators have lower variance the the LS, but this does not guarantee that the corresponding predictions \\(\\hat{\\boldsymbol{\\mu}}\\left(\\lambda\\right)=\\mathbf{X}\\hat{\\boldsymbol{\\beta}}(\\lambda)\\) are more accurate than the ones obtained by the LS \\(\\hat{\\boldsymbol{\\mu}}(0)=\\mathbf{X}\\hat{\\boldsymbol{\\beta}}(0)\\), since \\(\\hat{\\boldsymbol{\\beta}}(\\lambda)\\) is biased towards \\(\\mathbf{0}\\). However, in high dimensions, the prior belief is that most of the \\(\\beta_j\\) lie near zero, and biasing the LS estimator toward zero then becomes a necessity. Another notable advantage of the ridge estimator is that the inclusion of \\(\\lambda\\) makes the optimization problem solvable even if \\(\\mathbf{X}^T\\mathbf{X}\\) is not invertible (e.g. \\(\\mathbf{X}\\) is not of full rank when \\(p&gt;n\\)). This was the original motivation for ridge regression as proposed by Hoerl and Kennard (1970) who also recommended to select \\(\\lambda\\) graphically. Standard practice now is to use (10-fold) cross-validation. Exercise (solutions presented in 7.4.1): - Use the function glmnet() to perform a Ridge regression on Zambia dataset, plot the values as a function of \\(\\lambda\\) and comment on the results. - Use the function cv.glmnet() to perform model selection based on 10-fold Cross Validation (i.e. method to select the \\(\\lambda\\) parameter), plot the results and comment the graph that you obtain. Which values of \\(\\lambda\\) are shown by default? - Use the function predict() to retrieve the final model estimates and perform a simple linear model on the same covariates, what can you conclude? 4.3 The lasso estimator Instead of regularizing the estimator using \\(\\sum_{i=1}^p\\beta_j^2=\\Vert\\boldsymbol{\\beta}\\Vert^2&lt;t_{\\lambda}\\), one could use other \\(l_q\\)-norms, i.e. \\(\\Vert\\boldsymbol{\\beta}\\Vert_{q}=\\sum_{i=1}^p\\vert\\beta_j\\vert^{q}&lt;t_{\\lambda}\\). In particular, when \\(q=0\\), we have \\(\\Vert\\boldsymbol{\\beta}\\Vert_{0}=\\sum_{i=1}^p\\vert\\beta_j\\vert^{0}&lt;t_{\\lambda}\\) with, by definition, \\(\\sum_{i=1}^p\\vert\\beta_j\\vert^{0}=\\sum_{i=1}^pI\\left(\\beta_j\\neq 0\\right)\\), the number of slope coefficients different from \\(0\\), and hence corresponds to subset selection. Tibshirani (1996) proposed to use \\(q=1\\) leading to the very famous lasso estimator. The great advantage of the lasso, on top of the fact that the resulting optimization problem \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}(\\lambda)=\\mbox{argmin}_{\\boldsymbol{\\beta}}\\left\\Vert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\right\\Vert^2-\\lambda\\Vert\\boldsymbol{\\beta}\\Vert_1 \\end{equation}\\] is convex (since both the loss \\(\\left\\Vert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\right\\Vert^2\\) and the constraint \\(\\Vert\\boldsymbol{\\beta}\\Vert_1=\\sum_{j=1}^p\\vert\\beta_j\\vert\\) are convex in \\(\\boldsymbol{\\beta}\\)), is that the shrinking forces a set of \\(\\beta_j\\) to exactly \\(0\\). Hence, the lasso estimator provides simultaneously a regularized estimator and model (subset) selection. Reported from Efron and Hastie (2016), Section 16.2 As for the ridge estimator, \\(\\lambda\\) needs to be chosen and, for model selection purposes (or out-of-sample prediction error optimization), a criteria that assesses the model validity can be used. The most comon choices are the \\(C_p\\) and 10-fold CV. Practically, for well chosen values of \\(\\lambda\\), \\(\\hat{\\boldsymbol{\\beta}}(\\lambda)\\) and the corresponding model validity criterion are computed. \\(\\lambda\\) is chosen as to optimize (minimize) the later. To compute the entire lasso path (i.e. for all values of \\(\\lambda\\) that add one \\(\\beta_j\\neq 0\\) at each step), Efron et al. (2004) propose the Least Angle Regression algorithm (LARS), which is based on correlations between the covariates to enter and the residuals from the previous fit. LARS is introduced as a more democratic version of forward stepwise regression. Indeed, while forward stepwise regression builds a model sequentially, adding one variable at a time and updating the LS estimator to include all the active variables, the LARS only enters as much of a covariate as it deserves. At the first step it identifies the variable most correlated with the response, and moves the coefficient of this variable continuously toward (not fully) its LS estimate. The residuals are updated and this causes the correlations with other variables to evolve. As soon as another variable reaches the same correlation with the residual as the ones in the active set, this variable then joins the active set. Reported from Hastie, Tibshirani, and Friedman (2009), Section 3.4. Note that \\(\\langle \\mbox{x}_j,\\mathbf{r}\\rangle= \\mbox{x}_j^T\\mathbf{r}\\). In step 3 and 4, the values of the slopes in the active set, say \\(\\mathcal{A}\\), are augmented as \\(\\boldsymbol{\\beta}_{\\mathcal{A}}(\\epsilon)=\\boldsymbol{\\beta}_{\\mathcal{A}}+\\epsilon\\cdot\\delta_{\\mathcal{A}}\\) for small values of \\(\\epsilon\\), with \\(\\delta_{\\mathcal{A}}=\\left(\\mathbf{X}^T_{\\mathcal{A}}\\mathbf{X}_{\\mathcal{A}}\\right)^{-1}\\mathbf{X}^T_{\\mathcal{A}}\\mathbf{r}_{\\mathcal{A}}\\), \\(\\mathbf{r}_{\\mathcal{A}}=\\mathbf{y}-\\mathbf{X}_{\\mathcal{A}}\\boldsymbol{\\beta}_{\\mathcal{A}}\\). The LARS bears very close similarities with forward stagewise regression (see Hastie et al. (2007), Tibshirani (2015) and Efron and Hastie (2016), Section 16.4). Hence, if the covariates are correlated, the lasso path might not produce the suitable model selection path (risk of overfitting). Exercise (solutions presented in 7.4.2): - Use the function glmnet() to perform a lasso on Zambia dataset, plot the values as a function of \\(\\lambda\\) and comment on the results. - Use the function cv.glmnet() to perform model selection based on 10-fold Cross Validation (i.e. method to select the \\(\\lambda\\) parameter), plot the results and comment the graph that you obtain. Which values of \\(\\lambda\\) are shown by default? What can you conclude on the choice of \\(\\lambda\\) in terms of model selection? - Use the function predict() to retrieve the final model chosen by 10-fold CV (given lasso ordering) and perform a linear model on the covariates present in the final model. What can you conclude observing the estimates? 4.4 Alternative penalized regression methods 4.4.1 The adaptive and relaxed lasso Because the lasso finds a suitable compromise between estimation bias and variance, the price to pay is that it generally provides (asymptotically) biased slope estimators. It can be shown that the bias is larger for (true) large slope coefficients (in absolute value). Ideally then, one should consider, instead of the lasso penalty \\(\\lambda\\Vert\\boldsymbol{\\beta}\\Vert_1\\), more generally \\(\\sum_{j=1}^p\\lambda_j\\vert\\beta_j\\vert\\). Zou (2006) propose the adaptive lasso with \\(\\lambda_j=\\lambda w_j^{\\gamma}\\) and \\[\\begin{equation} w_j^{\\gamma}=\\left\\vert \\hat{\\beta}_{j}^{(LS)}\\right\\vert^{-\\gamma}, \\; \\gamma&gt;0 \\end{equation}\\] Hence, \\(w_j\\) assigns smaller weights to larger slope coefficients, and when \\(\\hat{\\beta}_{j}^{(LS)}\\rightarrow 0\\), \\(w_j \\rightarrow \\infty\\) so that the adaptive lasso estimate \\(\\hat{\\beta}_j \\rightarrow 0\\). Actually, \\(\\hat{\\boldsymbol{\\beta}}^{(LS)}\\) can be replaced by any (\\(\\sqrt{n}\\)) consistent estimator of \\(\\boldsymbol{\\beta}\\). In practice, when \\(p&gt;n\\), the ridge estimator is a suitable candidate. The corresponding optimization problem is \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}(\\lambda,\\gamma)=\\mbox{argmin}_{\\boldsymbol{\\beta}}\\left\\Vert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\right\\Vert^2-\\lambda\\sum_{j=1}^pw_j^{\\gamma}\\vert\\beta_j\\vert \\end{equation}\\] and remains a convex optimization problem. Actually, the LARS algorithm can be used to obtain the adaptive losso path, by applying it to the scaled covariates \\(\\tilde{\\mathbf{x}}_j=\\mathbf{x}_j/w_j^{\\gamma}\\). Namely, let \\(\\tilde{\\mathbf{X}}=\\left[\\tilde{\\mathbf{x}}_j\\right]_{j=1,\\ldots,p}\\) and \\[\\begin{equation} \\tilde{\\boldsymbol{\\beta}}(\\lambda)=\\mbox{argmin}_{\\boldsymbol{\\beta}}\\left\\Vert\\mathbf{y}-\\tilde{\\mathbf{X}}\\boldsymbol{\\beta}\\right\\Vert^2-\\lambda\\Vert\\boldsymbol{\\beta}\\Vert_1 \\end{equation}\\] the adpative lasso estimator is \\(\\left[\\hat{\\beta}_j(\\lambda,\\gamma)\\right]_{j=1,\\ldots,p}\\) with \\(\\hat{\\beta}_j(\\lambda,\\gamma)=\\tilde{\\beta}_j(\\lambda)/w_j^{\\gamma}\\). As with the lasso, the question of the choice of the tunning parameters is crucial. Since there are two, ideally all combiations of \\((\\lambda,\\gamma)\\) should be evaluated. Hence, for a given \\(\\gamma\\), \\(\\lambda\\) is optimized using an out-of-sample validation measure such as 10-fold CV or the \\(C_p\\) to get \\(\\lambda_{\\gamma}\\) and in a second layer, \\(\\gamma\\) is also optimized (actually \\((\\gamma,\\lambda_{\\gamma})\\) is optimized in \\(\\gamma\\)) again using an out-of-sample validation measure such as 10-fold CV or the \\(C_p\\). The choice for the latter does not have to be the same for \\(\\lambda\\) and \\(\\gamma\\). The adaptive lasso is a two-stage approach, with at the first stage a chosen initial estimator which produces a single set of weights \\(w_j^{\\gamma}\\) which are held constant (for fixed \\(\\gamma\\)) across all values of \\(\\lambda\\). An alternative approach, known as a pathwise approach is to let the weights change with \\(\\lambda\\): \\(w_j^{\\gamma\\lambda}:=w_j^{\\gamma\\lambda}\\left(\\hat{\\beta}_j\\right)\\). A notable example is the relaxed lasso of Meinshausen (2007) which, for \\(\\lambda\\geq 0\\) and \\(0&lt;\\phi\\leq 1\\) is given by \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}(\\lambda,\\phi)=\\mbox{argmin}_{\\boldsymbol{\\beta}}\\left\\Vert\\mathbf{y}-\\sum_{j=1}^p\\mathbf{x}_j{\\beta}_jI\\left(j\\in \\mathcal{A}_{\\lambda}\\right)\\right\\Vert^2-\\phi\\lambda\\Vert\\boldsymbol{\\beta}\\Vert_1 \\end{equation}\\] with \\(\\mathcal{A}_{\\lambda}\\) the set of indicators of the covariates in the active set produced by the corresponding \\(\\lambda\\). The later hence controls the variable selection part, as in the lasso, while the relaxation parameter \\(\\phi\\) controls the shrinkage of coefficients. If \\(\\phi= 1\\), the lasso and relaxed lasso estimators are identical, and for \\(\\phi&lt;1\\), the shrinkage of coefficients in the selected model is reduced compared to the lasso. When \\(\\phi\\rightarrow 0\\), \\(\\hat{\\beta}_j(\\lambda,\\phi)\\rightarrow\\hat{\\beta}_j^{(LS/\\mathcal{A}_{\\lambda})}\\), where the \\(\\hat{\\beta}_j^{(LS/\\mathcal{A}_{\\lambda})}\\) is the LS obtained on the subset \\(\\mathcal{A}_{\\lambda}\\). Note that if \\(\\phi=0\\) one gets the LS on the full model (the effect of \\(\\lambda\\) as variable selection tunning parameters disappears). To obtain the relaxed lasso solutions, one first sets \\(\\phi=1\\) and obtains the lasso path, then for each submodel of the lasso path, the LARS algorithm is run by varying the penalty \\(\\phi\\lambda\\) through \\(\\phi\\). There is in general no need to do the full LARS in the second stage so the compuation can be nearly as fast as a for the lasso. Again, the tunning parameters \\((\\lambda,\\phi)\\) can be determined by 10-fold CV. 4.4.2 The elastic net When there is strong correlation among the covariates, the lasso estimator will somehow arbitrarily choose among a set of strong but correlated variables. The ridge penalty, on the other hand, tends to shrink the coefficients of correlated variables toward each other. The elastic net penalty proposed by Zou and Hastie (2005) is a comprimise between the lasso and ridge regression and the estimators are the solution of the optimization problem \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}(\\lambda)=\\mbox{argmin}_{\\boldsymbol{\\beta}}\\left\\Vert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\right\\Vert^2-\\alpha\\lambda\\Vert\\boldsymbol{\\beta}\\Vert_1-\\frac{1}{2}(1-\\alpha)\\lambda\\Vert\\boldsymbol{\\beta}\\Vert^2 \\end{equation}\\] The elastic net solution contains in general more active covariates with smaller coefficients. The introduction of the ridge penalty has the effect of reducing the variance of lasso estimates at the cost of further increasing their bias. Adding a ridge penalty is not always universally benefficial, as the bias can dominate the variance. However, since like ridge regression, the elastic net shrinks the coefficients of correlated variables toward each other, it tends to select correlated variables in groups, which might be an advantage in for example biological studies. 4.4.3 The nonnegative garotte Based on the argument that shrinking decreases the estimation/prediction variance, Breiman (1995) proposes to directly define shrunk estimators as \\[\\begin{equation} \\hat{\\beta}_j(\\lambda)=u_j(\\lambda)\\hat{\\beta}_j^{(LS)} \\end{equation}\\] with shrinking factors (or garotte) \\(\\mathbf{u}(\\lambda)=[u_j(\\lambda)]_{j=1,\\ldots,p}\\) obtained as \\[\\begin{eqnarray} &amp; \\mathbf{u}(\\lambda)=\\mbox{argmin}_{\\mathbf{u}=[u_1\\ldots u_p]}\\left\\Vert\\mathbf{y}-\\mathbf{X}\\mathbf{B}\\mathbf{u}\\right\\Vert^2-2\\lambda\\sum_{j=1}^pu_j \\\\ &amp; \\mbox{s.t. } u_j\\geq 0 \\forall j \\end{eqnarray}\\] with \\(\\mathbf{B}=\\mbox{diag}\\left(\\hat{\\beta}_j^{(LS)}\\right)\\). The penalty corresponds to the constraint \\(\\sum_{j=1}^pu_j\\leq t_{\\lambda}\\). For sufficiently large \\(\\lambda\\) some of the \\(u_j\\) will be set to 0, so that the nonnegative garotte estimator performs simultaneously shinking and subset selection. Yuan and Lin (2007) propose to use a slightly modified version of the LARS to obtain the solution path for the nonnegative garotte estimators. Indeed, since \\(\\mathbf{B}\\) is provided in an initial stage, then one can use the LARS (adapted to the positivity constraints) in which \\(\\mathbf{X}\\rightarrow \\tilde{\\mathbf{X}}=\\mathbf{X}\\mathbf{B}\\) and \\(\\boldsymbol{\\beta}\\rightarrow \\mathbf{u}\\), \\(\\lambda\\rightarrow 2\\lambda\\). The adaptive lasso with \\(\\gamma=1\\) corresponds to the nonnegative garotte (when adding the positivity constraints). Indeed, we have \\(u_j=\\vert\\beta_j\\vert/\\vert\\beta_j^{(LS)}\\vert\\) with the constraints \\(\\beta_j\\beta_j^{(LS)}\\geq 0\\), \\(\\forall j\\), so that \\(\\mathbf{X}\\mathbf{B}\\mathbf{u}=\\mathbf{X}\\boldsymbol{\\beta}\\). Given the form of the nonnegation garotte estimator \\(\\hat{\\boldsymbol{\\beta}}(\\lambda)=\\left[\\hat{\\beta}_j(\\lambda)\\right]_{j=1,\\ldots,p}\\), it is straightforward to derive Efron’s covariance penalty estimator of out-of-sample prediction error. Indeed, \\(\\hat{\\mathbf{y}}=\\mathbf{X}\\mathbf{B}\\mathbf{u}=\\mathbf{X}\\mathbf{U}\\hat{\\boldsymbol{\\beta}}^{(LS)}=\\mathbf{X}\\mathbf{U}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}\\), with \\(\\mathbf{U}=\\mbox{diag}\\left(u_j\\right)\\), and \\[\\begin{equation} \\mbox{tr}\\left(\\mbox{cov}(\\hat{\\mathbf{Y}},\\mathbf{Y})\\right)= \\sigma^2\\mbox{tr}\\left(\\mathbf{X}\\mathbf{U}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\right)=\\sigma^2\\mbox{tr}\\left(\\mathbf{U}\\right)=\\sigma^2\\sum_{j=1}^pu_j; \\end{equation}\\] by using the cyclic property of the trace operator (see also Xiong (2010)). Therefore, one can determine the \\(u_j\\) by minimizing the estimated out-of-sample prediction error, i.e. \\(\\mbox{min}_{\\mathbf{u}} \\Vert \\mathbf{y}−\\mathbf{X}\\mathbf{U}\\hat{\\boldsymbol{\\beta}}^{(LS)}\\Vert+2\\sigma^2 \\sum_{j=1}^pu_j\\), s.t. \\(u_j \\geq 0\\), \\(\\forall j\\), which yields the nonnegative garotte estimator with tuning parameter \\(\\lambda = \\sigma^2\\). In practice, \\(\\sigma^2\\) is replaced by a consistent estimator \\(\\hat{\\sigma}^2\\); for a review, see e.g. Reid, Tibshirani, and Friedman (2016). 4.4.4 Non convex penalties The adaptive lasso consists of a two-stage approach involving an initial estimator to reduce bias for large regression coefficients. An alternative single-stage approach is to use a penalty that tapers off as \\(\\beta_j\\) becomes larger in absolute value. Unlike the absolute value penalty employed by the lasso, a tapering penalty function cannot be convex. Such functions are often referred to as folded concave, in that it is symmetric around \\(\\beta_j=0\\). The penalty function, say \\(\\sum_{j=1}^pP(\\beta_j\\vert \\lambda,\\gamma)\\), then involves a tuning parameter \\(\\gamma\\) that controls the concavity of the penalty (i.e., how rapidly the penalty tapers off). Using a penalty that does not taper off (e.g. the lasso penalty) is called soft thresholding, while using subset (forward stepwise) selection (i.e. the \\(l_0\\) norm) is called hard thresholding. A popular penalty is the smoothly clipped absolute deviations (SCAD) penalty of Fan and Li (2001), given by \\[\\begin{eqnarray} P(\\beta_j\\vert \\lambda,\\gamma)=\\left\\{\\begin{array}{ll} \\lambda\\vert\\beta_j\\vert &amp; \\mbox{if } \\;\\vert\\beta_j\\vert\\leq\\lambda \\\\ \\frac{2\\gamma\\lambda\\vert\\beta_j\\vert-\\beta_j^2-\\lambda^2}{2(\\gamma-1)} &amp; \\mbox{if } \\;\\lambda&lt;\\vert\\beta_j\\vert\\leq\\gamma\\lambda \\\\ \\frac{\\lambda^2(\\gamma+1)}{2} &amp; \\mbox{if } \\;\\vert\\beta_j\\vert&gt;\\gamma\\lambda \\end{array} \\right. \\end{eqnarray}\\] for \\(\\gamma&gt;2\\). The SCAD coincides with the lasso until \\(\\vert\\beta_j\\vert=\\lambda\\), then smoothly transits to a quadratic function until \\(\\vert\\beta_j\\vert = \\gamma\\lambda\\), after which it remains constant. The derivative of the SCAD penalty is \\[\\begin{eqnarray} \\frac{\\partial}{\\partial\\beta_j}P(\\beta_j\\vert \\lambda,\\gamma)=\\left\\{\\begin{array}{ll} \\lambda &amp; \\mbox{if } \\;\\vert\\beta_j\\vert\\leq\\lambda \\\\ \\frac{\\gamma\\lambda-\\vert\\beta_j\\vert}{\\gamma-1} &amp; \\mbox{if } \\;\\lambda&lt;\\vert\\beta_j\\vert\\leq\\gamma\\lambda \\\\ 0 &amp; \\mbox{if } \\;\\vert\\beta_j\\vert&gt;\\gamma\\lambda \\end{array} \\right. \\end{eqnarray}\\] The SCAD penalty retains the penalization rate (and bias) of the lasso for small coefficients, but continuously relaxes the rate of penalization (and bias) as the absolute value of the coefficient increases. Hence the overall bias is reduced. Another population non convex penalty has been proposed by Zhang (2010) as the Minimax Concave Penalty (MCP), given by \\[\\begin{eqnarray} P(\\beta_j\\vert \\lambda,\\gamma)=\\left\\{\\begin{array}{ll} \\lambda\\vert\\beta_j\\vert - \\frac{\\beta_j^2}{2\\gamma}&amp; \\mbox{if } \\;\\vert\\beta_j\\vert\\leq \\gamma\\lambda \\\\ \\frac{\\lambda^2\\gamma}{2} &amp; \\mbox{if } \\;\\vert\\beta_j\\vert&gt;\\gamma\\lambda \\end{array} \\right. \\end{eqnarray}\\] for \\(\\gamma&gt;1\\). Its derivative is \\[\\begin{eqnarray} \\frac{\\partial}{\\partial\\beta_j}P(\\beta_j\\vert \\lambda,\\gamma)=\\left\\{\\begin{array}{ll} \\mbox{sign}\\left(\\beta_j\\right)\\left(\\lambda - \\frac{\\vert\\beta_j\\vert}{\\gamma}\\right)&amp; \\mbox{if } \\;\\vert\\beta_j\\vert\\leq\\gamma\\lambda \\\\ 0 &amp; \\mbox{if } \\;\\vert\\beta_j\\vert&gt;\\gamma\\lambda \\end{array} \\right. \\end{eqnarray}\\] As with the SCAD, the MCP starts out by applying the same rate of penalization as the lasso, then smoothly relaxes the rate down to zero as the absolute value of the coefficient increases. It does however so immediately while with SCAD the rate remains flat for a while before decreasing. Reported from Zhang (2010), Figure 1. Penalty functions (right panel) and their derivatives (left panel) as a function of \\(\\vert\\beta_j\\vert\\), for the lasso (\\(t\\)), SCAD and MCP with \\(\\gamma=2.5\\). As \\(\\gamma\\rightarrow\\infty\\), both the MCP and SCAD penalties converge to the \\(l_1\\) penalty. As \\(\\gamma\\) approaches its minimum value, the bias is minimized. The price to pay is the instability of the resulting MCP and SCAD estimators, in the sense that as the penalty becomes more concave (i.e. for smaller \\(\\gamma\\)), there is a greater chance for the optimization problem to admit multiple local minima. Among others, Zhang (2010) and Breheny and Huang (2011) propose algorithms to find the SCAD and MCP estimates, or more generally, penalized regression estimators with non convex penalties. Moreover, as with the elastic net, one can add a ridge penalty to nonconvex penalties such as MCP and SCAD. Exercise (solutions presented in 7.4.3): After having read the documentation of the R package ncvreg and installed it, perform the following steps: Fix the generating vector \\(\\boldsymbol{\\beta}=(4,2,-4,-2,0,0,\\ldots,0)\\) and set the seed equal to 11 (i.e. set.seed(11)). Generate from a MVN (multivariate normal) a matrix \\(\\mathbf{X_{n*p}}\\) with \\(n = 200\\) and \\(p = 1000\\). You can choose the location vector as you wish but set the scale matrix with an autoregressive form \\(\\boldsymbol{\\Sigma}=[\\sigma_{lm}]_{l,m=1,\\ldots,p}\\) with \\(\\sigma_{lm} = \\rho^{\\mid l - m\\mid}\\). For each \\(\\boldsymbol{\\rho} = [0 \\; 0.2 \\; 0.5]\\) generate \\(\\mathbf{\\hat{y}}\\) thanks to the relation \\(\\mathbf{y} = \\mathbf{X_{n*p}} \\; \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) where \\(\\epsilon_{i}\\) is a standard normal. Suppose for simplicity that the errors are uncorrelated. Compare the solution paths (graphically as a function of \\(\\lambda\\)) for the lasso, SCAD and MCP by fixing several values for \\(\\gamma\\) (choose e.g. \\(\\gamma=(1.5, 2, 3, 3.7, 5)\\)) for each value of \\(\\rho\\) indicated at point c. References "],
["properties-of-model-selection-criteria.html", "5 Properties of model selection criteria 5.1 Introduction 5.2 Selection consistency 5.3 Selection efficiency 5.4 The oracle property 5.5 Probability of overfitting", " 5 Properties of model selection criteria 5.1 Introduction With model selection methods, defining suitable properties comes in several forms. We consider here the case of the linear regression model with \\(Y_i|\\mathbf{x}_i \\sim \\mathcal{N}(\\boldsymbol{\\mu}\\left(\\mathbf{x}_i\\right),\\sigma^2), 0&lt;\\sigma^2&lt;\\infty\\), \\(i=1,\\ldots,n\\), with \\[\\begin{equation} \\boldsymbol{\\mu}\\left(\\mathbf{x}_i\\right)=\\mathbf{x}_i^T \\boldsymbol{\\beta}, \\end{equation}\\] where \\(\\boldsymbol{\\beta} \\in \\mathbb{R}^p\\) and \\(\\mathbf{x}_i^T\\) is the ith row of \\(\\mathbf{X}\\) (that includes a column of ones for the intercept). The normality assumption is not necessary (but sufficient), weaker assumptions on the form of the error distribution can be instead assumed. Asymptotically, a consistent estimator for \\(\\boldsymbol{\\beta}\\) such as the MLE (LS estimator) will converge to the true regression coefficients \\(\\beta_j\\neq 0\\) for \\(j\\in \\mathcal{J}_S=\\{j\\in\\mathcal{J}=\\{j=1,\\ldots,p\\}\\;\\vert\\; \\beta_j\\neq 0,\\}\\) and \\(\\beta_j=0\\) for \\(j\\in\\mathcal{J}\\setminus\\mathcal{J}_{S}\\), where \\(\\mathcal{J}\\setminus\\mathcal{J}_{S}\\) is the complement set of \\(\\mathcal{J}_{S}\\) in \\(\\mathcal{J}\\). If \\(\\mathcal{J}\\setminus\\mathcal{J}_{S}\\neq\\emptyset\\), one is under a sparse setting, and under this setting, a model selection method should be able to identify the subset \\(\\mathcal{J}_S\\), by means of say \\(\\widehat{\\mathcal{J}_S}\\), in the most parsimonious way, i.e. \\(s=\\vert\\mathcal{J}_S\\vert\\leq\\vert\\widehat{\\mathcal{J}_S}\\vert&lt;\\vert\\mathcal{J}\\vert=p\\), where \\(\\vert\\mathcal{J}\\vert\\) denotes the number of elements in the set \\(\\mathcal{J}\\). Let \\(\\mathcal{M}=\\{\\mathcal{M}_l\\;\\vert\\;l=1,\\ldots,L\\}\\) denote the set all possible models that can be constructed with \\(K\\leq\\min(n,p)\\) covariates, with associated indicator’s set \\(\\mathcal{J}_{l}\\subseteq\\mathcal{J}\\), which are all submodels of the full one(s) (i.e. with \\(K\\) covariates). Let \\(\\mathcal{M}_S\\) denote the true model, one distinguishes two situations that are assumed a priori: \\(\\mathcal{M}_S\\in\\mathcal{M}\\) or \\(\\mathcal{M}_S\\notin\\mathcal{M}\\). If \\(\\mathcal{M}_S\\in\\mathcal{M}\\), then a reasonable property for a model selection criterion is its ability to find \\(\\mathcal{J}_S\\), measured by the consistency of \\(\\widehat{\\mathcal{J}_S}\\), i.e. its ability to converge, in some sense and when the available information is sufficiently large, to \\(\\mathcal{J}_S\\). If \\(\\mathcal{M}_S\\notin\\mathcal{M}\\), one instead assumes that one of the possible models \\(\\mathcal{M}_{\\jmath_S}\\in\\mathcal{M}\\) is the closest to the true one with respect to a suitable distance, e.g. the Kullback-Leibler (KL) distance. A selection criterion that selects (for sufficiently large \\(n\\)) this submodel is said to be (asymptotically) efficient. However, model selection consistency is not the same as estimation consistency, and the nearest property to estimation consistency is selection efficiency. As will be seen later, selection consistency excludes selection efficiency (and conversely), so that different criteria serve different purposes. For pure prediction, selection consistency is possibly the most desired property since it avoids the problem of overfitting. For model building for a better understanding of the phenomenon under study, selection efficiency is a more desirable property, if the probability of overfitting associated to the selection criterion remains reasonable. Indeed, since the model selection criteria are statistics that are subject to random sampling, their (asymptotic) distribution can be derived. From the later, one can evaluate the probability of overfitting and the signal-to-noise ratio (SNR). A property that is also invoked in the case of penalized methods (that perform simultaneously selection and estimation) is the oracle property (Fan and Li 2001) that concerns a form of conditional consistency for the regression coefficient estimator of the selected submodel, derived under the sparsity assumption (and selection consistency). For the linear regression model, there are (at least) two conditions that are assumed which are 1. For all \\(\\mathcal{M}_\\jmath\\in\\mathcal{M}\\) with associated covariates matrix \\(\\mathbf{X}_\\jmath\\) (i.e. \\(\\mathbf{X}_\\jmath\\) is formed by the columns \\(\\jmath\\in\\mathcal{J}_{\\jmath}\\) of \\(\\mathbf{X}\\)), we have \\(\\lim_{n\\rightarrow \\infty} \\frac{1}{n}\\mathbf{X}_\\jmath^T\\mathbf{X}_\\jmath=\\mathbf{\\Sigma}_{\\jmath\\jmath}\\), with \\(\\mathbf{\\Sigma}_{\\jmath\\jmath}\\) a positive definite matrix. 2. When \\({\\sigma}^2\\) is unknown it can be replaced by a consistent estimator \\(\\hat{\\sigma}^2\\). Moreover, in our context, information takes two forms, the sample size \\(n\\) and also the number of available covariates (features, factors, regressors) \\(p\\). We denote by \\(\\xi\\left(\\mathbf{A}\\right)\\) the the smallest eigenvalue of the matrix \\(\\mathbf{A}\\). 5.2 Selection consistency Let \\(R(\\boldsymbol{\\beta})=\\Vert \\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta}\\Vert^2\\) and consider the LS estimator obtained from a subset \\(\\mathcal{J}_{\\jmath}\\) of covariates given by \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}}_{\\mathcal{J}_{\\jmath}}=\\mbox{argmin}_{\\boldsymbol{\\beta}\\vert \\beta_j=0, j\\in \\mathcal{J}\\setminus\\mathcal{J}_{\\jmath}}R(\\boldsymbol{\\beta}) \\end{equation}\\] We consider here model selection criteria from a Generalized Information Criterion (GIC) class (Shao 1997,Kim, Kwon, and Choi (2012)) that estimate a subset of covariates as \\[\\begin{equation} \\widehat{\\mathcal{J}_{\\jmath}}\\left(\\lambda_n\\right)=\\mbox{argmin}_{\\mathcal{J}_{\\jmath}}R\\left(\\hat{\\boldsymbol{\\beta}}_{\\mathcal{J}_{\\jmath}}\\right)+\\lambda_n\\vert\\mathcal{J}_{\\jmath}\\vert\\sigma^2 \\end{equation}\\] where \\(\\lambda_n\\) possibly depends on \\(n\\). The AIC (\\(C_p\\)) is obtained with \\(\\lambda_n=2\\), while the BIC is obtained with \\(\\lambda_n=\\log(n)\\). Note that \\(\\vert\\mathcal{J}\\vert=p&gt;n\\) in high dimensions, it is therefore more suitable to restrict the GIC class to submodels \\(\\mathcal{J}_{\\jmath,s_n}\\) such that \\(\\vert\\mathcal{J}_{\\jmath,s_n}\\vert\\leq s_n\\) where \\(s_n\\) can possibly grow with the sample size \\(n\\). A member of the GIC class is said to be selection consistent if \\[\\begin{equation} \\lim_{n\\rightarrow\\infty}P\\left(\\widehat{\\mathcal{J}_{S,s_n}}\\left(\\lambda_n\\right)=\\mathcal{J}_{S}\\right)=1 \\end{equation}\\] Kim, Kwon, and Choi (2012) sets the necessary regularity conditions (essentially on the design matrix \\(\\mathbf{X}\\)) for a member of the GIC class to be selection consistent when \\(p:=p_n\\) is allowed to grow with \\(n\\). The result is then Theorem: Selection consistency of the (restricted) GIC class (Kim, Kwon, and Choi 2012) If \\(\\mathbb{E}\\left[\\varepsilon^{2k}\\right]&lt;\\infty\\) for some \\(k\\in\\mathbb{N}^{+}\\), then for \\(\\lambda_n\\) such that \\(\\lambda_n=o\\left(n^{c_2-c_1}\\right)\\) and \\(\\lim_{n\\rightarrow\\infty}p_n/\\left(\\lambda_n\\rho_n\\right)^k=0\\), with \\(0\\leq c_1&lt;1/2\\), \\(2c_1&lt;c_2\\leq 1\\), \\(\\rho_n=\\mbox{inf}_{\\mathcal{J}_{S}\\;\\vert\\;\\vert \\mathcal{J}_{S}\\vert \\leq s_n}\\xi\\left(\\mathbf{X}_{S}^T\\mathbf{X}_{S}/n\\right)\\), a member of the GIC class is selection consistent. The notation \\(\\lambda_n=o\\left(n^{c_2-c_1}\\right)\\) is equivalent to \\(\\lim_{n\\rightarrow\\infty} \\lambda_n/n^{c_2-c_1}=0\\). In particular, we can take \\(c_1=0\\) and \\(c_2=1\\) so that \\(\\lambda_n\\) must be \\(o(n)\\). Condition \\(\\lim_{n\\rightarrow\\infty}p_n/\\left(\\lambda_n\\rho_n\\right)^k=0\\) indicated that the penalty needs to grow faster with \\(n\\) (through \\(\\lambda_n\\)) than the number of available covariates \\(p_n\\). If \\(p_n\\) does not depend on \\(n\\) (hence it is fixed), then selection consistency is achieved when \\(\\lambda_n=o(n)\\) and \\(\\lim_{n\\rightarrow\\infty}\\lambda_n=\\infty\\). Hence, the BIC is selection consistent while the AIC (\\(C_p\\)) is not. Selection consistency is an asymptotic concept which does not indicate what happens in finite samples. For example, the \\(C_p\\) could be transformed by changing the factor \\(2\\) with say \\(2\\cdot n^{\\alpha}\\), with e.g. \\(\\alpha=0.0000001\\) and we then have a selection consistent estimator, but \\(2\\cdot n^{\\alpha}\\approx 2\\) even for very large values of \\(n\\). 5.3 Selection efficiency While selection consistency is concerned by the selection of the most parsimonious model, efficiency studies the behavior of a model selection criterion relative to a distance between the true model and the selected one (asymptotically). In particular, one can use for the distance, a loss function for prediction purposes. Such a loss function can be the squared prediction error (\\(L_2\\) loss function), given by \\[\\begin{equation} L_n\\left(\\widehat{\\mathcal{J}_{\\jmath}}\\right)=\\frac{1}{n}\\Vert \\boldsymbol{\\mu}\\left(\\mathbf{X}\\right) - \\hat{\\mathbf{Y}}_{\\widehat{\\mathcal{J}_{\\jmath}}}\\Vert^2=\\frac{1}{n}\\Vert \\mathbf{X}\\boldsymbol{\\beta} - \\hat{\\mathbf{Y}}_{\\widehat{\\mathcal{J}_{\\jmath}}}\\Vert^2 \\end{equation}\\] where \\(\\hat{\\mathbf{Y}}_{\\widehat{\\mathcal{J}_{\\jmath}}}\\) is the vector of predicted responses \\(\\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{{\\widehat{\\mathcal{J}_{\\jmath}}}}\\) with \\(\\hat{\\boldsymbol{\\beta}}_{{\\widehat{\\mathcal{J}_{\\jmath}}}}\\) of length \\(p\\) with elements \\(\\beta_j:=0, j\\in \\mathcal{J}\\setminus{\\widehat{\\mathcal{J}_{\\jmath}}}\\). Let \\(\\mathcal{J}_{S}\\) be defined as \\[\\begin{equation} L_n\\left(\\mathcal{J}_{S}\\right)= \\min_{\\mathcal{M}_l}L_n\\left(\\mathcal{J}_{l}\\right) = \\min_{\\mathcal{M}_l}\\frac{1}{n}\\Vert \\hat{\\mathbf{Y}}_{\\mathcal{J}_{l}}-\\mathbf{X}\\boldsymbol{\\beta}\\Vert^2 \\end{equation}\\] An efficient model selection criterion that selects the submodel with indices \\(\\widehat{\\mathcal{J}_S}\\) is such that \\[\\begin{equation} \\frac{\\mathbb{E}\\left[L_n\\left(\\widehat{\\mathcal{J}_{S}}\\right)\\right]}{\\mathbb{E}\\left[L_n\\left(\\mathcal{J}_S\\right)\\right]}\\overset{p}{\\rightarrow}1 \\end{equation}\\] In other words, a model selection criterion is efficient if it selects a model such that the ratio of the expected loss function at the selected model and the expected loss function at its theoretical minimiser converges to one in probability (Shao 1997). Under some suitable technical conditions, the \\(C_p\\) (AIC) is an efficient model selection criterion. This is also true for other model selection criteria with finite \\(\\lambda_n\\). More importantly, a consistent model selection (i.e. \\(\\lambda_n\\rightarrow\\infty\\)) cannot be efficient (Yang 2005). Actually, consistency guaranties the identification of the true or best model first while efficiency is related to model estimation accuracy. For the later, a suitable measure is the minimax rate optimality (Yang 2005). The \\(C_p\\) (AIC) is minimax-rate optimal for estimating the regression function, while the BIC and adaptive model selection (penalized regression methods) are not (see also Leeb and Pötscher 2008). Let \\(\\mathbf{H}_{\\mathcal{J}_{\\jmath}}=\\mathbf{X}_{\\mathcal{J}_{\\jmath}}\\left(\\mathbf{X}_{\\mathcal{J}_{\\jmath}}^T\\mathbf{X}_{\\mathcal{J}_{\\jmath}}\\right)^{-1}\\mathbf{X}_{\\mathcal{J}_{\\jmath}}^T\\), with \\(\\mathbf{X}_{\\mathcal{J}_{\\jmath}}\\) formed by the columns of \\(\\mathbf{X}\\) in \\(\\mathcal{J}_{\\jmath}\\). We have \\(\\hat{\\mathbf{Y}}_{\\mathcal{J}_{\\jmath}}=\\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\mathbf{Y}\\). Hence \\[\\begin{eqnarray} L_n\\left(\\mathcal{J}_{\\jmath}\\right)&amp;=&amp;\\frac{1}{n}\\Vert \\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\left(\\mathbf{X}\\boldsymbol{\\beta}+\\boldsymbol{\\varepsilon}\\right)\\Vert^2= \\frac{1}{n}\\Vert \\left(\\mathbf{I} - \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\right)\\mathbf{X}\\boldsymbol{\\beta} -\\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\boldsymbol{\\varepsilon}\\Vert^2\\\\ &amp;=&amp; \\frac{1}{n}\\Vert \\left(\\mathbf{I} - \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\right)\\mathbf{X}\\boldsymbol{\\beta}\\Vert^2 -2\\frac{1}{n}\\Vert\\boldsymbol{\\varepsilon}^{T}\\mathbf{H}_{\\mathcal{J}_{\\jmath}}^T\\left(\\mathbf{I} - \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\right)\\mathbf{X}\\boldsymbol{\\beta}\\Vert^2 +\\frac{1}{n}\\boldsymbol{\\varepsilon}^T\\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\boldsymbol{\\varepsilon} \\\\ &amp;=&amp; \\frac{1}{n}\\Vert \\left(\\mathbf{I} - \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\right)\\mathbf{X}\\boldsymbol{\\beta}\\Vert^2 +\\frac{1}{n}\\boldsymbol{\\varepsilon}^T\\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\boldsymbol{\\varepsilon} = \\Delta\\left(\\mathcal{J}_{\\jmath}\\right)+\\frac{1}{n}\\boldsymbol{\\varepsilon}^T\\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\boldsymbol{\\varepsilon} \\end{eqnarray}\\] with \\(\\boldsymbol{\\varepsilon}=\\mathbf{Y}-\\mathbf{X}\\boldsymbol{\\beta}\\). Within the GIC class, \\(\\widehat{\\mathcal{J}_{\\jmath}}\\) is is obtained as the minimizer in \\(\\mathcal{J}_{\\jmath}\\) of \\[\\begin{equation} \\frac{1}{n}\\Vert \\mathbf{Y} - \\hat{\\mathbf{Y}}_{\\mathcal{J}_{\\jmath}}\\Vert^2 +\\frac{1}{n}\\lambda_n\\vert\\mathcal{J}_{\\jmath}\\vert\\hat{\\sigma}^2 \\end{equation}\\] We have that \\[\\begin{eqnarray} \\frac{1}{n}\\Vert \\mathbf{Y} - \\hat{\\mathbf{Y}}_{\\mathcal{J}_{\\jmath}}\\Vert^2&amp;=&amp; \\frac{1}{n}\\Vert \\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\mathbf{X}\\boldsymbol{\\beta}+ \\boldsymbol{\\varepsilon} - \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\boldsymbol{\\varepsilon}\\Vert^2 \\\\ &amp;=&amp; \\frac{1}{n}\\Vert \\left(\\mathbf{I}- \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\right)\\mathbf{X}\\boldsymbol{\\beta}\\Vert^2 + 2\\frac{1}{n}\\boldsymbol{\\beta}^T\\mathbf{X}^T\\left(\\mathbf{I}- \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\right) \\left(\\mathbf{I} - \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\right)\\boldsymbol{\\varepsilon} +\\frac{1}{n}\\Vert \\boldsymbol{\\varepsilon} - \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\boldsymbol{\\varepsilon}\\Vert^2\\\\ &amp;=&amp; \\Delta\\left(\\mathcal{J}_{\\jmath}\\right)+ 2\\frac{1}{n}\\boldsymbol{\\beta}^T\\mathbf{X}^T \\left(\\mathbf{I} - \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\right)\\boldsymbol{\\varepsilon} +\\frac{1}{n}\\Vert \\boldsymbol{\\varepsilon}\\Vert^2 -2\\frac{1}{n}\\boldsymbol{\\varepsilon}^T \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\boldsymbol{\\varepsilon}+\\frac{1}{n}\\Vert\\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\boldsymbol{\\varepsilon}\\Vert^2\\\\ &amp;=&amp; \\Delta\\left(\\mathcal{J}_{\\jmath}\\right)+ 2\\frac{1}{n}\\boldsymbol{\\beta}^T\\mathbf{X}^T \\left(\\mathbf{I} - \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\right)\\boldsymbol{\\varepsilon} +\\frac{1}{n}\\Vert \\boldsymbol{\\varepsilon}\\Vert^2 -\\frac{1}{n}\\boldsymbol{\\varepsilon}^T \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\boldsymbol{\\varepsilon} \\end{eqnarray}\\] When \\(\\mathcal{J}_S\\subset\\widehat{\\mathcal{J}_{\\jmath}}\\), that is \\(\\beta_j\\neq 0 \\forall j\\in \\mathcal{J}_S\\) and \\(\\beta_j=0\\) otherwise, \\(\\boldsymbol{\\beta}^T\\mathbf{X}^T \\left(\\mathbf{I} - \\mathbf{H}_{\\widehat{\\mathcal{J}_{\\jmath}}}\\right)=0\\) and \\(\\Delta\\left(\\mathcal{J}_{\\jmath}\\right)=0\\), so that \\(\\widehat{\\mathcal{J}_{\\jmath}}\\) is is obtained as the minimizer in \\(\\mathcal{J}_{\\jmath}\\) of \\[\\begin{equation} -\\frac{1}{n}\\boldsymbol{\\varepsilon}^T \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\boldsymbol{\\varepsilon} +\\frac{1}{n}\\lambda_n\\vert\\mathcal{J}_{\\jmath}\\vert\\hat{\\sigma}^2 = L_n\\left(\\mathcal{J}_{\\jmath}\\right) +\\left(\\frac{1}{n}\\lambda_n\\vert\\mathcal{J}_{\\jmath}\\vert\\hat{\\sigma}^2-2\\frac{1}{n}\\boldsymbol{\\varepsilon}^T \\mathbf{H}_{\\mathcal{J}_{\\jmath}}\\boldsymbol{\\varepsilon}\\right) \\end{equation}\\] 5.4 The oracle property The oracle property was set by Fan and Li (2001) as a desirable property for model selection methods that do model selection and estimation simultaneously (regularized regression). It concerns the properties of the resulting model estimator, \\(\\hat{\\boldsymbol{\\beta}}\\left(\\widehat{\\mathcal{J}_{\\jmath}}\\right)\\) obtained with the penalty term \\(\\lambda_n\\) (e.g. in the linear regression model), that, for the oracle property to hold, must satisfy: 1. The corresponding model selection criterion, based on \\(\\lambda_n\\), is consistent (by providing \\(\\widehat{\\mathcal{J}_S}\\)) 2. \\[\\begin{equation} \\sqrt{n}\\left( \\hat{\\boldsymbol{\\beta}}\\left(\\widehat{\\mathcal{J}_S}\\right)- \\hat{\\boldsymbol{\\beta}}\\left(\\mathcal{J}_S\\right) \\right) \\overset{D}{\\rightarrow} \\mathcal{N}\\left(\\mathbf{0},\\boldsymbol{\\Sigma}\\left(\\mathcal{J}_S\\right)\\right), \\end{equation}\\] where \\(\\boldsymbol{\\Sigma}\\left(\\mathcal{J}_S\\right)\\) is the covariance matrix knowing the true subset model. The oracle property is based on the sparsity assumption, i.e. the ability for the model selection methods to estimate \\(\\beta_j\\in\\mathcal{J}\\setminus\\mathcal{J}_S\\) exactly as zero (with probability approaching one as sample size increases). As a consequence, a model selection procedure enjoying the oracle property does as good (asymptotically) as the maximum likelihood estimator on the full model. The oracle property is a concept that holds only asymptotically. In finite samples, Leeb and Pötscher (2008) show that the estimator \\(\\hat{\\boldsymbol{\\beta}}\\left(\\widehat{\\mathcal{J}_S}\\right)\\) satisfying the sparsity condition \\[\\begin{equation} \\mathcal{P}\\left( I\\left( \\hat{\\beta}_j\\left(\\widehat{\\mathcal{J}_S}\\right) \\neq 0 \\right) \\leq I\\left( \\beta_j\\neq 0\\right) \\right)\\rightarrow 1, \\mbox{as }\\; n\\rightarrow\\infty, \\; \\forall j\\in \\{1,\\ldots,p\\} \\end{equation}\\] have unbounded estimation risk as measured by the maximal mean squared error of estimation \\[\\begin{equation} \\mbox{sup}_{\\boldsymbol{\\beta}}\\mathbb{E}\\left[n\\Vert \\hat{\\boldsymbol{\\beta}}\\left(\\widehat{\\mathcal{J}_S}\\right)- \\boldsymbol{\\beta}\\Vert^2\\right]\\rightarrow\\infty, \\mbox{as }\\; n\\rightarrow\\infty \\end{equation}\\] Note that for the LS (MLE), we have \\(\\mathbb{E}\\left[n\\Vert \\hat{\\boldsymbol{\\beta}}_{LS}- \\boldsymbol{\\beta}\\Vert^2\\right] =\\mbox{tr}\\left[\\left(n^{-1}\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\right]\\) which does not depend on \\(\\boldsymbol{\\beta}\\) and remains bounded as the sample size increases. This means that we cannot have it all: model selection consistency and estimation efficiency. 5.5 Probability of overfitting Model selection criteria that are efficient (hence not consistent), will select models that are not the most parsimonious ones. One can then study their associated probability of overfitting. Overfitting can be defined as choosing a model that has more variables than the model identified as closest to the true model, thereby reducing efficiency. Similarly, underfitting is defined as choosing a model with too few variables compared to the closest model, also reducing efficiency. An underfitted model may have poor predictive ability due to a lack of detail in the model, while an overfitted model may be unstable in the sense that predictions are highly variable (noisy). The probability of overfitting for a model selection criterion, say \\(\\mbox{MSC}_l(\\lambda)\\), with \\(\\lambda\\) indicating the strength of the penalty (e.g. \\(\\lambda=2\\) for the \\(C_p\\) and AIC) and \\(l\\) the size of the selected model, is computed, for nested models, as the probability of choosing \\(L\\) additional variables to the \\(s\\) of the best (nearest) model (or true one) \\(\\mathcal{M}_S\\). Namely \\[\\begin{equation} \\mathcal{P}\\left(\\mbox{MSC}_{s+L}(\\lambda)&lt;\\mbox{MSC}_s(\\lambda)\\right) \\end{equation}\\] Note that the probability is defined for finite samples \\(n\\), but cannot always be given in a closed form (but computed by means of simulations). To compute the probability of overfitting one first derives the (sampling) distribution of the random variable \\(\\mbox{MSC}_{s+L}(\\lambda)-\\mbox{MSC}_s(\\lambda)\\) to get the probability distribution that depends on \\(L\\). McQuarrie and Tsai (1998), Section 2.5, provide the expressions of the probabilities of overfitting by \\(L\\) variables for several model selection criteria, for finite samples and for \\(n\\rightarrow\\infty\\). A measure that is associated to the probability of overfitting is the signal-to-noise ratio (SNR) given by \\[\\begin{equation} \\frac{\\mathbb{E}\\left[\\mbox{MSC}_{s+L}(\\lambda)-\\mbox{MSC}_s(\\lambda)\\right]} {\\sqrt{\\mbox{var}\\left[\\mbox{MSC}_{s+L}(\\lambda)-\\mbox{MSC}_s(\\lambda)\\right]}} \\end{equation}\\] where \\(\\mathbb{E}\\left[\\mbox{MSC}_{s+L}(\\lambda)-\\mbox{MSC}_s(\\lambda)\\right]\\) is the signal and \\(\\sqrt{\\mbox{var}\\left[\\mbox{MSC}_{s+L}(\\lambda)-\\mbox{MSC}_s(\\lambda)\\right]}\\) is the noise. While the signal depends primarily on the penalty function (\\(\\lambda\\)), the noise depends on the distribution of the (in-sample) prediction error measure. If the penalty \\(\\lambda\\) is weaker than the noise, the model selection criterion will have a weak signal, a weak signal-to-noise ratio, and will tend to overfit. McQuarrie and Tsai (1998), Section 2.5, provide the SNR (finite sample and as \\(n\\rightarrow\\infty\\)) for several model selection criteria. In general, efficient criteria have equivalent (finite) SNR, and consistent criteria all have have much larger SNR. Larger SNR is an indicator of the propensity of a model selection criterion to underfit in finite samples. Exercise: In this exercise we would like to prove that the Mallow \\(C_p\\) criterion is over-fitting asymptotically, thus not consistent in the strong sense. In order to understand the more general implications of this result, we work with a \\(\\lambda_n\\) parameter which, in the special \\(C_p\\) case, is equal to 2. We limit our reasoning to the linear model case where, by default, \\(AIC\\) and \\(C_p\\) expressions coincide. Perform the following steps: Derive the small small sample distribution of the quantity \\((C_{p, L + K} - C_{p, K})\\) where \\(C_{p, K}\\) is the \\(C_p\\) value for the supposed true model and \\(C_{p, L + K}\\) is the \\(C_p\\) value of a generic over-fitted model which selects L regressors more than the true number (i.e. K). For doing so, use the fact that: \\(C_{p,K} = \\frac{SSE_{K}}{s^2_{k^{\\star}}} - n + \\lambda_n K\\) and \\(s^2_{k^{\\star}} = \\frac{SSE_{k^{\\star}}}{n - k^{\\star}}\\) meaning that we are estimating the variance of some larger model with \\(k^{\\star}\\) regressors (e.g. full model). Thanks to the previous step, retrieve the small sample probability of over-fitting which has to depend on \\(\\lambda_n\\). Let \\(n \\rightarrow + \\infty\\) and derive the asymptotic probability of over-fitting thanks to the Slutsky theorem. For \\(\\lambda_n = [0 \\; 2 \\; log(n)]\\) derive the asymptotic probabilities of over-fitting. Show that Mallow \\(C_p\\) is not strong consistent and that BIC is not over-fitting asymptotically. What can you conclude on the role of the penalty in this specific situation? References "],
["post-selection-inference.html", "6 Post-Selection Inference 6.1 Introduction 6.2 Inference via the nonparametric Bootstrap 6.3 Improving on the Bootstrap: Smoothed Bootstrap or Bagging 6.4 Post selection significance testing", " 6 Post-Selection Inference 6.1 Introduction When model selection is performed with the intention to use the selected model for inference (not only prediction), i.e. for a better understanding of the phenomenon under study, one needs inferential methods (e.g. building confidence intervals, testing) that take account the variability induced by data-based model selection methods. In other words, by ignoring the variability introduced by the model selection step, one introduces biases when computing for example confidence intervals for slope coefficients in the linear regression model. The bias always lowers the risk measure (variance) estimator, resulting in smaller confidence intervals and possibly also not centered around the true values. This can have important impacts when for example evaluating the strength of a drug (dosage) on an health-related outcome. As an illustrative example, consider the data set cholesterol analysed in Efron and Hastie (2016) which concerns the response of 164 men to a treatment (over a period of 7 years) for lowering cholesterol levels in the body. The single predictor is a (standardized) fraction of intended dose (of the cholesterol lowering drug cholestyramine) actually taken (compliance). The response is the decrease in cholesterol level over the course of the experiment. A potential suitable model (or family of models) for the relationship between the decrease of the cholesterol level and the amount of cholestryramine (compliance) is a polynomial regression model \\[\\begin{equation} \\boldsymbol{Y}=\\sum_{j=0}^J\\beta_jX^j+\\boldsymbol{\\varepsilon}=\\beta_0+ \\beta_1X+\\beta_2X^2+\\ldots +\\beta_JX^J+\\boldsymbol{\\varepsilon} \\end{equation}\\] A model selection method is typically applied here to select \\(J\\). For the cholesterol data, using the \\(C_p\\) as model selection criterion (note that the sequence of ordered models is given), provides \\(J=3\\) as the best model. Polynomial fit (\\(J=3\\)) for the cholesterol data With this example, one might be interested in building a confidence interval for the response to treatment, at a given treatment level \\(X\\). Letting \\(\\mu=\\sum_{j=0}^J\\beta_jX^j\\), one actually seeks a confidence interval \\(CI_{1-\\alpha}\\left(\\mu\\vert \\hat{\\boldsymbol{\\beta}},\\hat{J}\\right)\\) constructed from the sample via \\(\\hat{\\boldsymbol{\\beta}}\\) and \\(\\hat{J}\\). Ignoring the randomness of \\(\\hat{J}\\) (i.e. by fixing \\(J=\\hat{J}\\)) and performing standard inference using \\(\\hat{\\mu}=\\sum_{j=0}^J\\hat{\\beta}_j\\mathbf{x}^j\\) will provide misleading confidence intervals. 6.2 Inference via the nonparametric Bootstrap One way to infer (computing confidence intervals) is by reproducing simultaneously the selection and estimation mechanism by means of the nonparametric bootstrap: \\(B\\) samples of size \\(n\\), \\(\\left(y_i^b,x_i^b\\right), i=1,\\ldots,n\\), are created by drawing with replacement from the original sample. On each of the \\(B\\) samples, selection (using e.g. the \\(C_p\\)) is performed to obtain \\(\\hat{J}^b\\), estimates \\(\\hat{\\beta}_j^b\\) used in computing \\(\\hat{\\mu}^b=\\sum_{j=0}^{\\hat{J}^b}\\hat{\\beta}_j^b\\mathbf{x}^j\\). The Figure below provides the bootstrap distribution of \\(\\hat{\\mu}^b\\), for a compliance \\(x=-2\\), and for \\(\\widetilde{\\mu}^b=\\sum_{j=0}^{J}\\hat{\\beta}_j^b\\mathbf{x}^j\\) with \\(J=3\\), the value found in the original sample. Bootstrap distribution of \\(\\tilde{\\mu}^b\\) (fixed) and \\(\\hat{\\mu}^b\\) (adaptive) on the cholesterol data As expected, the length of the confidence intervals are underestimated when one ignores the randomness introduced by the selection procedure (i.e. the randomness of \\(\\hat{J}\\)). Actually, the confidence interval for \\(\\mu\\) at \\(x=-2\\) (estimated via the nonparametric bootstrap) ignoring the randomness of the selection procedure is \\(CI_{1-\\alpha}\\left(\\mu\\vert J=3\\right)=1.05 \\pm 8.09\\) while not ignoring the randomness of the model selection procedure it is \\(CI_{1-\\alpha}\\left(\\mu\\vert \\hat{J}=3\\right)=1.40 \\pm 16.17\\). What actually happens with (hard thresholding) model selection criteria such as the \\(C_p\\) is that it is a discontinuous process, creating jumps in the estimates. In the cholesterol data example, with the nonparametric bootstrap, for about \\(20\\)% of the bootstrap samples, the \\(C_p\\) selected \\(\\hat{J}=1\\), i.e. the linear model. In these cases, \\(\\hat{\\mu}^b\\vert \\hat{J}=1\\) are smaller. This is illustrated in the Figure below. Bootstrap distribution of \\(\\tilde{\\mu}^b\\) (Fixed: \\(J=3\\)), \\(\\hat{\\mu}^b\\vert \\hat{J}=1\\) (Adaptive: \\(J=1\\)) and \\(\\hat{\\mu}^b\\vert \\hat{J}&gt;1\\) (Adaptive: \\(J&gt;1\\)) on the cholesterol data The distribution of the \\(\\hat{\\mu}^b\\) when \\(\\hat{J}=1\\) is clearly different (different location) that the distribution of the \\(\\hat{\\mu}^b\\) when \\(\\hat{J}&gt;1\\). \\(\\hat{J}=1\\) happens in about 20% of the bootstrap samples, which creates, from one sample to the other, important jumps. 6.3 Improving on the Bootstrap: Smoothed Bootstrap or Bagging Bagging stands for bootstrap averaging and leads to “improvements for unstable procedures” (Breiman 1996), which include subset selection in linear regression (Breiman 1994). Bagging is also known as bootstrap smoothing (Efron and Tibshirani 1996). Given bootstrap replicates of a bootstrapped statistic \\(T^b, b=1,\\ldots,B\\), the bootstrap standard errors are computed using the bootstrap distribution as \\[\\begin{equation} \\widehat{\\mbox{sd}}_B=\\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B\\left(T^b-\\overline{T}_B\\right)^2} \\end{equation}\\] with \\[\\begin{equation} \\overline{T}_B = \\frac{1}{B}\\sum_{b=1}^BT^b \\end{equation}\\] Since after a bootstrap sampling, \\(\\overline{T}_B\\) is available, the bagging method proposes to compute the standard errors associated to \\(\\overline{T}_B\\), which are lower. A brute force method would bootstrap several values for \\(\\overline{T}_B\\) which ends up in large computational times and is prohibitive in high dimensions. Actually, there is no need to resample further. Let \\(N_{bi}\\) denote the number of times observation \\(i\\) occurs in the bootstrap sample \\(b\\). The vector \\(\\mathbf{N}_b=\\left(N_{1b},\\ldots,N_{nb}\\right)\\) has a multinomial distribution with \\(n\\) draws on \\(n\\) categories each with associated probability \\(1/n\\), and has mean vector and covariance matrix \\[\\begin{equation} \\mathbf{N}_b \\sim \\mbox{Mult}\\left(\\boldsymbol{1}_n, \\mathbf{I}_n-\\frac{1}{n}\\boldsymbol{1}_n\\boldsymbol{1}_n\\right) \\end{equation}\\] with \\(\\boldsymbol{1}_n\\) the \\(n\\times 1\\) vector ones. Using this setting, Efron (2014) shows that the infinitesimal jackknife estimate of the standard deviation for \\(\\overline{T}_B\\) is \\[\\begin{equation} \\sqrt{\\sum_{i=1}^n\\mbox{cov}^2\\left(N_{bi},T^b\\right)} \\end{equation}\\] A suitable estimator is given by \\[\\begin{equation} \\widetilde{\\mbox{sd}}_B=\\sqrt{\\sum_{i=1}^n\\widehat{\\mbox{cov}}^2\\left(N_{bi},T^b\\right)} \\end{equation}\\] with \\[\\begin{equation} \\widehat{\\mbox{cov}}^2\\left(N_{bi},T^b\\right)=\\frac{1}{B}\\sum_{b=1}^B\\left(N_{bi}-N_{\\cdot i}\\right)\\left(T^b-\\overline{T}_B\\right) \\end{equation}\\] and \\(N_{\\cdot i}=(1/B)\\sum_{b=1}^BN_{bi}\\). Moreover, it is always true that \\[\\begin{equation} \\frac{\\widetilde{\\mbox{sd}}_B}{\\widehat{\\mbox{sd}}_B}\\leq 1 \\end{equation}\\] Bagging with the infinitesimal jackknife allows to be more precise than with the bootstrap without the need to simulate (bootstrap) more replicates, which makes it a computationally convenient inferential method. For the cholesterol data example, the standard deviations (across values for \\(x\\)) decrease of about 12%. In general, the savings due to bagging increase with the nonlinearity of \\(T^b\\) and is therefore a suitable approach for inference after selection. Wager, Hastie, and Efron (2014) have used this approach to derive confidence intervals for Random Forests. Bagging is actually a form of model averaging. 6.4 Post selection significance testing Very recently, test statistics and their associated distribution have been proposed in the linear regression case, to test hypothesis about (linear combinations of) slope parameters, that take into account the randomness of the selection part of the analysis. It is actually possible to formalize the conditional distribution of (a linear combination of) the slope estimators (typically the LS), conditionally on the distribution of the outcome of the selection method. For the later, an important result is provided by the polyhedral lemma which allows to formalize the model selection selection operation in a more tractable way. References include Lee et al. (2016), Tibshirani et al. (2016), Hastie, Tibshirani, and Wainwright (2016), Section 6.3, and Tian and Taylor (2018). References "],
["solutions.html", "7 Solutions 7.1 Chapter 1 7.2 Chapter 2 7.3 Chapter 3 7.4 Chapter 4", " 7 Solutions 7.1 Chapter 1 7.1.1 Data on Malnutrition in Zambia Load the dataset and build the variables so that they can be used for a regression analysis. require(foreign) # install foreign package if you do not have it yet # See section 1.6.2 e-book for information on the dataset. # dat = read.spss(&quot;Zambia.SAV&quot;, add.undeclared.levels = &quot;no&quot;) dat = read.spss(&quot;Zambia.SAV&quot;) # Construct system matrix # The idea behind this exercise is to be aware that data cleaning is most of the times the real issue # with a real problem. It is sensitive to say that 80% of the work is cleaning and only 20% is modeling. # Extract response variable i.e. HW70 Height for age standard deviation (according to WHO) y = dat$HW70 y[y == 9996] = NA y[y == 9997] = NA y[y == 9998] = NA y[y == 9999] = NA # Revert tranformation (i.e. z-score) y = y/100 # Variable 1: The calculated months of breastfeeding gives the duration of breastfeeding x1 = dat$M5 x1[x1 == 94] = 0 x1[x1 == 97] = NA x1[x1 == 98] = NA x1[x1 == 99] = NA x1[x1 &gt; 40] = NA # Variable 2: Age in months of the child x2 = dat$HW1 # Variable 3: Age of the mother at birth x3 = dat$V012 - dat$B8 x3[x3&gt;45] = NA # Variable 4: Body mass index (BMI) of the mother x4 = dat$V445 x4 = x4/100 # no sense without this division # Variable 5: Height of the mother in meters x5 = dat$V438 x5[x5 == 9998] = NA x5[x5 == 9999] = NA x5[x5 &lt; 1300] = NA x5[x5 &gt; 1900] = NA x5 = x5/1000 # it was in mm, we need to transform from original # Variable 6: Weight of the mother in kilograms x6 = dat$V437 x6=x6/10 # we need to go back to Kg # Variable 7: De facto region of residence # Creating dummies (i.e. indicator functions) for each level of an existing factor enables # to check the coefficients of each level in a possible future model estimation x7 = as.factor(dat$V101) x7 = model.matrix(~x7-1) dim(x7) # Variable 8: Mother highest education level attended x8 = as.factor(dat$V106) x8 = model.matrix(~x8-1) dim(x8) # Variable 9: Wealth index factor score x9 = dat$V191 # Variable 10: Weight of child at birth given in kilograms with three implied decimal places x10 = dat$M19 x10[x10 == 9996] = NA x10[x10 == 9997] = NA x10[x10 == 9998] = NA x10[x10 == 9999] = NA x10 = x10/1000 # Variable 11: Child Sex x11 = dat$B4 # Variable 12: Preceding birth interval is calculated as the difference in months between the current birth and the previous birth x12 = dat$B11 x12[x12 &gt; 125] = NA # Variable 13: Drinking Water x13 = dat$V113 x13 = model.matrix(~x13-1) x13 = x13[,c(2,3,4,8,9,13,17,18)] dim(x13) levels(x13) mat.sys = na.omit(cbind(y,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13)) dim(mat.sys)[2] # Number of regressor p = dim(mat.sys)[2] # Construct X and Y y = mat.sys[,1] X = mat.sys[,2:p] # Create a dataframe data_zambia = cbind(y,X) data_zambia = data.frame(data_zambia) Associate proper names to each variable (hint: look at the previous comments in the r chunk). colnames(data_zambia) = c(&quot;Height for age sd&quot;, &quot;Breastfeeding duration (months)&quot;,&quot;Age of the child (months)&quot;, &quot;Age of the mother (years)&quot;, &quot;BMI mother&quot;, &quot;Heigth mother (meter)&quot;, &quot;Weight mother (kg)&quot;, &quot;Region:Central&quot;, &quot;Region:Copperbelt&quot;, &quot;Region:Eastern&quot;, &quot;Region:Luapula&quot;, &quot;Region:Lusaka&quot;, &quot;Region:Northern&quot;, &quot;Region:Northwestern&quot;, &quot;Region:Southern&quot;, &quot;Region:Western&quot;, &quot;Ed:No education&quot;, &quot;Ed:Primary&quot;, &quot;Ed:Secondary&quot;, &quot;Ed:Higher&quot;, &quot;Wealth index factor score&quot;, &quot;Child weight at birth (kg)&quot;, &quot;Child sex&quot;, &quot;Interval between births&quot;,&quot;Water:Piped into dwelling&quot;, &quot;Water:Piped to yard/plot&quot;, &quot;Water:Public tap/standpipe&quot;, &quot;Water:Protected well&quot;, &quot;Water:Unprotected well&quot;, &quot;Water:River/dam/lake/ponds/stream/canal/irrigation channel&quot;, &quot;Water:Bottled water&quot;, &quot;Water:Other&quot;) Perform a linear regression on all the available variables. attach(data_zambia) lm_zambia = lm(`Height for age sd` ~ . -`Region:Central`- `Ed:No education`, data = data_zambia) # We take off two levels to avoid multicollinearity. This should always be done when you create dummies. summary(lm_zambia) # read the output understand the benchmark of the factor lm_zambia_full = lm(`Height for age sd` ~ . , data = data_zambia) summary(lm_zambia_full) #here it is R who choses the benchmark for the factors (i.e. NA variables) detach(data_zambia) Reduce the number of covariates (e.g. using the t-test) and add some interactions. Perform a linear regression on the new dataset. attach(data_zambia) # Eliminate variables with t-test in a stepwise manner (fixed alfa = 0.05 in this case) model_zambia_reduced = lm(`Height for age sd` ~ ., data = data_zambia[,c(1:2,4,9:16,21:23)]) summary(model_zambia_reduced) # notice what is happening to the age of the mother variable # Introduce one interaction in the reduced model. We start with the childsex factor. model_zambia_int = lm(`Height for age sd` ~ . + `Breastfeeding duration (months)`*`Child sex`, data = data_zambia[,c(1:2,4,9:16,21:23)]) summary(model_zambia_int) #We take out the interaction from the model as it is not significant #### Remember: the hierarchical effect states that anytime you add an interaction also the marginal effects #### should be part of your model detach(data_zambia) Other available procedures for a first model selection in this specific case: # (1) VIF (variance inflation factor) for avoiding multicollinearity, # (2) Automatic Stepwise procedures (e.g. forward and backward) # (3) Exhaustive search (See practical 3 exercises) # Example with an automatic stepwise procedure help(&quot;step&quot;) stepwise_procedue = step(lm_zambia_full,direction = &quot;backward&quot;) #or forward # This procedure evaluates, given a criterion, a sequence of variables stopping when # the criterion is increasing Analyse your chosen estimated model with a residual analysis (e.g. residuals vs fitted plot, normal QQ plot etc.). # Validate your model looking at residuals vs fitted plot and normal QQ plot plot(model_zambia_reduced, which = 1) # Residuals vs fitted: no particular structure plot(model_zambia_reduced, which = 2) # Normal QQ plot: We observe right tail which is not compatible with a normal assumption 7.1.2 Prognostic Factors in Childhood Leukemia Exercises Load the data from the URL http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv leukemia_big &lt;- read.csv(&quot;http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv&quot;) Create the response variable y according to the number of ALL and AML patients. In the same fashion create the matrix X of independent variables. See https://web.stanford.edu/~hastie/CASI_files/DATA/leukemia.html for further details. leukemia_mat = as.matrix(leukemia_big) dim(leukemia_mat) leukemia_mat = t(leukemia_mat) #this is the design matrix for the analysis # Generate the 0 and 1 values for the two different categories: there are 20 ALL, 14 AML, 27 ALL and # 11 AML for a total of 47 ALL and 25 AML. # Given the above excerpt from the cancer society, I have decided to code ALL as 1 and AML as 0 since # doctors are interested in knowing the characteristics which differentiate ALL from AML in order to # understand if we can use standard treatment or a more aggressive one. y = c(rep(1,20),rep(0,14), rep(1,27), rep(0,11)) #the response vector length(y) X = leukemia_mat dim(X) Choose the correct exponential family for this situation and perform a GLM on the data. Comment on the results that you obtain. model_glm = glm(formula = y ~ X,family = &quot;binomial&quot;) summary(model_glm) #singularity issues in the IWLS algorithm of GLM. It is impossible to invert the matrix. # The binary Lasso is a possible way to solve the issue and have an actual estimate. See glmnet package. 7.2 Chapter 2 7.2.1 Cross-validation (solutions provided by Alexander Maslev, Hanxiong Wang and Minyoung Lee). Program k-fold Cross-Validation (with k=2) and do model selection in a specific simulation setting with an exhaustive search. Follow these steps: Generate from a MVN (multivariate normal) a matrix \\(\\mathbf{X_{n*p}}\\) with \\(n = 1000\\) and \\(p = 5\\). You can choose the location vector as you wish but set the scale matrix as the identity. We have chosen the location vector [2,4,6,8,10] and the scale matrix as the identity. n&lt;-1000 p&lt;-5 Mu&lt;-c(2,4,6,8,10) # location vector sigma&lt;-diag(5) # scale matrix as the identity X&lt;-mvrnorm(n , Mu, sigma) Choose the generating vector \\(\\boldsymbol{\\beta }= [3 \\; 1.5 \\; 0 \\; 2 \\; 0]\\) and retrieve the signal to noise ratio of this setting. We found in Wikipedia that the statistical definition of SNR is the reciprocal of the coefficient of variation (i.e. the ratio of mean to standard deviation of a signal or measurement) : \\[SNR = \\frac{\\mu}{\\sigma}\\] where \\(\\mu\\) is the signal mean or expected value and \\(\\sigma\\) is the standard deviation of the noise. beta&lt;-c(3,1.5,0,2,0) e&lt;-rnorm(n, mean = 0, sd = 1) SNR&lt;-mean(X%*%beta)/sqrt(var(e)) SNR ## [1] 27.90095 In the engineering literature, there is an alternative definition: \\[SNR_{eng} = \\frac{Var(f(x))}{Var(\\epsilon)}\\] where \\(f(x)\\) is the chosen prediction rule (e.g. linear function in the OLS case) and \\(\\epsilon\\) is the noise. You can fix in advance your SNR with the following code and generate the data accordingly: signal_to_noise_ratio = #number to fix as you wish# data = X%*%beta noise = rnorm(n,0,1) k = sqrt(var(data)/(signal_to_noise_ratio*var(noise))) y_hat_eng = data + k*noise # how you generate data to retrieve your fixed signal to noise ratio Generate \\(\\hat{\\mathbf{y}}\\) thanks to the relation \\(\\mathbf{y} = \\mathbf{X_{n*p}} \\; \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) where \\(\\epsilon_{i}\\) is a standard normal, \\(n = 1000\\) and \\(p = 5\\). Suppose for simplicity that the errors are uncorrelated. Y_hat&lt;-X%*%beta+e Split the data randomly in two halves (k=2) and use the training set to determine \\(\\hat{\\boldsymbol{\\beta}}\\). Then, compute the squared loss function as prediction error measure for each possible model. Observe which model is the best model. index &lt;- sample(1:n, size=0.5*n) # Split data y_train&lt;- Y_hat[-index,] x_train&lt;-X[-index,] y_test&lt;- Y_hat[index,] x_test&lt;-X[index,] index_sub_choose&lt;-c(1:p) sub_matrix &lt;- matrix(data = NA,ncol = p,nrow = 2^p-1) t=0 for(i in 1:5) { index_matrix &lt;- combn(index_sub_choose,i) for(j in 1:ncol(index_matrix)) { t &lt;- t+1 index_sub &lt;- index_matrix[,j] sub_matrix[t,c(index_sub)] &lt;- 1 } } k&lt;-nrow(sub_matrix) cv &lt;- matrix(data=NA,nrow = k,ncol = 1) for(j in 1:k){ Xsub &lt;-x_train[,which(sub_matrix[j,]==1)] betaMLE &lt;-solve(t(Xsub)%*%Xsub)%*%t(Xsub)%*%y_train new_Y &lt;-x_test[,which(sub_matrix[j,]==1)]%*%betaMLE cv[j,] &lt;- t(y_test-new_Y)%*%(y_test-new_Y) } BEST_cv&lt;-which(sub_matrix[which.min(cv),]==1) BEST_cv ## [1] 1 2 4 5 Xsub_cv&lt;-x_train[,BEST_cv] betaMLE_cv&lt;-solve(t(Xsub_cv)%*%Xsub_cv)%*%t(Xsub_cv)%*%y_train betaMLE_cv ## [,1] ## [1,] 3.03433961 ## [2,] 1.51075393 ## [3,] 2.01760617 ## [4,] -0.02911818 Each time it changes but most of the times we obtain the best model when p=5. The beta sometimes is not close to generating vector \\(\\beta\\) = [3 1.5 0 2 0]. Suppose now that we increase the size of \\(\\boldsymbol{\\beta}\\) to 100 (i.e. \\(p = 100\\) ). Calculate the number of possible models to evaluate together with an estimate of the time needed for an exhaustive search (hint: use previous results). Conclude on the feasibility of this task. ## [1] 1.267651e+30 ## [1] 6.915153e+27 When we run a CV process with p=5, it takes 0.1691079 seconds. For p = 5, we can have 31 different models thanks to Newton’s binomial theorem (i.e. \\(2^p - 1\\)). Unfortunately when we increase p to 100, we have 1.26e+30 different models. Thus it will take approximately 1.9e+24 hours. This is the case when we do k=2 cross validation. Moreover if we increase the number of k, we will drastically increase the time needed. This task is not feasible. 7.2.2 Akaike Information Criterion (solutions provided by Alexander Maslev, Hanxiong Wang and Minyoung Lee). Program AIC and do model selection in a specific simulation setting with an exhaustive search (follow the passages listed in the CV exercise section). n&lt;-1000 p&lt;-5 Mu&lt;-c(2,4,6,8,10) # location vector sigma&lt;-diag(5) # scale matrix as the identity X&lt;-mvrnorm(n , Mu, sigma) beta&lt;-c(3,1.5,0,2,0) e&lt;-rnorm(n, mean = 0, sd = 1) SNR&lt;-mean(X%*%beta)/var(e) Y_hat&lt;-X%*%beta+e index_sub_choose&lt;-c(1,2,3,4,5) sub_matrix &lt;- matrix(data = NA,ncol = 5,nrow = 31) t=0 for(i in 1:5) { index_matrix &lt;- combn(index_sub_choose,i) for(j in 1:ncol(index_matrix)) { t &lt;- t+1 index_sub &lt;- index_matrix[,j] sub_matrix[t,c(index_sub)] &lt;- 1 } } # AIC RSS&lt;-rep(0,k) AIC&lt;-rep(0,k) k&lt;-nrow(sub_matrix) for(j in 1:k){ Xsub&lt;-as.matrix(X[,which(sub_matrix[j,]==1)]) betaMLE&lt;-solve(t(Xsub)%*%Xsub)%*%t(Xsub)%*%Y_hat new_Y&lt;-Xsub%*%betaMLE for(i in 1:(n/2)){ RSS[j]&lt;-RSS[j]+(new_Y[i]-Y_hat[i])^2 } AIC[j]&lt;-RSS[j]/var(e)+2*ncol(Xsub) } BEST&lt;-which(sub_matrix[which.min(AIC),]==1) BEST ## [1] 1 2 4 Xsub&lt;-as.matrix(X[,BEST]) betaMLE&lt;-solve(t(Xsub)%*%Xsub)%*%t(Xsub)%*%Y_hat betaMLE ## [,1] ## [1,] 2.982799 ## [2,] 1.440587 ## [3,] 2.035385 Each time it changes but most of the times we obtain the best model when p=4 in position[1 2 4 5]. The estimated beta is very close to generating vector \\(\\beta\\) = [3 1.5 0 2 0]. The above results hold not considering \\(\\sigma\\) and the intercept as parameters. In the general formulation of the Akaike’s information criterion, \\(p = dim(\\Theta)\\) where \\(\\Theta\\) is the parameters space. As we computed in point (e) in CV exercise, when p = 100, we have 1.26e+30 different model. The time needed to run the AIC process with p = 5 takes 0.2441621 seconds. When we calculate the time that we need to run when p = 100, we obtain approximately 2.7e+24 hours. Thus we still think that the task to do all the combinations when p = 100 is not feasible. ## [1] 1.267651e+30 ## [1] 9.984266e+27 ## [1] 2.773407e+24 Compare the performance of your programmed CV and AIC by replicating 100 times the tasks. In particular you should evaluate three specific criteria: the proportion of times the correct model is selected (Exact), the proportion of times the selected model contains the correct one (Correct) and the average number of selected regressors (Average \\(\\sharp\\)) ## Rep 100 times AIC cv &lt;- matrix(data=NA,nrow = k,ncol = 1) Exact&lt;-data.frame(t(c(0,0))) colnames(Exact)&lt;-c(&quot;AIC&quot;,&quot;CV&quot;) AverageN&lt;-matrix(NA,nrow=100,ncol = 2) colnames(AverageN)&lt;-c(&quot;AIC&quot;,&quot;CV&quot;) Correct&lt;-data.frame(t(c(0,0))) colnames(Correct)&lt;-c(&quot;AIC&quot;,&quot;CV&quot;) for(l in 1:100){ ### SETTING ### n&lt;-1000 p&lt;-5 Mu&lt;-c(2,4,6,8,10) # location vector sigma&lt;-diag(5) # scale matrix as the identity X&lt;-mvrnorm(n , Mu, sigma) beta&lt;-c(3,1.5,0,2,0) e&lt;-rnorm(n, mean = 0, sd = 1) Y_hat&lt;-X%*%beta+e ### AIC ### RSS&lt;-rep(0,k) AIC&lt;-rep(0,k) for(j in 1:k){ Xsub&lt;-as.matrix(X[,which(sub_matrix[j,]==1)]) betaMLE&lt;-solve(t(Xsub)%*%Xsub)%*%t(Xsub)%*%Y_hat new_Y&lt;-Xsub%*%betaMLE for(i in 1:(n/2)){ RSS[j]&lt;-RSS[j]+(new_Y[i]-Y_hat[i])^2 } AIC[j]&lt;-RSS[j]/var(e)+2*ncol(Xsub) } BEST&lt;-sub_matrix[which.min(AIC),] BEST[is.na(BEST)] &lt;-0 if(sum(BEST-c(1,1,0,1,0))==0){ Exact[1]&lt;-Exact[1]+1 } if(sum((BEST[c(1,2,4)]-c(1,1,1)))==0){ Correct[1]&lt;-Correct[1]+1 } AverageN[l,1]&lt;-sum(BEST) ### CV ### index &lt;- sample(1:n, size=0.5*n) y_train&lt;- Y_hat[-index,] x_train&lt;-X[-index,] y_test&lt;- Y_hat[index,] x_test&lt;-X[index,] for(j in 1:k){ Xsub &lt;-x_train[,which(sub_matrix[j,]==1)] betaMLE &lt;-solve(t(Xsub)%*%Xsub)%*%t(Xsub)%*%y_train new_Y &lt;-x_test[,which(sub_matrix[j,]==1)]%*%betaMLE cv[j,] &lt;- t(y_test-new_Y)%*%(y_test-new_Y) } BEST_cv&lt;-sub_matrix[which.min(cv),] BEST_cv[is.na(BEST_cv)] &lt;-0 if(sum(BEST_cv-c(1,1,0,1,0))==0){ Exact[2]&lt;-Exact[2]+1 } if(sum((BEST_cv[c(1,2,4)]-c(1,1,1)))==0){ Correct[2]&lt;-Correct[2]+1 } AverageN[l,2]&lt;-sum(BEST_cv) } Exact/100 ## AIC CV ## 1 0.84 0.47 Correct/100 ## AIC CV ## 1 1 1 colMeans(AverageN) ## AIC CV ## 3.20 3.63 In this part, we have simulated the AIC process and the CV process for 100 times and evaluated the three criteria. We have found that AIC gives better results than CV in terms of Exact models and it has a lower number of selected regressors on average. Both methods achieves model selection consistency in this simple setting as the proportion of correct models reaches the value of 1. This result is expected since AIC is derived from the likelihood which inherits all the nice properties (e.g. Cramer-Rao bound etc.) when the model is correct. On the other hand, CV is a non parametric method thus inferior by definition to the AIC in this setting. However in a real application our conjectured model maybe far from the truth and CV could be a better choice. In the same simulation setting outlined in the CV exercise section, generate from a MVN (multivariate normal) a matrix \\(\\mathbf{X_{n*p}}\\) with \\(n = 1000\\) and \\(p = 5\\) but now fix the scale matrix with an autoregressive form \\(\\boldsymbol{\\Sigma}=[\\sigma_{lm}]_{l,m=1,\\ldots,p}\\) with \\(\\sigma_{lm} = \\rho^{\\mid l - m\\mid}\\). Compare the performance of CV and AIC for \\(\\boldsymbol{\\rho} = [0.2 \\; 0.5\\; 0.7]\\) (\\(\\rho = 0\\) corresponds to the identity case that you have already treated). There are different sources of randomness: X matrix, epsilon (error), y (induced) and the split for CV.We need to choose what to preserve: we replicate every time only the randomness of epsilon and of course the split of the CV thus we need to set a seed for the X matrix. All this discussion on randomness is pivotal for a significant and controlled simulation setting. require(MASS) set.seed(5) # For the X matrix n = 1000 p = 5 rho = 0.5 # change rho as you please to inspect other correlations among the predictors mu = rep(1,p) sigma = rep(0,p^2) sigma = matrix(data = sigma, ncol = p,nrow = p) # Autoregressive structure for (i in 1:p) { for (j in 1:p) { sigma[i,j] = rho^(abs(i-j)) } } X = mvrnorm(n,mu,sigma) # beta = c(0,0,1,0,0) alternative beta to check bias properties beta = c(3,1.5,0,2,0) ind = 1:1000 pos_CV = rep(0,100) pos_AIC = rep(0,100) for(z in 1:100) { y_hat = X%*%beta + rnorm(n,0,1) data = data.frame(cbind(y_hat,X)) colnames(data) = c(&quot;y&quot;,&quot;x1&quot;,&quot;x2&quot;,&quot;x3&quot;,&quot;x4&quot;,&quot;x5&quot;) index = sample(x = ind,size = 500,replace = F) train_set = data[index,] test_set = data[-index,] cv_error = rep(0,31) aic_values = rep(0,31) # One regressor only models for(i in 1:5) { m_1 = lm(train_set$y ~ train_set[,i+1], data = train_set) cv_error[i] = mean((test_set$y - (cbind(rep(1,500),test_set[,i+1])%*% m_1$coefficients))^2) m_1 = lm(data$y ~ data[,i+1], data) aic_values[i] = -2*logLik(m_1) + 2*3 } # Two regressors models x = c(1,2,3,4,5) M = combn(x,2) for(i in 1:10) { m_2 = lm(train_set$y ~ train_set[,M[1,i]+1] + train_set[,M[2,i]+1], data = train_set) cv_error[i+5] = mean((test_set$y - cbind(rep(1,500),test_set[,M[1,i]+1], test_set[, M[2,i] +1])%*%m_2$coefficients)^2) m_2 = lm(data$y ~ data[,M[1,i]+1] + data[,M[2,i]+1], data) aic_values[i+5] = -2*logLik(m_2) + 2*4 } # Three regressors models M = combn(x,3) for(i in 1:10) { m_3 = lm(train_set$y ~ train_set[,M[1,i]+1] + train_set[,M[2,i]+1] + train_set[,M[3,i]+1], data = train_set) cv_error[i+ 15] = mean((test_set$y - cbind(rep(1,500),test_set[,M[1,i]+1], test_set[, M[2,i] +1],test_set[, M[3,i]+1])%*%m_3$coefficients)^2) m_3 = lm(data$y ~ data[,M[1,i]+1] + data[,M[2,i]+1] + data[,M[3,i]+1], data) aic_values[i+15] = -2*logLik(m_3) + 2*5 } # Four regressors models M = combn(x,4) for(i in 1:5) { m_4 = lm(train_set$y ~ train_set[,M[1,i]+1] + train_set[,M[2,i]+1] + train_set[,M[3,i]+1] + train_set[,M[4,i]+1], data = train_set) cv_error[i+25] = mean((test_set$y - cbind(rep(1,500),test_set[,M[1,i]+1], test_set[, M[2,i] +1],test_set[, M[3,i]+1],test_set[, M[3,i]+1])%*% m_4$coefficients)^2) m_4 = lm(data$y ~ data[,M[1,i]+1] + data[,M[2,i]+1] + data[,M[3,i]+1] + data[,M[4,i]+1], data) aic_values[i+25] = -2*logLik(m_4) + 2*6 } # Full model full_model = lm(train_set$y ~ ., data = train_set) full_model$coefficients cv_error_full = mean((test_set$y - cbind(rep(1,500), test_set$x1,test_set$x2,test_set$x3,test_set$x4,test_set$x5)%*% full_model$coefficients)^2) cv_error[31] = cv_error_full full_model = lm(data$y ~ ., data) aic_values[31] = -2*logLik(full_model) + 2*7 pos_CV[z] = which.min(cv_error) pos_AIC[z] = which.min(aic_values) } # We know that the right position should be 17 which is the exact model. # Exact model proportion sum(pos_CV == &quot;17&quot;)/100 ## [1] 0.48 sum(pos_AIC == &quot;17&quot;)/100 ## [1] 0.71 # Correct model proportion (linked to the consistency of the model selection procedure) sum(pos_CV &gt; 16)/100 ## [1] 1 sum(pos_AIC &gt; 16)/100 ## [1] 1 # Average number of regressors counted = c(rep(1,5),rep(2,10), rep(3,10), rep(4,5),5) result_cv = rep(0,100) result_aic = rep(0,100) for (i in 1:100) { result_cv[i] = counted[pos_CV[i]] result_aic[i] = counted[pos_AIC[i]] } # CV CASE mean(result_cv) ## [1] 3.8 # AIC case mean(result_aic) ## [1] 3.3 CONCLUSIONS: correlation does not play a role here because it biases our estimates but not that much that is necessary to make AIC (less so CV) fails to recognize the significant regressors. This holds especially in the presence of strong signal so for a beta with important components. The transition chain is: we create correlated X, we transmit correlation to the ys, the likelihood theory is not optimal anymore (y should be independent, X fixed) and I get biased estimates. However I need a really important bias to make AIC fails to recognize significant variables in the linear case. The GLM case is different: correlation is a real issue because of the link function. Even a small bias in that direction can alter the order of variables and thus AIC results. Upload the Zambia dataset and perform an exhaustive search on the continuous covariates (i.e. avoiding factors) based on CV and AIC in order to find the best model. You can either employ your codes derived in previous exercises or make use of the existing R packages: leaps, glmulti, MuMIn and caret. # Load Zambia dataset load(file = &quot;your_directory/data_zambia.Rda&quot;&quot;) data_zambia = data_zambia[,c(1:7,21,22,24)] #exclude the factors from the analysis ### Exhaustive search with leaps (AIC case) ### require(leaps) regsubsets.out &lt;- regsubsets(data_zambia$`Height for age sd` ~ .,data = data_zambia,nbest = 1, nvmax = NULL, # NULL for no limit on number of variables force.in = NULL, force.out = NULL, method = &quot;exhaustive&quot;) summary(regsubsets.out) plot(regsubsets.out) # BIC is default plot(regsubsets.out,scale = &quot;Cp&quot;) #C_p case which is equal to AIC with a linear model ### Exhaustive search with glmulti (AIC case) ### glmulti.lm.out &lt;- glmulti::glmulti(data_zambia$`Height for age sd` ~ .,data = data_zambia, level = 1, # No interaction considered method = &quot;h&quot;, # Exhaustive approach crit = &quot;aic&quot;, # AIC as criteria confsetsize = 5, # Keep 5 best models plotty = F, report = F, # No plot or interim reports fitfunction = &quot;lm&quot;) # lm function ### Exhaustive search with MumIn (AIC case) ### require(MuMIn) data_model &lt;- lm(data_zambia$`Height for age sd` ~ .,data = data_zambia) combinations &lt;- dredge(data_model) print(combinations) ### Exhaustive search with caret (CV case and AIC case) ### require(caret) attach(data_zambia) # Unfortunately there is no exhaustive search based on CV in Caret, it is just stepwise. #setting up 10-fold cross-validation control &lt;- trainControl(method=&quot;cv&quot;, number=10) #finding the ideal model by AIC criterion model_AIC = train(`Height for age sd`~.,data=data_zambia,method=&quot;lmStepAIC&quot;,trControl=control) #finding the ideal model by mean square error modelCV = train(`Height for age sd`~.,data=data_zambia,method=&quot;lm&quot;,trControl=control) detach(data_zambia) # In order to do a full exhaustive search with CV, we should exploit the codes produced in CV exercise. Of course # as the number of variables increase, the task becomes impossible. 7.2.3 ROC curves Read the information on the Leukemia Dataset in the first chapter of the book. Then load the Leukemia dataset reduced which contains a subset of 11 eleven predictors among the 3571 present in the leukemia_small.csv. These variables have been selected, because of their importance, by the binary lasso which is a shrinkage method that will be discussed later on in the course. Now perform the following steps: Fit the appropriate GLM for the situation using only one of the available predictors (e.g. V457) load(&quot;data_leukemia_reduced.Rda&quot;) ## Part (a) mod_1 = glm(y ~ V457, data = data_leukemia_reduced,family = &quot;binomial&quot;) Read the ROC curve section of the e-book. Then find the TPR (i.e. true positive rate), FPR (i.e. false positive rate), TNR (i.e. true negative rate), FNR (i.e. false negative rate) of the fitted values found at point (a) with a cut-off value \\(c = 0.5\\). # Confusion matrix c0 = 0.5 mod_1 = glm(y ~ V457, data = data_leukemia_reduced,family = &quot;binomial&quot;) conf_mat = table(data_leukemia_reduced$y, mod_1$fitted.values &gt; c0) #unbalanced sample 47 ones and 25 0s conf_mat ## ## FALSE TRUE ## 0 22 3 ## 1 3 44 prop_confmat = prop.table(conf_mat, 1) # For ROC curve is on the line prop_confmat ## ## FALSE TRUE ## 0 0.88000000 0.12000000 ## 1 0.06382979 0.93617021 TNR = conf_mat[1,1]/(conf_mat[1,1] + conf_mat[1,2]) # TRUE NEGATIVE (TNR) FPR = conf_mat[1,2]/(conf_mat[1,1] + conf_mat[1,2]) # FALSE POSITIVE (FPR) FNR = conf_mat[2,1]/(conf_mat[2,1] + conf_mat[2,2]) # FALSE NEGATIVE (FNR) TPR = conf_mat[2,2]/(conf_mat[2,1] + conf_mat[2,2]) # TRUE POSITIVE (TPR) # Also possible with caret but with train and test arguments. Not full dataset inquiry as in this case. # require(caret) # confusionMatrix() For a given cut-off grid of values, that you can choose as you wish, plot the ROC curve relative to the estimated model. c0 = seq(0.1,0.9,0.1) TP = rep(0,length(c0)) FP = rep(0,length(c0)) mod_1 = glm(y ~ V457, data = data_leukemia_reduced,family = &quot;binomial&quot;) for (i in 1:length(c0)) { conf_mat = table(data_leukemia_reduced$y, mod_1$fitted.values &gt; c0[i]) #unbalanced sample 47 ones and 25 0s prop_confmat = prop.table(conf_mat, 1) # For ROC curve is on the columns TP[i] = prop_confmat[2,2] FP[i] = prop_confmat[1,2] } FP = sort(FP) # for a correct line TP = sort(TP) # for a correct line # Every ROC curve passes towards (0,0) and (1,1) TP = c(0,TP,1) FP = c(0,FP,1) plot(FP,TP, col=&quot;green&quot;,main = &quot;ROC Curve&quot;,xlab = &quot;1 - Specificity&quot;,ylab = &quot;Sensitivity&quot;,xlim=c(0, 1), ylim=c(0,1),type = &quot;l&quot;) lines(c(0, 1), c(0, 1), type=&#39;l&#39;,col=&quot;red&quot;) Check the quality of your result at point (c) with the R package pROC. # install.packages(&quot;pROC&quot;) require(pROC) ## Loading required package: pROC ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var roc_1 = roc(response=data_leukemia_reduced$y, predictor = mod_1$fitted.values) plot(roc_1) 7.3 Chapter 3 7.3.1 Selection by Hypothesis Testing First of all we retrieve the simulation setting used in Practical 3. # install.packages(&quot;selectiveInference&quot;) require(selectiveInference) require(MASS) # Create the simulation setting set.seed(11) n = 1000 p = 10 # change rho as you please to inspect other correlations among the predictors rho = 0 mu = rep(1,p) sigma = rep(0,p^2) sigma = matrix(data = sigma, ncol = p,nrow = p) # Autoregressive structure for (i in 1:p) { for (j in 1:p) { sigma[i,j] = rho^(abs(i-j)) } } X = mvrnorm(n,mu,sigma) beta = c(3,1.5,0,2,rep(0,6)) y = X%*%beta + rnorm(n,0,1) #for us sigma is 1 Now, after having read the documentation of the R package selectiveInference and installed it, perform the following steps: a) Use the functions fs(), fsInf() and forwardStop() to do a stepwise regression based on partial correlations and a model selection phase with the ForwardStop rule on your generated data. Try different values for the type one error: how does the choice of \\(\\alpha\\) impact the model selection technique? ## Part a ## fsfit = fs(X,y) #partial correlations like OMP or stagewise fsfit$action #order of regressors ## [1] 1 4 2 10 9 3 8 7 6 5 fsfit$beta #the beta estimation at each step ## 1 2 3 4 5 6 7 ## [1,] 0 3.047019 3.014175 2.965561 2.96179608 2.96371055 2.96277233 ## [2,] 0 0.000000 0.000000 1.486185 1.48523930 1.48682723 1.48759323 ## [3,] 0 0.000000 0.000000 0.000000 0.00000000 0.00000000 0.04017513 ## [4,] 0 0.000000 2.061130 2.020475 2.02252158 2.02605822 2.02658398 ## [5,] 0 0.000000 0.000000 0.000000 0.00000000 0.00000000 0.00000000 ## [6,] 0 0.000000 0.000000 0.000000 0.00000000 0.00000000 0.00000000 ## [7,] 0 0.000000 0.000000 0.000000 0.00000000 0.00000000 0.00000000 ## [8,] 0 0.000000 0.000000 0.000000 0.00000000 0.00000000 0.00000000 ## [9,] 0 0.000000 0.000000 0.000000 0.00000000 0.07326696 0.07269192 ## [10,] 0 0.000000 0.000000 0.000000 -0.07675033 -0.07526561 -0.07532284 ## 8 9 10 ## [1,] 2.96311034 2.96286548 2.96191272 ## [2,] 1.48716658 1.48859575 1.48770072 ## [3,] 0.03987444 0.03973915 0.04053624 ## [4,] 2.02693788 2.02776730 2.02659795 ## [5,] 0.00000000 0.00000000 0.00000000 ## [6,] 0.00000000 0.00000000 0.01967913 ## [7,] 0.00000000 -0.02997558 -0.02864951 ## [8,] -0.03210415 -0.03298970 -0.03201230 ## [9,] 0.07351783 0.07405473 0.07443275 ## [10,] -0.07361395 -0.07454985 -0.07369007 # compute sequential p-values and confidence intervals (sigma estimated from full model) out = fsInf(fsfit,alpha = 0.1) #default out # the value of forward stop is the last regressor which is active ## ## Call: ## fsInf(obj = fsfit, alpha = 0.1) ## ## Standard deviation of noise (specified or estimated) sigma = 0.997 ## ## Sequential testing results with alpha = 0.100 ## Step Var Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea ## 1 1 3.047 97.561 0.000 2.995 3.099 0.048 0.048 ## 2 4 2.061 65.227 0.000 2.009 2.113 0.049 0.049 ## 3 2 1.486 47.535 0.000 1.435 1.538 0.049 0.049 ## 4 10 -0.077 -2.420 0.834 -0.077 1.345 0.050 0.050 ## 5 9 0.073 2.306 0.019 0.052 1.527 0.050 0.050 ## 6 3 0.040 1.267 0.567 -0.334 0.193 0.050 0.050 ## 7 8 -0.032 -1.003 0.660 -0.368 0.857 0.050 0.050 ## 8 7 -0.030 -0.922 0.261 -0.906 0.291 0.050 0.050 ## 9 6 0.020 0.614 0.621 -0.694 0.377 0.050 0.050 ## 10 5 0.016 0.482 0.223 -0.175 0.737 0.050 0.050 ## ## Estimated stopping point from ForwardStop rule = 3 # estimate optimal stopping point last = forwardStop(out$pv, alpha=0.4) # you want to reject more often fixing higher alfa stop_point = forwardStop(out$pv,alpha = 0.85) fsfit$action[1:last] # to catch right components ## [1] 1 4 2 10 9 # Conclusions: fixing a higher alfa will let you include on average more regressors as you will reject more often H0 the null hypothesis. Given the order of variables produced by fs(), use AIC and BIC criteria for model selection to retrieve your final model (Hint: you do not need to program them, use an existing function of the selectiveInference package). ## Part b ## # We use the same fit, the k step is decided by AIC penalty out_2 = fsInf(fsfit,type = &quot;aic&quot;,alpha = 0.05) #akaike case, the fixed value of alfa is 0.1 out_2 #be careful check with classic AIC evaluation (sometimes is not correct) ## ## Call: ## fsInf(obj = fsfit, alpha = 0.05, type = &quot;aic&quot;) ## ## Standard deviation of noise (specified or estimated) sigma = 0.997 ## ## Testing results at step = 5, with alpha = 0.050 ## Var Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea ## 1 2.964 94.680 0.000 2.903 3.103 0.025 0.025 ## 4 2.026 63.996 0.000 1.964 2.091 0.024 0.025 ## 2 1.487 47.540 0.000 1.425 1.563 0.025 0.024 ## 10 -0.075 -2.373 0.839 -0.099 1.711 0.025 0.025 ## 9 0.073 2.306 0.024 0.001 1.863 0.025 0.025 ## ## Estimated stopping point from AIC rule = 5 # We have already an idea that AIC tends to overfit ### YOU CAN PROGRAM AIC USING ORDER IN fsfit$action ### # We use the same fit, the k step is decided by BIC penalty out_3 = fsInf(fsfit, type = &quot;aic&quot;, mult = log(n)) #bic case out_3 ## ## Call: ## fsInf(obj = fsfit, type = &quot;aic&quot;, mult = log(n)) ## ## Standard deviation of noise (specified or estimated) sigma = 0.997 ## ## Testing results at step = 3, with alpha = 0.100 ## Var Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea ## 1 2.966 94.889 0 2.914 3.018 0.048 0.048 ## 4 2.020 63.917 0 1.968 2.073 0.050 0.049 ## 2 1.486 47.535 0 1.435 1.538 0.049 0.049 ## ## Estimated stopping point from AIC rule = 3 # Another nice evidence, BIC has harsh penalty than AIC. # Other possibilities of fsInf() function out_4 = fsInf(fsfit,type = &quot;all&quot;,k = 4,mult = log(n)) # this is just to stop at a fixed number of steps out_4 ## ## Call: ## fsInf(obj = fsfit, k = 4, type = &quot;all&quot;, mult = log(n)) ## ## Standard deviation of noise (specified or estimated) sigma = 0.997 ## ## Testing results at step = 4, with alpha = 0.100 ## Var Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea ## 1 2.962 94.652 0.000 2.910 3.040 0.048 0.049 ## 4 2.023 63.959 0.000 1.967 2.075 0.050 0.049 ## 2 1.485 47.501 0.000 1.433 1.537 0.049 0.050 ## 10 -0.077 -2.420 0.834 -0.077 1.345 0.050 0.050 Calculate how many models are needed for an exhaustive search in this simulation setting. Use your previous results obtained in Practical 3 to understand the computational time gained by stepwise regression with respect to exhaustive search. Use the package tictoc for this comparison. ## Part c ## # Look at Practical 3, see your exhaustive search time for p=10. # In my case around 17 minutes for 1023 models (2^10 -1) since I used 1sec per model require(tictoc) tic() fsfit = fs(X,y) out = fsInf(fsfit) #default last = forwardStop(out$pv, alpha=0.05) # you want to reject more often fixing higher alfa Sys.sleep(1) toc() # 1.81 against 17 minutes... Exhaustive vs stepwise, a huge gain indeed! (Optional) Change the simulation setting outlined above to an high dimensional one: generate from a MVN (multivariate normal) a matrix \\(\\mathbf{X_{nxp}}\\) with \\(n = 100\\) and \\(p = 150\\). Evaluate the performance of the ForwardStop rule in this high dimensional setting (i.e. by replicating the model selection task 100 times) thanks to the usual three specific criteria: the proportion of times the correct model is selected (Exact), the proportion of times the selected model contains the correct one (Correct) and the average number of selected regressors (Average \\(\\sharp\\)}). What do you observe? What is the role of \\(\\alpha\\) in this case? ## Part d ## # Create the simulation setting set.seed(11) n = 100 p = 100 #p = 150 --&gt; It does not work as we can not invert this kind of matrix thus we need n &gt; or = than p # change rho as you please to inspect other correlations among the predictors rho = 0 mu = rep(1,p) sigma = rep(0,p^2) sigma = matrix(data = sigma, ncol = p,nrow = p) # Autoregressive structure for (i in 1:p) { for (j in 1:p) { sigma[i,j] = rho^(abs(i-j)) } } X = mvrnorm(n,mu,sigma) beta = c(3,1.5,0,2,rep(0,96)) y = X%*%beta + rnorm(n,0,1) #for us sigma is 1 fsfit = fs(X,y) #partial correlations like OMP or stagewise # You need to have p &lt; n to work with this kind of package. Or you can do a first screening based on # marginal correlations (like in SIS): order them and take the first n. sigma_hat = estimateSigma(X,y) # You need to estimate sigma in this big models. If not there is bad estimation by default since sigma # is estimated on the full model. # compute sequential p-values and confidence intervals out = fsInf(fsfit,sigma = 0.977) out # the value of forward stop is the last regressor which is active # estimate optimal stopping point last = forwardStop(out$pv, alpha=0.05) # you want to reject more often fixing higher alfa fsfit$action[1:last] # to catch right components # Evaluation with model selection criteria order = list() for (z in 1:100) { y = X%*%beta + rnorm(n,0,1) fsfit = fs(X,y) sigma_hat = estimateSigma(X,y) a = sigma_hat$sigmahat out = fsInf(fsfit,sigma = a) last = forwardStop(out$pv, alpha=0.05) # you want to reject more often fixing higher alfa order[[z]] = fsfit$action[1:last] } ### Exact models ### exact = rep(0,100) for (i in 1:100) { if (identical(order[[i]],c(1,4,2)) == TRUE){ exact[i] = 1 } } sum(exact)/100 ### Correct (related to consistency in model selection) ### prop_cor = rep(0,100) for (i in 1:100) { if (length(order[[i]]) &gt;= 3) { prop_cor[i] = 1 } } sum(prop_cor)/100 #Due to the low number of observations n=100 ### Average number of regressors ### # Find the number of elements in each order[[z]] num.el = sapply(order, length) sum(num.el)/100 # The role of alfa is the same as before, as you increase it you get overfitting while if you lower it # you will reject more often thus leading to a sparser model. Consider the Malnutrition in Zambia dataset. For simplicity work only on the continuous covariates (i.e. avoiding factors) and order them according to their partial correlations using the R function fs of the Selective Inference R Package (https://cran.r-project.org/web/packages/selectiveInference/index.html). Compare the selected models when using: the ForwardStop the \\(C_p\\) or AIC (equal in linear case) the BIC load(&quot;malnutrion_zambia_cleaned.Rda&quot;) data_zambia = data_zambia[,c(1:7,21,22,24)] #exclude the factors from the analysis ### selective inference package ### ## ForwardStop rule ## X = as.matrix(data_zambia[,2:10]) y = data_zambia[,1] fsfit = fs(X,y) fsfit$action #order by partial correlations ## [1] 1 5 8 7 6 3 2 9 4 # compute sequential p-values and confidence intervals # (sigma estimated from full model) out = fsInf(fsfit) # The above function is using forward stop and pvalue evaluated at each step with forwardstop # It is a formula to control FDR. out ## ## Call: ## fsInf(obj = fsfit) ## ## Standard deviation of noise (specified or estimated) sigma = 1.592 ## ## Sequential testing results with alpha = 0.100 ## Step Var Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea ## 1 1 -0.062 -12.229 0.000 -0.070 -0.053 0.049 0.050 ## 2 5 5.209 8.937 0.000 3.812 6.179 0.049 0.049 ## 3 8 0.283 4.901 0.400 -0.662 0.337 0.050 0.050 ## 4 7 0.000 4.719 0.126 0.000 0.000 0.050 0.050 ## 5 6 0.011 2.902 0.369 -0.051 0.074 0.050 0.050 ## 6 3 0.015 2.380 0.182 -0.029 0.131 0.050 0.050 ## 7 2 -0.005 -1.865 0.422 -0.029 0.026 0.050 0.050 ## 8 9 0.003 1.616 0.107 -0.002 0.025 0.050 0.050 ## 9 4 -0.081 -0.683 0.478 -0.451 0.529 0.050 0.050 ## ## Estimated stopping point from ForwardStop rule = 2 forwardStop(out$pv, alpha=.10) #only breastfeeding and height of the mother matters ## [1] 2 ## AIC and BIC penalty to decide step k ## out_2 = fsInf(fsfit,type = &quot;aic&quot;) #akaike case, the fixed value of alfa is 0.1 out_2 #estimated stopping point is correct. ## ## Call: ## fsInf(obj = fsfit, type = &quot;aic&quot;) ## ## Standard deviation of noise (specified or estimated) sigma = 1.592 ## ## Testing results at step = 9, with alpha = 0.100 ## Var Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea ## 1 -0.053 -7.681 0.016 -0.105 -0.015 0.050 0.05 ## 5 1.348 0.394 0.552 -27.065 17.845 0.000 0.05 ## 8 0.285 4.904 0.418 -0.716 0.340 0.050 0.05 ## 7 0.000 3.178 0.330 0.000 0.000 0.050 0.05 ## 6 0.043 0.909 0.401 -0.196 0.236 0.050 0.05 ## 3 0.012 1.804 0.266 -0.048 0.127 0.050 0.05 ## 2 -0.006 -1.904 0.414 -0.029 0.026 0.050 0.05 ## 9 0.003 1.636 0.483 -0.026 0.024 0.050 0.05 ## 4 -0.081 -0.683 0.436 -0.575 0.527 0.049 0.05 ## ## Estimated stopping point from AIC rule = 9 # You notice that AIC tends to overfit in this framework. Remember that we were doing # exhaustive search and not stepwise with AIC in the other Practicals. out_3 = fsInf(fsfit, type = &quot;aic&quot;, mult = log(n)) #bic case out_3 # BIC has a harsh penalty that correct AIC tendency to overfit (in this applied context) ## ## Call: ## fsInf(obj = fsfit, type = &quot;aic&quot;, mult = log(n)) ## ## Standard deviation of noise (specified or estimated) sigma = 1.592 ## ## Testing results at step = 5, with alpha = 0.100 ## Var Coef Z-score P-value LowConfPt UpConfPt LowTailArea UpTailArea ## 1 -0.061 -12.032 0.000 -0.070 -0.053 0.049 0.049 ## 5 3.735 5.775 0.060 -0.271 7.845 0.050 0.050 ## 8 0.285 4.917 0.417 -0.716 0.345 0.050 0.049 ## 7 0.000 3.292 0.340 0.000 0.000 0.050 0.050 ## 6 0.011 2.902 0.369 -0.051 0.074 0.050 0.050 ## ## Estimated stopping point from AIC rule = 5 # Here we are in a reality case so AIC is not optimal anymore as in our simulation setting, we expect # something different while forward stop control the FRD. BIC has a harsh penalty that is why it # selects less variables. Moreover we know that, from previous practicals, our residuals inspection # told us that we had a right tail with respect to the Normal QQ plot (see ebook corrections). 7.3.2 Sure Independence Screening Load the Leukemia dataset. Split the dataset randomly and create a train and test sample. require(SIS) #install it if you do not have it # Leukemia dataset taken from SIS package. train_set = data(leukemia.train) test_set = data(&quot;leukemia.test&quot;) train_set = as.data.frame(train_set) test_set = as.data.frame(test_set) X_train = as.matrix(leukemia.train[,1:7129]) y_train = leukemia.train[,7130] X_test = as.matrix(leukemia.test[,1:7129]) y_test = leukemia.test[,7130] The functions SIS() performs first a screening procedure based on marginal correlations and then applies a penalized method (Chapter 4 of the e-book) to obtain the final model. Choose among all the available options (i.e. in terms both of penalized methods and tuning constants) three candidates and evaluate the predictions of the selected models on the test sample. Which penalized method performs best in this specific example after the SIS? # Model Phase model_leuk_scad=SIS(X_train, y_train, family=&#39;binomial&#39;) #SCAD is default ## Iter 1 , screening: 3320 ## Iter 1 , selection: 3320 ## Iter 1 , conditional-screening: 847 ## Iter 2 , screening: 847 3320 ## Iter 2 , selection: 3320 ## Model already selected model_leuk_lasso = SIS(X_train, y_train, family=&#39;binomial&#39;,penalty = &#39;lasso&#39;) ## Iter 1 , screening: 3320 ## Iter 1 , selection: 3320 ## Iter 1 , conditional-screening: 847 ## Iter 2 , screening: 847 3320 ## Iter 2 , selection: 847 3320 ## Maximum number of variables selected model_leuk_mcp = SIS(X_train, y_train, family=&#39;binomial&#39;,penalty = &#39;MCP&#39;) ## Iter 1 , screening: 3320 ## Iter 1 , selection: 3320 ## Iter 1 , conditional-screening: 847 ## Iter 2 , screening: 847 3320 ## Iter 2 , selection: 3320 ## Model already selected # Prediction Phase pred_scad=predict(model_leuk_scad,X_test,type=&quot;class&quot;) sum(pred_scad == y_test)/34 #0.7941176 ## [1] 0.7941176 pred_lasso=predict(model_leuk_lasso,X_test,type=&quot;class&quot;) sum(pred_lasso == y_test)/34 # 0.7941176 ## [1] 0.7941176 pred_mcp=predict(model_leuk_mcp,X_test,type=&quot;class&quot;) sum(pred_mcp == y_test)/34 # 0.7941176 ## [1] 0.7941176 # We got the same predictions for every method. # SCAD and MCP are near in terms of coefficient estimates but Lasso has one regressor more. 7.3.3 PC-simple algorithm First of all build a simulation setting as explained below: - Generate from a MVN (multivariate normal) a matrix \\(\\mathbf{X_{n*p}}\\) with \\(n = 1000\\) and \\(p = 10\\). Choose the location but set the scale matrix with an autoregressive form \\(\\boldsymbol{\\Sigma}=[\\sigma_{lm}]_{l,m=1,\\ldots,p}\\) with \\(\\sigma_{lm} = \\rho^{\\mid l - m\\mid}\\). - Fix \\(\\rho = 0.5\\) and set the seed equal to 11 (i.e. set.seed(11)). - Choose the generating vector \\(\\boldsymbol{\\beta }= [3 \\; 1.5 \\; 0 \\; 2 \\; rep(0,6)]\\). - Generate \\(\\mathbf{\\hat{y}}\\) thanks to the relation \\(\\mathbf{y} = \\mathbf{X_{n*p}} \\; \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) where \\(\\epsilon_{i}\\) is a standard normal. Suppose for simplicity that the errors are uncorrelated. ## Simulation Setting require(MASS) set.seed(11) n = 100 p = 10 # change rho as you please to inspect other correlations among the predictors rho = 0.5 mu = rep(1,p) sigma = rep(0,p^2) sigma = matrix(data = sigma, ncol = p,nrow = p) # Autoregressive structure for (i in 1:p) { for (j in 1:p) { sigma[i,j] = rho^(abs(i-j)) } } X = mvrnorm(n,mu,sigma) dim(X) ## [1] 100 10 beta = c(3,1.5,0,2,rep(0,6)) y = X%*%beta + rnorm(n,0,1) #for us sigma is 1 Now perform the following passages on your simulated data: a) Find the active set \\(M_{1}\\) using the Fisher’s Z transformation and the associated correlation coefficient test (fix \\(\\alpha = 0.05\\) for the rest of the exercise). ## Part a # Fix alfa at 5% for all the exercise v_cor = cor(x = X,y = y) q=rep(0,10) for (i in 1:10) { if(sqrt(n-3)*abs(1/2*log((1 + v_cor[i])/(1-v_cor[i]))) &gt; 1.96 ) { q[i]=1 } } A_0 = X #Active set zero # With the first step we select the first 5 variables, we reject H0 so that we have evidence that the # correlation is different from 0. A_1 = X[,1:5] #new active set Calculate all the partial correlations of order 1 (i.e. one variable at the time) of the active set \\(M_{1}\\), test them and retrieve \\(M_{2} \\subseteq M_1\\) which is the new active set. ## Part b # We can now create the 20x1 vector of first order partial correlation and then apply previous formula P = rep(0,25) P = matrix(P,nrow = 5,ncol = 5) for (z in 1:5) { for (i in 1:5) { if (i != z ) { mod_1 = lm(A_1[,z] ~ A_1[,i]) e_1 = A_1[,z] - cbind(rep(1,n),A_1[,i])%*%mod_1$coefficients mod_2 = lm(y ~ A_1[,i]) e_2 = y - cbind(rep(1,n),A_1[,i])%*%mod_2$coefficients P[i,z] = cor(e_1,e_2) } } } P = P + diag(5) v_cor = c(P) #vectorization # fix alfa at 5% and test the sample correlation. q=rep(0,25) # Cardinality of the active set is now 1 because we are evaluating partial correaltions of order 1 for (i in 1:25) { if(sqrt(n-4)*abs(1/2*log((1 + v_cor[i])/(1-v_cor[i]))) &gt; 1.96 ) { q[i]=1 } } q # variables 3 and 5 are eliminated from the active set A_2 = X[,c(1,2,4)] Find the partial correlations of higher order and test them until your reach the condition \\(M_{m-1} = M_{m}\\) which implies the convergence of the PC-simple algorithm. Do you obtain the exact model? ## Part c # Now we need to check 2nd order partial correlation to discriminate. # Note: we could also update estimation for partial correlation in order to speed up the computations # but in this easy example is not necessary. # VARIABLE 1 ACTIVE SET A_2 mod_1 = lm(A_2[,1] ~ A_2[,2] + A_2[,3]) e_1 = A_2[,1] - cbind(rep(1,n),A_2[,2],A_2[,3])%*%mod_1$coefficients mod_2 = lm(y ~ A_2[,2] + A_2[,3]) e_2 = y - cbind(rep(1,n),A_2[,2],A_2[,3])%*%mod_2$coefficients # Cardinality of active set now is two! sqrt(n-5)*abs(1/2*log((1 + cor(e_1,e_2))/(1-cor(e_1,e_2)))) &gt; 1.96 # VARIABLE 2 ACTIVE SET A_2 mod_1 = lm(A_2[,2] ~ A_2[,1] + A_2[,3]) e_1 = A_2[,2] - cbind(rep(1,n),A_2[,1],A_2[,3])%*%mod_1$coefficients mod_2 = lm(y ~ A_2[,1] + A_2[,3]) e_2 = y - cbind(rep(1,n),A_2[,1],A_2[,3])%*%mod_2$coefficients # Cardinality of active set now is two! sqrt(n-5)*abs(1/2*log((1 + cor(e_1,e_2))/(1-cor(e_1,e_2)))) &gt; 1.96 # VARIABLE 3 ACTIVE SET A_2 mod_1 = lm(A_2[,3] ~ A_2[,1] + A_2[,2]) e_1 = A_2[,3] - cbind(rep(1,n),A_2[,1],A_2[,2])%*%mod_1$coefficients mod_2 = lm(y ~ A_2[,1] + A_2[,2]) e_2 = y - cbind(rep(1,n),A_2[,1],A_2[,2])%*%mod_2$coefficients # Cardinality of active set now is two! sqrt(n-5)*abs(1/2*log((1 + cor(e_1,e_2))/(1-cor(e_1,e_2)))) &gt; 1.96 # Conclusions: all conditions are true, then the algorithm will not move anymore and we have # our final model with the original variables c(1,2,4) which is also the exact one. 7.3.4 Regression Tree Load the Zambia dataset, split it randomly in a train and test sample (common choice is \\(\\frac{2}{3}\\) train and \\(\\frac{1}{3}\\) test). For simplicity, you can consider only the continuous variables. require(rpart) load(&quot;malnutrion_zambia_cleaned.Rda&quot;) data_zambia = data_zambia[,c(1:7,21,22,24)] #exclude the factors from the analysis X = as.matrix(data_zambia[,2:10]) y = data_zambia[,1] ## Part a ind = 1:1927 index = sample(x = ind,size = 1284,replace = F) data_zambia_train= data_zambia[index,] data_zambia_test= data_zambia[-index,] Fit a regression tree with the function rpart() and plot the tree. Have a look at rpart.plot package if you want to improve the appearance of the fitted tree. ## Part b # grow tree attach(data_zambia_train) fit &lt;- rpart(`Height for age sd`~ `Breastfeeding duration (months)` + `Age of the child (months)`+`Age of the mother (years)`+`BMI mother`+`Heigth mother (meter)`+`Weight mother (kg)`+`Wealth index factor score`+`Child weight at birth (kg)`+`Interval between births`, method=&quot;anova&quot;, data=data_zambia_train) printcp(fit) # display the results ## ## Regression tree: ## rpart(formula = `Height for age sd` ~ `Breastfeeding duration (months)` + ## `Age of the child (months)` + `Age of the mother (years)` + ## `BMI mother` + `Heigth mother (meter)` + `Weight mother (kg)` + ## `Wealth index factor score` + `Child weight at birth (kg)` + ## `Interval between births`, data = data_zambia_train, method = &quot;anova&quot;) ## ## Variables actually used in tree construction: ## [1] Age of the child (months) Child weight at birth (kg) ## [3] Heigth mother (meter) Interval between births ## [5] Wealth index factor score Weight mother (kg) ## ## Root node error: 3949.2/1284 = 3.0757 ## ## n= 1284 ## ## CP nsplit rel error xerror xstd ## 1 0.085434 0 1.00000 1.00140 0.049312 ## 2 0.027354 1 0.91457 0.92121 0.043830 ## 3 0.014385 2 0.88721 0.91542 0.044182 ## 4 0.013865 3 0.87283 0.94016 0.045852 ## 5 0.011370 4 0.85896 0.94585 0.046185 ## 6 0.010284 5 0.84759 0.94109 0.045973 ## 7 0.010160 7 0.82702 0.94040 0.045758 ## 8 0.010000 8 0.81686 0.94034 0.045801 plotcp(fit) # visualize cross-validation results # summary(fit) detailed summary of splits library(rattle) library(rpart.plot) library(RColorBrewer) fancyRpartPlot(fit) detach(data_zambia_train) After having pruned the tree, evaluate its prediction on the test sample (i.e. use predict() on a tree object. ## Part c attach(data_zambia_train) # prune the tree fit$cptable[which.min(fit$cptable[,&quot;xerror&quot;]),&quot;CP&quot;] ## [1] 0.01438482 fit$cptable ## CP nsplit rel error xerror xstd ## 1 0.08543400 0 1.0000000 1.0013950 0.04931249 ## 2 0.02735374 1 0.9145660 0.9212097 0.04382993 ## 3 0.01438482 2 0.8872123 0.9154191 0.04418249 ## 4 0.01386532 3 0.8728274 0.9401599 0.04585160 ## 5 0.01137031 4 0.8589621 0.9458524 0.04618497 ## 6 0.01028396 5 0.8475918 0.9410920 0.04597282 ## 7 0.01016050 7 0.8270239 0.9404004 0.04575772 ## 8 0.01000000 8 0.8168634 0.9403443 0.04580068 pfit&lt;- prune(fit, cp=0.02406722) # from cptable fancyRpartPlot(pfit) # Notice the different tree after pruning: Age of the child and Weight of the mother stays relevant # while other variables disappear. Prediction &lt;- predict(pfit, data_zambia_test, type = &quot;vector&quot;) # We can use the mean squared error to evaluate the predictions mse = sum((Prediction - data_zambia_test$`Height for age sd`)^2)/length(Prediction) detach(data_zambia_train) # Now we should find a benchmark to understand how well is doing the Regression Tree # In this case we will use linear regression plus model selection thanks to FDR rule. X_train = as.matrix(data_zambia_train[,2:10]) mod_zambia = lm(data_zambia_train[,1] ~ X_train, data = data_zambia_train) object = summary(mod_zambia) # Fix a level q = 0.05 of FDR (see Hochberg&amp;Benjamini), order the p-values and we reject three H0 thus # selecting variables: Breastfeeding duration, Weight at birth and wealth index factor scores. mod_zambia_final = lm(data_zambia_train[,1] ~ X_train[,c(1,7,8)], data = data_zambia_train) X_test = as.matrix(data_zambia_test[,c(2,8,9)]) predictions_linear = cbind(rep(1,643),X_test)%*%mod_zambia_final$coefficients mse_linear = sum((predictions_linear - data_zambia_test$`Height for age sd`)^2)/length(Prediction) # Similar to the one of the Regression Tree so there is not a big improvement. mse ## [1] 2.401796 mse_linear ## [1] 2.367926 7.3.5 Classification Tree Load the Iris dataset already present in R, split it randomly in a train and test sample (common choice is \\(\\frac{2}{3}\\) train and \\(\\frac{1}{3}\\) test). ## Part a data_iris = iris ind = 1:150 index = sample(x = ind,size = 100,replace = F) train= data_iris[index,] test= data_iris[-index,] Fit a classification tree with the function rpart() and plot the tree. Have a look at rpart.plot package if you want to improve the appearance of the fitted tree. ## Part b attach(train) fit_iris = rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, data=train, method=&quot;class&quot;) printcp(fit_iris) # display the results ## ## Classification tree: ## rpart(formula = Species ~ Sepal.Length + Sepal.Width + Petal.Length + ## Petal.Width, data = train, method = &quot;class&quot;) ## ## Variables actually used in tree construction: ## [1] Petal.Length Petal.Width ## ## Root node error: 63/100 = 0.63 ## ## n= 100 ## ## CP nsplit rel error xerror xstd ## 1 0.50794 0 1.000000 1.07937 0.074044 ## 2 0.42857 1 0.492063 0.49206 0.073412 ## 3 0.01000 2 0.063492 0.12698 0.043062 plotcp(fit_iris) # visualize cross-validation results fancyRpartPlot(fit_iris) detach(train) After having pruned the tree, evaluate its prediction on the test sample (i.e. use predict() on a tree object). ## Part c # prune the tree pfit_iris&lt;- prune(fit_iris, cp= fit$cptable[which.min(fit$cptable[,&quot;xerror&quot;]),&quot;CP&quot;]) fancyRpartPlot(pfit_iris) Prediction &lt;- predict(fit_iris, test, type = &quot;class&quot;) #pruned or not pruned trees are equal in this example t = Prediction == test$Species sum(t)/length(test$Species) ## [1] 0.96 7.4 Chapter 4 7.4.1 Ridge Regression Use the function glmnet() to perform a Ridge regression on Zambia dataset, plot the values as a function of \\(\\lambda\\) and comment on the results. require(glmnet) load(&quot;malnutrion_zambia_cleaned.Rda&quot;) data_zambia = data_zambia[,c(1:7,21,22,24)] #exclude the factors from the analysis X = as.matrix(data_zambia[,2:10]) y = data_zambia[,1] ## Part a # Ridge penalty is obtained with parameter alpha = 0 mod_pen_L2 = glmnet(X,y,family = &quot;gaussian&quot;,alpha = 0) plot(mod_pen_L2, xvar=&quot;lambda&quot;) #more smoothed, no model selection property just shrinkage # mod_pen_L2$beta shrinked but all present Use the function cv.glmnet() to perform model selection based on 10-fold Cross Validation (i.e. method to select the \\(\\lambda\\) parameter), plot the results and comment the graph that you obtain. Which values of \\(\\lambda\\) are shown by default? ## Part b # Selection based on 10 fold CV cv.zero_L2 &lt;- cv.glmnet(X,y, alpha=0) #a way in which you can select lambda (10k CV) cv.zero_L2$lambda.min ## [1] 0.08504366 plot(cv.zero_L2) # The two vertical lines are respectively: the value of lambda that minimizes the CV error curve and # the value lambda.1SE which is one standard error from lambda.min on the right. Use the function predict() to retrieve the final model estimates and perform a simple linear model on the same covariates, what can you conclude? ## Part c # Easier with predict rather than check all the order based on the single value of lambda ### MESSAGE FROM HASTIE AND TIBSHIRANI ### # Use directly lambda.min it is prone to errors # WARNING: use with care. Do not supply a single value for lambda (for predictions after CV use predict() instead). # Supply instead a decreasing sequence of lambda values. glmnet relies on its warms starts for speed, and its often # faster to fit a whole path than compute a single fit. coef_L2_min = predict(cv.zero_L2,type=&quot;coefficient&quot;,s = &quot;lambda.min&quot;) coef_L2_1SE = predict(cv.zero_L2,type=&quot;coefficient&quot;) #default is lambda.1SE model_linear = lm(y ~ .,data = data_zambia[,c(2:10)]) summary(model_linear) ## ## Call: ## lm(formula = y ~ ., data = data_zambia[, c(2:10)]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4167 -0.9869 -0.0761 0.8483 7.9208 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.646e+00 5.439e+00 -0.854 0.3931 ## `Breastfeeding duration (months)` -5.273e-02 6.866e-03 -7.681 2.50e-14 ## `Age of the child (months)` -5.574e-03 2.928e-03 -1.904 0.0571 ## `Age of the mother (years)` 1.170e-02 6.485e-03 1.804 0.0715 ## `BMI mother` -8.108e-02 1.186e-01 -0.683 0.4944 ## `Heigth mother (meter)` 1.348e+00 3.423e+00 0.394 0.6938 ## `Weight mother (kg)` 4.251e-02 4.674e-02 0.909 0.3632 ## `Wealth index factor score` 1.261e-06 3.968e-07 3.178 0.0015 ## `Child weight at birth (kg)` 2.851e-01 5.813e-02 4.904 1.02e-06 ## `Interval between births` 3.250e-03 1.987e-03 1.636 0.1021 ## ## (Intercept) ## `Breastfeeding duration (months)` *** ## `Age of the child (months)` . ## `Age of the mother (years)` . ## `BMI mother` ## `Heigth mother (meter)` ## `Weight mother (kg)` ## `Wealth index factor score` ** ## `Child weight at birth (kg)` *** ## `Interval between births` ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.592 on 1917 degrees of freedom ## Multiple R-squared: 0.1339, Adjusted R-squared: 0.1298 ## F-statistic: 32.93 on 9 and 1917 DF, p-value: &lt; 2.2e-16 # We observe the shrinkage property of L2 norms penalties, the coefficients are shrinked to achieve a better compromise between bias and variance. For supplementary theory see Stein&#39;s estimators. 7.4.2 Lasso Use the function glmnet() to perform a lasso on Zambia dataset, plot the values as a function of \\(\\lambda\\) and comment on the results. ## Part 1 # Lasso penalty is obtained by setting alpha equal to one mod_pen = glmnet(X,y,family = &quot;gaussian&quot;,alpha = 1) # mod_pen$beta # here you can see order of the lasso plot(mod_pen, xvar=&quot;lambda&quot;) #plot coefficients as lambda varys, use lambda found below to select. Use the function cv.glmnet() to perform model selection based on 10-fold Cross Validation (i.e. method to select the \\(\\lambda\\) parameter), plot the results and comment the graph that you obtain. Which values of \\(\\lambda\\) are shown by default? What can you conclude on the choice of \\(\\lambda\\) in terms of model selection? ## Part 2 # Selection based on 10 fold CV cv.zero_mod &lt;- cv.glmnet(X,y, alpha=1) #a way in which you can select lambda (10k CV) cv.zero_mod$lambda.min ## [1] 0.002010848 plot(cv.zero_mod) # The two vertical lines are respectively: the value of lambda that minimizes the CV error curve and # the value lambda.1SE which is one standard error from lambda.min in the direction of a more # parsimonious model. Use the function predict() to retrieve the final model chosen by 10-fold CV (given lasso ordering) and perform a linear model on the covariates present in the final model. What can you conclude observing the estimates? ## Part 3 mod_pen = glmnet(X,y,family = &quot;gaussian&quot;,alpha = 1,lambda = 0.00201) #wrong, do not use this way! # The right way is to use the predict function coef_L1_min = predict(cv.zero_mod,type=&quot;coefficient&quot;,s = &quot;lambda.min&quot;) coef_L1_1SE = predict(cv.zero_mod,type=&quot;coefficient&quot;) #default is lambda.1SE # As you can notice, lambda min tends to select more regressors so it is less parsimonious then 1.SE model_linear = lm(y ~ .,data = data_zambia[,c(2,3,6:9)]) summary(model_linear) ## ## Call: ## lm(formula = y ~ ., data = data_zambia[, c(2, 3, 6:9)]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.4782 -0.9864 -0.0534 0.8430 7.8345 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.971e+00 9.654e-01 -8.257 2.75e-16 ## `Breastfeeding duration (months)` -5.196e-02 6.863e-03 -7.571 5.74e-14 ## `Age of the child (months)` -5.901e-03 2.924e-03 -2.018 0.043726 ## `Heigth mother (meter)` 3.675e+00 6.483e-01 5.668 1.66e-08 ## `Weight mother (kg)` 1.203e-02 3.970e-03 3.031 0.002469 ## `Wealth index factor score` 1.311e-06 3.916e-07 3.349 0.000827 ## `Child weight at birth (kg)` 2.822e-01 5.811e-02 4.856 1.30e-06 ## ## (Intercept) *** ## `Breastfeeding duration (months)` *** ## `Age of the child (months)` * ## `Heigth mother (meter)` *** ## `Weight mother (kg)` ** ## `Wealth index factor score` *** ## `Child weight at birth (kg)` *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.594 on 1920 degrees of freedom ## Multiple R-squared: 0.1302, Adjusted R-squared: 0.1275 ## F-statistic: 47.91 on 6 and 1920 DF, p-value: &lt; 2.2e-16 # Lasso estimates are known to be biased and we are not even sure about the signs if dimensions of predictor space # is greater than 2. There are methods which are called &quot;debiasing the lasso&quot; that you can search if you are interested by this topic. Of course here we are using real data so we have always to be aware of the assumptions of each model that we are fitting. 7.4.3 Non Convex Penalties Fix the generating vector \\(\\boldsymbol{\\beta}=(4,2,-4,-2,0,0,\\ldots,0)\\) and set the seed equal to 11 (i.e. set.seed(11)). require(MASS) set.seed(11) beta = c(4,2,-4,-2,rep(0,996)) Generate from a MVN (multivariate normal) a matrix \\(\\mathbf{X_{n*p}}\\) with \\(n = 200\\) and \\(p = 1000\\). You can choose the location vector as you wish but set the scale matrix with an autoregressive form \\(\\boldsymbol{\\Sigma}=[\\sigma_{lm}]_{l,m=1,\\ldots,p}\\) with \\(\\sigma_{lm} = \\rho^{\\mid l - m\\mid}\\). n = 200 p = 1000 # Three values needed for rho = (0 0.2 0.5) rho = 0 mu = rep(1,p) sigma = rep(0,p^2) sigma = matrix(data = sigma, ncol = p,nrow = p) # Autoregressive structure for (i in 1:p) { for (j in 1:p) { sigma[i,j] = rho^(abs(i-j)) } } X = mvrnorm(n,mu,sigma) For each \\(\\boldsymbol{\\rho} = [0 \\; 0.2 \\; 0.5]\\) generate \\(\\mathbf{\\hat{y}}\\) thanks to the relation \\(\\mathbf{y} = \\mathbf{X_{n*p}} \\; \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\\) where \\(\\epsilon_{i}\\) is a standard normal. Suppose for simplicity that the errors are uncorrelated. y = X%*%beta + rnorm(n,0,1) #for us sigma is 1 Compare the solution paths (graphically as a function of \\(\\lambda\\)) for the lasso, SCAD and MCP by fixing several values for \\(\\gamma\\) (choose e.g. \\(\\gamma=(1.5, 2, 3, 3.7, 5)\\)) for each value of \\(\\rho\\) indicated at previous point. # Paths will be presented for gamma = (2, default values, 5) # The value 1.5 for gamma is not possible for SCAD while 3 and 3.7 are the default value for MCP and Scad respectively require(ncvreg) # Default gamma model_scad = ncvreg(X = X,y = y,family = &quot;gaussian&quot;,penalty = &quot;SCAD&quot;) #gamma default is 3.7 model_mcp = ncvreg(X = X,y = y,family = &quot;gaussian&quot;,penalty = &quot;MCP&quot;) #gamma default is 3 model_lasso = ncvreg(X = X,y = y,family = &quot;gaussian&quot;,penalty = &quot;lasso&quot;) # Gamma = 2 model_scad_2 = ncvreg(X = X,y = y,family = &quot;gaussian&quot;,penalty = &quot;SCAD&quot;,gamma = 2.1) #no scad for gamma lower than 2 model_mcp_2 = ncvreg(X = X,y = y,family = &quot;gaussian&quot;,penalty = &quot;MCP&quot;, gamma = 2) # Gamma = 5 model_scad_5 = ncvreg(X = X,y = y,family = &quot;gaussian&quot;,penalty = &quot;SCAD&quot;, gamma = 5) model_mcp_5 = ncvreg(X = X,y = y,family = &quot;gaussian&quot;,penalty = &quot;MCP&quot;, gamma = 5) par(mfrow=c(1,3)) # gamma = 2 plot(model_lasso,main=&quot;Lasso&quot;) plot(model_scad_2, main=&quot;Scad - Gamma 2.1&quot;) plot(model_mcp_2, main=&quot;MCP - Gamma 2&quot;) par(mfrow=c(1,3)) #default gamma plot(model_lasso,main=&quot;Lasso&quot;) plot(model_scad,main=&quot;Scad - Gamma 3.7&quot;) plot(model_mcp, main=&quot;MCP - Gamma 3&quot;) par(mfrow=c(1,3)) # gamma = 5 plot(model_lasso,main=&quot;Lasso&quot;) plot(model_scad_5,main=&quot;Scad - Gamma 5&quot;) plot(model_mcp_5, main=&quot;MCP - Gamma 5&quot;) \\(\\rho = 0\\) \\(\\rho = 0.2\\) \\(\\rho = 0.5\\) # Comments: # 1) We can already see that there is a non convex behavior of Scad and MCP while Lasso path is more # smoothed. # 2) As gamma goes to plus infinity we notice that we go back to lasso path (smoothing procedure). # 3) The bias is minimized as gamma approaches his minimum value. # 4) Correlation creates a distorsion in the paths of all the three penalties. "],
["references.html", "References", " References "]
]
