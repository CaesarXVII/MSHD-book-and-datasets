<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Ordering the variables | Model Selection in High Dimensions</title>
  <meta name="description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Ordering the variables | Model Selection in High Dimensions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Ordering the variables | Model Selection in High Dimensions" />
  
  <meta name="twitter:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  

<meta name="author" content="Maria-Pia Victoria-Feser (professor), Cesare Miglioli and Guillaume Blanc (teaching assistants)" />


<meta name="date" content="2020-02-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="assessing-the-validity-of-a-model.html"/>
<link rel="next" href="shrinkage-methods.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#read-this-part-first"><i class="fa fa-check"></i><b>1.1</b> Read this part first</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#content-choice-and-structure"><i class="fa fa-check"></i><b>1.2</b> Content choice and structure</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#bibliography"><i class="fa fa-check"></i><b>1.2.1</b> Bibliography</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#useful-links"><i class="fa fa-check"></i><b>1.2.2</b> Useful links</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#using-r"><i class="fa fa-check"></i><b>1.3</b> Using R</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#useful-r-packages"><i class="fa fa-check"></i><b>1.3.1</b> Useful R packages</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#managing-data"><i class="fa fa-check"></i><b>1.3.2</b> Managing Data</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#loading-data-from-an-r-package"><i class="fa fa-check"></i><b>1.3.3</b> Loading data from an R package</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#loading-data-from-a-local-file"><i class="fa fa-check"></i><b>1.3.4</b> Loading data from a local file</a></li>
<li class="chapter" data-level="1.3.5" data-path="index.html"><a href="index.html#loading-data-from-an-online-file"><i class="fa fa-check"></i><b>1.3.5</b> Loading data from an online file</a></li>
<li class="chapter" data-level="1.3.6" data-path="index.html"><a href="index.html#loading-data-from-an-online-database-using-a-mysql-query-optional"><i class="fa fa-check"></i><b>1.3.6</b> Loading data from an online database using a mySQL query (Optional)</a></li>
<li class="chapter" data-level="1.3.7" data-path="index.html"><a href="index.html#data-wrangling"><i class="fa fa-check"></i><b>1.3.7</b> Data Wrangling</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#writing-reports"><i class="fa fa-check"></i><b>1.4</b> Writing reports</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#r-markdown"><i class="fa fa-check"></i><b>1.4.1</b> R Markdown</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4.2</b> GitHub</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.5</b> Examples</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#data-on-malnutrition-in-zambia"><i class="fa fa-check"></i><b>1.5.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#prognostic-factors-in-childhood-leukemia"><i class="fa fa-check"></i><b>1.5.2</b> Prognostic Factors in Childhood Leukemia</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#gene-expression-in-prostate-cancer"><i class="fa fa-check"></i><b>1.5.3</b> Gene Expression in Prostate Cancer</a></li>
<li class="chapter" data-level="1.5.4" data-path="index.html"><a href="index.html#gene-expression-ratios-in-lung-cancer-and-mesothelioma"><i class="fa fa-check"></i><b>1.5.4</b> Gene Expression Ratios in Lung Cancer and Mesothelioma</a></li>
<li class="chapter" data-level="1.5.5" data-path="index.html"><a href="index.html#r-package-quantmod"><i class="fa fa-check"></i><b>1.5.5</b> R package quantmod</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#fundamental-statistical-concepts"><i class="fa fa-check"></i><b>1.6</b> Fundamental statistical concepts</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#sample-and-population"><i class="fa fa-check"></i><b>1.6.1</b> Sample and population</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#models-and-risk"><i class="fa fa-check"></i><b>1.6.2</b> Models and risk</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#estimators-and-associated-variability"><i class="fa fa-check"></i><b>1.6.3</b> Estimators and associated variability</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#simulating-the-population-using-resampling-techniques"><i class="fa fa-check"></i><b>1.6.4</b> Simulating the population using resampling techniques</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#model-selection"><i class="fa fa-check"></i><b>1.6.5</b> Model Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html"><i class="fa fa-check"></i><b>2</b> Assessing the validity of a model</a><ul>
<li class="chapter" data-level="2.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#cross-validation"><i class="fa fa-check"></i><b>2.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="2.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#covariance-penalties-criteria"><i class="fa fa-check"></i><b>2.3</b> Covariance Penalties Criteria</a><ul>
<li class="chapter" data-level="2.3.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-2"><i class="fa fa-check"></i><b>2.3.1</b> Introduction</a></li>
<li class="chapter" data-level="2.3.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mallows-c_p"><i class="fa fa-check"></i><b>2.3.2</b> Mallows <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="2.3.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#efrons-q-class"><i class="fa fa-check"></i><b>2.3.3</b> Efron’s <span class="math inline">\(q\)</span>-class</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#information-theory-and-bayesian-criteria"><i class="fa fa-check"></i><b>2.4</b> Information Theory and Bayesian Criteria</a><ul>
<li class="chapter" data-level="2.4.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#aic-akaike-information-criterion"><i class="fa fa-check"></i><b>2.4.1</b> AIC: Akaike Information Criterion</a></li>
<li class="chapter" data-level="2.4.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#bic-bayesian-information-criterion"><i class="fa fa-check"></i><b>2.4.2</b> BIC: Bayesian Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mean-squared-error-based-criteria"><i class="fa fa-check"></i><b>2.5</b> Mean Squared Error Based Criteria</a><ul>
<li class="chapter" data-level="2.5.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#steins-unbiased-risk-estimator-sure"><i class="fa fa-check"></i><b>2.5.1</b> Stein’s unbiased risk estimator (SURE)</a></li>
<li class="chapter" data-level="2.5.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-focused-information-criterion-fic"><i class="fa fa-check"></i><b>2.5.2</b> The Focused Information Criterion (FIC)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-measures"><i class="fa fa-check"></i><b>2.6</b> Classification measures</a><ul>
<li class="chapter" data-level="2.6.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-logistic-model"><i class="fa fa-check"></i><b>2.6.1</b> The logistic model</a></li>
<li class="chapter" data-level="2.6.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#prediction-error-measures-for-binary-classification"><i class="fa fa-check"></i><b>2.6.2</b> Prediction error measures for Binary classification</a></li>
<li class="chapter" data-level="2.6.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-error-estimation"><i class="fa fa-check"></i><b>2.6.3</b> Classification error estimation</a></li>
<li class="chapter" data-level="2.6.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-roc-curve"><i class="fa fa-check"></i><b>2.6.4</b> The ROC curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html"><i class="fa fa-check"></i><b>3</b> Ordering the variables</a><ul>
<li class="chapter" data-level="3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#stepwise-forward-regression"><i class="fa fa-check"></i><b>3.2</b> Stepwise forward regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#partial-correlations"><i class="fa fa-check"></i><b>3.2.1</b> Partial correlations</a></li>
<li class="chapter" data-level="3.2.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#selection-by-hypothesis-testing"><i class="fa fa-check"></i><b>3.2.2</b> Selection by hypothesis testing</a></li>
<li class="chapter" data-level="3.2.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#orthogonal-matching-pursuit"><i class="fa fa-check"></i><b>3.2.3</b> Orthogonal matching pursuit</a></li>
<li class="chapter" data-level="3.2.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#forward-stagewise-regression"><i class="fa fa-check"></i><b>3.2.4</b> Forward stagewise regression</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#streamwise-regression"><i class="fa fa-check"></i><b>3.3</b> Streamwise regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-4"><i class="fa fa-check"></i><b>3.3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#sure-independence-screening"><i class="fa fa-check"></i><b>3.3.2</b> Sure Independence Screening</a></li>
<li class="chapter" data-level="3.3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#pc-simple-algorithm"><i class="fa fa-check"></i><b>3.3.3</b> PC-simple algorithm</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-and-regression-tree-cart"><i class="fa fa-check"></i><b>3.4</b> Classification And Regression Tree (CART)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#regression-tree"><i class="fa fa-check"></i><b>3.4.1</b> Regression tree</a></li>
<li class="chapter" data-level="3.4.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-trees"><i class="fa fa-check"></i><b>3.4.2</b> Classification Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html"><i class="fa fa-check"></i><b>4</b> Shrinkage Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#ridge-regression"><i class="fa fa-check"></i><b>4.2</b> Ridge regression</a></li>
<li class="chapter" data-level="4.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-lasso-estimator"><i class="fa fa-check"></i><b>4.3</b> The lasso estimator</a></li>
<li class="chapter" data-level="4.4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#alternative-penalized-regression-methods"><i class="fa fa-check"></i><b>4.4</b> Alternative penalized regression methods</a><ul>
<li class="chapter" data-level="4.4.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-adaptive-and-relaxed-lasso"><i class="fa fa-check"></i><b>4.4.1</b> The adaptive and relaxed lasso</a></li>
<li class="chapter" data-level="4.4.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-elastic-net"><i class="fa fa-check"></i><b>4.4.2</b> The elastic net</a></li>
<li class="chapter" data-level="4.4.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-nonnegative-garotte"><i class="fa fa-check"></i><b>4.4.3</b> The nonnegative garotte</a></li>
<li class="chapter" data-level="4.4.4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#non-convex-penalties"><i class="fa fa-check"></i><b>4.4.4</b> Non convex penalties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html"><i class="fa fa-check"></i><b>5</b> Properties of model selection criteria</a><ul>
<li class="chapter" data-level="5.1" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#introduction-6"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#selection-consistency"><i class="fa fa-check"></i><b>5.2</b> Selection consistency</a></li>
<li class="chapter" data-level="5.3" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#selection-efficiency"><i class="fa fa-check"></i><b>5.3</b> Selection efficiency</a></li>
<li class="chapter" data-level="5.4" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#the-oracle-property"><i class="fa fa-check"></i><b>5.4</b> The oracle property</a></li>
<li class="chapter" data-level="5.5" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#probability-of-overfitting"><i class="fa fa-check"></i><b>5.5</b> Probability of overfitting</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="post-selection-inference.html"><a href="post-selection-inference.html"><i class="fa fa-check"></i><b>6</b> Post-Selection Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="post-selection-inference.html"><a href="post-selection-inference.html#introduction-7"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="post-selection-inference.html"><a href="post-selection-inference.html#inference-via-the-nonparametric-bootstrap"><i class="fa fa-check"></i><b>6.2</b> Inference via the nonparametric Bootstrap</a></li>
<li class="chapter" data-level="6.3" data-path="post-selection-inference.html"><a href="post-selection-inference.html#improving-on-the-bootstrap-smoothed-bootstrap-or-bagging"><i class="fa fa-check"></i><b>6.3</b> Improving on the Bootstrap: Smoothed Bootstrap or Bagging</a></li>
<li class="chapter" data-level="6.4" data-path="post-selection-inference.html"><a href="post-selection-inference.html#post-selection-significance-testing"><i class="fa fa-check"></i><b>6.4</b> Post selection significance testing</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i><b>7</b> Solutions</a><ul>
<li class="chapter" data-level="7.1" data-path="solutions.html"><a href="solutions.html#chapter-1"><i class="fa fa-check"></i><b>7.1</b> Chapter 1</a><ul>
<li class="chapter" data-level="7.1.1" data-path="solutions.html"><a href="solutions.html#zam"><i class="fa fa-check"></i><b>7.1.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="7.1.2" data-path="solutions.html"><a href="solutions.html#leuk"><i class="fa fa-check"></i><b>7.1.2</b> Prognostic Factors in Childhood Leukemia</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="solutions.html"><a href="solutions.html#chapter-2"><i class="fa fa-check"></i><b>7.2</b> Chapter 2</a><ul>
<li class="chapter" data-level="7.2.1" data-path="solutions.html"><a href="solutions.html#cv"><i class="fa fa-check"></i><b>7.2.1</b> Cross-validation</a></li>
<li class="chapter" data-level="7.2.2" data-path="solutions.html"><a href="solutions.html#aic"><i class="fa fa-check"></i><b>7.2.2</b> Akaike Information Criterion</a></li>
<li class="chapter" data-level="7.2.3" data-path="solutions.html"><a href="solutions.html#roc"><i class="fa fa-check"></i><b>7.2.3</b> ROC curves</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="solutions.html"><a href="solutions.html#chapter-3"><i class="fa fa-check"></i><b>7.3</b> Chapter 3</a><ul>
<li class="chapter" data-level="7.3.1" data-path="solutions.html"><a href="solutions.html#HT"><i class="fa fa-check"></i><b>7.3.1</b> Selection by Hypothesis Testing</a></li>
<li class="chapter" data-level="7.3.2" data-path="solutions.html"><a href="solutions.html#SIS"><i class="fa fa-check"></i><b>7.3.2</b> Sure Independence Screening</a></li>
<li class="chapter" data-level="7.3.3" data-path="solutions.html"><a href="solutions.html#PC"><i class="fa fa-check"></i><b>7.3.3</b> PC-simple algorithm</a></li>
<li class="chapter" data-level="7.3.4" data-path="solutions.html"><a href="solutions.html#RT"><i class="fa fa-check"></i><b>7.3.4</b> Regression Tree</a></li>
<li class="chapter" data-level="7.3.5" data-path="solutions.html"><a href="solutions.html#CT"><i class="fa fa-check"></i><b>7.3.5</b> Classification Tree</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="solutions.html"><a href="solutions.html#chapter-4"><i class="fa fa-check"></i><b>7.4</b> Chapter 4</a><ul>
<li class="chapter" data-level="7.4.1" data-path="solutions.html"><a href="solutions.html#Ridge"><i class="fa fa-check"></i><b>7.4.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="solutions.html"><a href="solutions.html#lasso"><i class="fa fa-check"></i><b>7.4.2</b> Lasso</a></li>
<li class="chapter" data-level="7.4.3" data-path="solutions.html"><a href="solutions.html#nonconvex"><i class="fa fa-check"></i><b>7.4.3</b> Non Convex Penalties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Model Selection in High Dimensions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ordering-the-variables" class="section level1">
<h1><span class="header-section-number">3</span> Ordering the variables</h1>
<div id="introduction-3" class="section level2">
<h2><span class="header-section-number">3.1</span> Introduction</h2>
<p>In high dimensional settings, for causal models, the available set of potential predictors can be very large and an exhaustive building of potential models (to be compared in terms of model validity) is practically impossible. One hence needs to find <em>suitable</em> methods for reducing the model size (number of predictors) by either:<br />
(a) ordering the predictors that enter the model sequentially, to constitute, at most, <span class="math inline">\(p\)</span> potential models to be assessed and compared,<br />
(b) averaging groups of predictors.<br />
<br />
For (a), the ordering consists in a sequence of steps in which, starting from a null model (usually just the intercept), a covariate is chosen for addition to the current model based on some prespecified criterion. The sequence then stops when an overall criterion is met.</p>
<p>For (b) averaging predictors is very popular in machine learning (supervised learning) and in this chapter we only review Classification And Regression Tree (CART).</p>
<p>Obviously, the two approaches do not lead to the same type of chosen model. While stepwise methods lead to models that only include a subset of the available covariates, averaging methods use all the available covariates and group them in weighted averages. If the model is used to understand the underlying phenomenon, then stepwise methods are usually preferred.</p>
<blockquote>
<p>Exercise: Compare the number of models to be considered in an exhaustive approach to the ones considered in a stepwise forward approach. Choose <span class="math inline">\(p=\{5,10,15,20\}\)</span> and suppose that the true (best) model has <span class="math inline">\(p\)</span> and <span class="math inline">\(p/2\)</span> predictors.</p>
</blockquote>
</div>
<div id="stepwise-forward-regression" class="section level2">
<h2><span class="header-section-number">3.2</span> Stepwise forward regression</h2>
<p>Very generally, a forward stepwise regression follows the steps:<br />
1. Let <span class="math inline">\(\cal{M}_0\)</span> denote the null model, which typically contains no predictors.<br />
2. For <span class="math inline">\(k = 1,\ldots,p-1\)</span>, do :<br />
(a) Consider all <span class="math inline">\(p-k\)</span> models that augment the predictors in <span class="math inline">\(\mathcal{M}_k\)</span> with one additional predictor.<br />
(b) Choose the <em>best</em> among these <span class="math inline">\(p-k\)</span> models, and call it <span class="math inline">\(\mathcal{M}_{k+1}\)</span>.<br />
3. Stop the algorithm if <span class="math inline">\(\mathcal{M}_{k+1}\)</span> is not better than <span class="math inline">\(\mathcal{M}_{k}\)</span> and provide <span class="math inline">\(\mathcal{M}_{k}\)</span> as output.</p>
<p>The algorithms differ in the definition of <em>best</em> in 2(b) and in the stopping rule criteria in 3. For the latter, it can be chosen among the criteria presented in Chapter 2.</p>
<p>As an example, consider the <span class="math inline">\(C_p\)</span> given in <a href="assessing-the-validity-of-a-model.html#eq:mallow">(2.2)</a> as model selection criterion. We note that for fixed <span class="math inline">\(p\)</span>, i.e. when comparing models of the same size <span class="math inline">\(p\)</span>, the difference among the possible models that can be constructed is measured by the residual sum of squares <span class="math inline">\(\sum_{i=1}^n\left(\hat{y}_i-y_i\right)^2\)</span>. Hence, choosing the <em>best</em> model in 2(b) amounts at choosing the added covariate that most reduces the residual sum of squares of the augmented model.</p>
<p>In a sequential framework, optimizing the choice of the covariate to add to the current model does not necessary lead to the <em>best</em> model of the current size since the optimization is done on the added covariate only. Additional properties need therefore to be considered.</p>
<div id="partial-correlations" class="section level3">
<h3><span class="header-section-number">3.2.1</span> Partial correlations</h3>
<p>The partial correlation between <span class="math inline">\(X_j\)</span> and <span class="math inline">\(Y\)</span> conditional of a set of <span class="math inline">\(q\)</span> variables <span class="math inline">\(\mathbf{X} = \{X_1, X_2, \ldots, X_q\}, X_j\notin \mathbf{X}\)</span>, written <span class="math inline">\(\hat{\rho}_{X_jY,\mathbf{X}}\)</span>, is the correlation between the residuals resulting from the linear regression of <span class="math inline">\(X_j\)</span> on <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(Y\)</span> on <span class="math inline">\(\mathbf{X}\)</span>.</p>
Namely, let <span class="math inline">\(\mathbf{X}_k\)</span>, of size <span class="math inline">\(n\times q\)</span>, and <span class="math inline">\(\mathbf{X}_{k+1}=[\mathbf{x}_j]_{j=1,\ldots,p-q}\)</span> be the matrices formed by the columns of <span class="math inline">\(\mathbf{X}\)</span> (containing all the potential predictors) that are a) present at the current stepwise forward step for <span class="math inline">\(\mathbf{X}_k\)</span> and b) not present at the current stepwise forward step for <span class="math inline">\(\mathbf{X}_{k+1}\)</span>. The partial correlations between each covariate <span class="math inline">\(X_j\)</span> (corresponding to the columns of <span class="math inline">\(\mathbf{X}_{k+1}\)</span>) and the response vector <span class="math inline">\(Y\)</span>, given the set of <span class="math inline">\(q\)</span> covariates corresponding to <span class="math inline">\(\mathbf{X}_k\)</span>, can be written as
<span class="math display">\[\begin{equation}
\hat{\rho}_{X_jY,\mathbf{X}_k}=\frac{\mathbf{e}^T(\mathbf{I}-\mathbf{H})\mathbf{x}_j}{\sqrt{\left(\mathbf{e}^T\mathbf{e}\right)\left(\mathbf{x}_j^T(\mathbf{I}-\mathbf{H})\mathbf{x}_j\right)}}
\end{equation}\]</span>
<p>with <span class="math inline">\(\mathbf{e}=\mathbf{y}-\mathbf{X}_k\hat{\boldsymbol{\beta}}\)</span>, <span class="math inline">\(\hat{\boldsymbol{\beta}}=\left(\mathbf{X}_k^T\mathbf{X}_k\right)^{-1}\mathbf{X}_k^T\mathbf{y}\)</span>, so that <span class="math inline">\(\mathbf{e}=\left(\mathbf{I}-\mathbf{X}_k\left(\mathbf{X}_k^T\mathbf{X}_k\right)^{-1}\mathbf{X}_k^T\right)\mathbf{y}=\left(\mathbf{I}-\mathbf{H}\right)\mathbf{y}\)</span>. The predictor <span class="math inline">\(\mathbf{X}_j\)</span> with the largest partial correlation offers the greatest reduction in the residual sum of squares; see e.g. <span class="citation">Foster and Stine (<a href="#ref-FoSt:04">2004</a>)</span>. In high dimensional settings, the sweep operator <span class="citation">(Goodnight <a href="#ref-Good:79">1979</a>)</span> can be used to obtain the partial correlations in a computationally fast manner.</p>
<blockquote>
<p>Exercise (<a href="https://github.com/CaesarXVII/Model-Selection-in-High-Dimensions/blob/master/Practicals/Practical%205/TP5.ex1_GROUP_A.pdf">solution</a> provided by <em>Talia Béatrice Kimber</em>):<br />
- Prove that optimization problem <a href="ordering-the-variables.html#eq:minRSS">(3.1)</a> (i.e. min RSS at a given step) and <a href="ordering-the-variables.html#eq:maxPC">(3.2)</a> (i.e. max partial correlations) are equivalent:</p>
</blockquote>
<span class="math display" id="eq:minRSS">\[\begin{equation}
\begin{aligned}
&amp; \underset{j \in P\backslash S}{\text{min}}
&amp; &amp; \| \mathbf{y} - \mathbf{X}^{\star} \hat{\beta}^{\star} - \mathbf{x}_j \beta_{j} \|_{2}^{2} \\
\end{aligned}\tag{3.1}
\end{equation}\]</span>
<span class="math display" id="eq:maxPC">\[\begin{equation}
\begin{aligned}
&amp; \underset{j \in P\backslash S}{\text{max}}
&amp; &amp; \mid \mathbf{x}_{j}^{T} (\mathbf{y} - \mathbf{X}^{\star} \hat{\beta}^{\star}) \mid \\
\end{aligned}\tag{3.2}
\end{equation}\]</span>
<blockquote>
<p>where:<br />
a) <span class="math inline">\(P = \{1,..,p\}\)</span> is the set of all available predictors.<br />
b) <span class="math inline">\(S = \{a \in P : \; \mid S \mid = q\}\)</span> is the solution set at the current step of the procedure.<br />
c) <span class="math inline">\(\mathbf{X}^{\star}\)</span> is the <span class="math inline">\(n \;x\; q\)</span> matrix of predictors selected at previous steps.<br />
d) <span class="math inline">\(\hat{\beta}^{\star}\)</span> is the OLS estimator of the model <span class="math inline">\(\mathbf{y} = \mathbf{X}^{\star} {\beta}^{\star} + \boldsymbol{\epsilon}\)</span>.<br />
e) <span class="math inline">\(\mathbf{x}_{j}\)</span> is one of the <span class="math inline">\(p - q\)</span> predictors left at the current step.<br />
Assume also that all the predictors have unitary norm (i.e. <span class="math inline">\(\mathbf{x}_{l}^{T}\mathbf{x}_{l} = 1 \; \forall \;l \in P\)</span>).</p>
</blockquote>
<blockquote>
Exercise (optional):<br />
- Consider the Malnutrition in Zambia dataset and order the covariates according to their partial correlations using the R function <em>fs</em> of the <em>Selective Inference</em> R Package (<a href="https://cran.r-project.org/web/packages/selectiveInference/index.html" class="uri">https://cran.r-project.org/web/packages/selectiveInference/index.html</a>).<br />
- Compare the ordering with the one obtained by the lasso (LARS) and discuss why they are different.<br />

</blockquote>
</div>
<div id="selection-by-hypothesis-testing" class="section level3">
<h3><span class="header-section-number">3.2.2</span> Selection by hypothesis testing</h3>
<p>Partial correlations have been used in several instances to <em>order</em> the covariates in stepwise forward regression, combined with a <em>stopping</em> criterion based on a test statistic, like the <span class="math inline">\(t\)</span>-test for significance testing. That is, the covariates are entered in the model using the partial correlation criterion (that is maximized) and the procedure stops when the <span class="math inline">\(t\)</span>-test is not significant. However, with testing multiple hypothesis, one has to pay attention to the <em>familywise error rate</em> (FWER), i.e. the probability of making one or more false discoveries, or type I errors (rejecting the null hypothesis when it is true). This probability increases very rapidly with the number of hypothesis to be tested.</p>
<p>Another concept is the <em>false discovery rate</em> (FDR) which is a measure that provides less stringent control of Type I errors compared to familywise error rate <span class="citation">(Benjamini and Hochberg <a href="#ref-BeHo:95">1995</a>)</span>. FDR actually controls the expected proportion of <em>discoveries</em> (rejected null hypotheses) that are false (incorrect rejections). When all hypotheses are true, the FDR is equal to the FWER, and smaller otherwise. Hence, substantial power can be gained when testing multiple hypothesis, while controlling the FDR.</p>
<p>Basically, one can see model selection (subset selection) as a series of hypothesis to be tested, namely significance testing associated to each potential slope <span class="math inline">\(\beta_j, j=1,\ldots,p\)</span> at the full model. Because one cannot consider each test separately (FWER effect), what one actually seeks is to set the threshold <span class="math inline">\(\alpha\)</span> so that the proportion of false discoveries, i.e. the number of rejected hypothesis when they are true, is below this threshold.</p>
The proportion of false discovery is relative to the (unknown) number of <span class="math inline">\(\beta_j\neq 0\)</span>, it is hence given by the ratio
<span class="math display">\[\begin{equation}
\mbox{frd}=\frac{V_p}{R_p(\alpha)}
\end{equation}\]</span>
with <span class="math inline">\(V_p\)</span> the (unknown) number of false discoveries and <span class="math inline">\(R_p(\alpha)\)</span> the number of rejected hypothesis, among <span class="math inline">\(p\)</span> hypothesis. frd is set to 0 when <span class="math inline">\(R_p(\alpha)=0\)</span>. For the later, given a level <span class="math inline">\(\alpha\)</span>, one can compute a <span class="math inline">\(p\)</span>-value associated to each null hypothesis <span class="math inline">\(\mathbb{H}_{j0}:\beta_j=0,j=1,\ldots,p\)</span>, say <span class="math inline">\(p_j\)</span>, so that <span class="math inline">\(R_p(\alpha)=\sum_{j=1}^pI(p_j&gt;1-\alpha)\)</span>. The FRD is defined as the expected value of frd (over the <span class="math inline">\(p\)</span>-values distribution), i.e.
<span class="math display">\[\begin{equation}
\mbox{FRD}=\mathbb{E}\left[\frac{V_p}{R_p(\alpha)}\right]
\end{equation}\]</span>
<p>If all <span class="math inline">\(\mathbb{H}_{j0}\)</span> are true, FDR is equivalent to FWER and in general controlling the FRD controls as well the FWER.</p>
<p>In a sequential testing procedure in which the <span class="math inline">\(p\)</span>-values <span class="math inline">\(p_k,k=1,\ldots p\)</span> (and associated null hypotheses) have been ordered in an ascending order, <span class="math inline">\(\mathbb{H}_{0j}, j=k,\ldots,p\)</span> are rejected if <span class="math inline">\(p_k&lt;\alpha_k\)</span> with <span class="math display">\[\alpha_k=\alpha(k+1)/p.\]</span> With this rejection rule, the FDR (<span class="math inline">\(\leq\alpha\)</span>) is controlled <span class="citation">(Benjamini and Hochberg <a href="#ref-BeHo:95">1995</a>)</span>. Hence, in a stepwise procedure for variable selection, the search stops at step <span class="math inline">\(k-1\)</span> if <span class="math inline">\(p_k&gt;\alpha_k\)</span>, <span class="math inline">\(p_k\)</span> being associated to the variable to enter at step <span class="math inline">\(k\)</span>.</p>
<p><span class="citation">G’Sell et al. (<a href="#ref-GSWaChTi:16">2016</a>)</span> propose two other rules to find the cutoff when stopping the stepwise forward search, namely when stopping to reject the null hypothesis. Let <span class="math inline">\(\hat{k}\)</span> denote the last rejected null hypothesis (in the sequence), the rules are:<br />
- ForwardStop: <span class="math inline">\(\hat{k}_F=\max\left\{k\in\{1,\ldots,p\}\vert-\frac{1}{k}\sum_{j=1}^k\log\left(1-p_j\right)\leq\alpha\right\}\)</span><br />
- StrongStop: <span class="math inline">\(\hat{k}_S=\max\left\{k\in\{1,\ldots,p\}\vert \exp\left\{\sum_{j=k}^p\frac{\log\left(p_j\right)}{j}\right\}\leq\frac{k\alpha}{p}\right\}\)</span><br />
with <span class="math inline">\(p_j\)</span> the <span class="math inline">\(p\)</span>-value associated to <span class="math inline">\(\mathbb{H}_j\)</span> and by convention <span class="math inline">\(\hat{k}=0\)</span> whenever no rejections can be made. Both rules control the FDR at level <span class="math inline">\(\alpha\)</span>.</p>
To order the covariates (hence the null hypotheses), one can use partial correlations, or, alternatively, as proposed in <span class="citation">Lin, Foster, and Ungar (<a href="#ref-LiFoUn:11">2011</a>)</span>, to use <span class="math inline">\(\hat{\gamma}_j\)</span>, the least squares estimator of the model
<span class="math display">\[\begin{equation}
\mathbf{e}=\gamma_j\mathbf{x}_j+\tilde{\varepsilon}
\end{equation}\]</span>
with <span class="math inline">\(\mathbf{e}=\mathbf{y}-\mathbf{X}_k\hat{\boldsymbol{\beta}}\)</span> and <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is the least squares estimator of the model at step <span class="math inline">\(k\)</span> (i.e. with <span class="math inline">\(\mathbf{X}_k\)</span>). <span class="citation">Lin, Foster, and Ungar (<a href="#ref-LiFoUn:11">2011</a>)</span> show that <span class="math inline">\(\hat{\gamma}_j=\rho^2\hat{\beta}_j\)</span>, where <span class="math inline">\(\hat{\beta}_j\)</span> is the least squares estimator of <span class="math inline">\(\beta_j\)</span> in the model with covariates matrix <span class="math inline">\(\left[\mathbf{X}_k \; \mathbf{x}_j\right]\)</span> and with
<span class="math display">\[\begin{equation}
\rho^2=\mathbf{x}_j^T(\mathbf{I}-\mathbf{H})\mathbf{x}_j
\end{equation}\]</span>
<p>and <span class="math inline">\(\mathbf{H}=\mathbf{X}_k\left(\mathbf{X}_k^T\mathbf{X}_k\right)^{-1}\mathbf{X}_k^T\)</span>. <span class="math inline">\(p\)</span>-values associated to <span class="math inline">\(\beta_j\)</span> (<span class="math inline">\(\forall j\)</span> corresponding to the columns of <span class="math inline">\(\mathbf{X}_{k+1}\)</span>) can be calculated without re-estimating each potential model at a given step and selection is made on the basis their size (in absolute value).</p>
<blockquote>
Exercise (solutions presented in <a href="solutions.html#HT">7.3.1</a>):<br />
- First of all we retrieve the simulation setting used in <em>Practical 3</em>. Now, after having read the documentation of the R package <em>selectiveInference</em> and installed it, perform the following steps:<br />
a) Use the functions <em>fs()</em>, <em>fsInf()</em> and <em>forwardStop()</em> to do a stepwise regression based on partial correlations and a model selection phase with the ForwardStop rule on your generated data. Try different values for the type one error: how does the choice of <span class="math inline">\(\alpha\)</span> impact the model selection technique?<br />
b) Given the order of variables produced by <em>fs()</em>, use AIC and BIC criteria for model selection to retrieve your final model (<em>Hint: you do not need to program them, use an existing function of the selectiveInference package</em>).<br />
c) Calculate how many models are needed for an exhaustive search in this simulation setting. Use your previous results obtained in <em>Practical 3</em> to understand the computational time gained by stepwise regression with respect to exhaustive search. Use the package <em>tictoc</em> for this comparison.  d) (<em>Optional</em>) Change the simulation setting outlined above to an high dimensional one: generate from a MVN (multivariate normal) a matrix <span class="math inline">\(\mathbf{X_{nxp}}\)</span> with <span class="math inline">\(n = 100\)</span> and <span class="math inline">\(p = 150\)</span>. Evaluate the performance of the ForwardStop rule in this high dimensional setting (i.e. by replicating the model selection task 100 times) thanks to the usual three specific criteria: the proportion of times the correct model is selected (<em>Exact</em>), the proportion of times the selected model contains the correct one (<em>Correct</em>) and the average number of selected regressors (<em>Average</em> <span class="math inline">\(\sharp\)</span>}). What do you observe? What is the role of <span class="math inline">\(\alpha\)</span> in this case?<br />

</blockquote>
<blockquote>
<p>Consider the Malnutrition in Zambia dataset. For simplicity work only on the continuous covariates (i.e. avoiding factors) and order them according to their partial correlations using the R function <em>fs</em> of the <em>Selective Inference</em> R Package (<a href="https://cran.r-project.org/web/packages/selectiveInference/index.html" class="uri">https://cran.r-project.org/web/packages/selectiveInference/index.html</a>). Compare the selected models when using:  </p>
</blockquote>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>the ForwardStop<br />
</li>
<li>the StrongStop<br />
</li>
<li><span class="math inline">\(\alpha_k=\alpha(k+1)/p\)</span> <span class="citation">(Benjamini and Hochberg <a href="#ref-BeHo:95">1995</a>)</span><br />
</li>
<li>the <span class="math inline">\(C_p\)</span> or AIC (equal in linear case)<br />
</li>
<li>the BIC</li>
</ol>
</blockquote>
</div>
<div id="orthogonal-matching-pursuit" class="section level3">
<h3><span class="header-section-number">3.2.3</span> Orthogonal matching pursuit</h3>
<p>The Orthogonal Matching pursuit algorithm (OMP) is a greedy algorithms for least squares regression, often called forward greedy selection in the machine learning literature. <span class="citation">Zhang (<a href="#ref-zhang2009">2009</a>)</span> establishes the conditions for the algorithm (see below) to find the correct feature set <span class="math inline">\(\{j\;\vert \; \beta_j\neq 0\}\)</span> (when <span class="math inline">\(n\rightarrow \infty\)</span>).</p>
<hr />
<p><strong>Orthogonal matching pursuit (OMP) algorithm</strong>:<br />
1. Inputs: <span class="math inline">\(\mathbf{X}=[\mathbf{x}_{1},\dots,\mathbf{x}_{p}]\)</span>, of size <span class="math inline">\(n\times p\)</span>, <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\epsilon&gt;0\)</span><br />
2. Outputs: <span class="math inline">\(S^{(\ell)}\)</span> and <span class="math inline">\(\boldsymbol{\beta}_{S^{(\ell)}}\)</span><br />
3. Initial steps:<br />
- <span class="math inline">\(\mathbf{\tilde{x}}_j=\mathbf{x}_{j}/\|\mathbf{x}_{j}\|_2\)</span>, <span class="math inline">\(\forall j=1,\dots,p\)</span>,<br />
- <span class="math inline">\(S^{(0)}=\emptyset\)</span>,<br />
- <span class="math inline">\(\mathcal{P}=\{1,\ldots,p\}\)</span>,<br />
- <span class="math inline">\(\hat{\boldsymbol{\beta}}_{S^{(0)}}=\mathbf{0}\)</span><br />
4. For<span class="math inline">\(\ell = 1, 2, \ldots, p\)</span>, do:<br />
- Set <span class="math inline">\(j^{(\ell)}=\arg\max_{j\in \mathcal{P}\setminus S^{(l-1)}}|\mathbf{\tilde{x}}_j^T(\mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}}_{S^{(\ell-1)}})|\)</span>,<br />
-  <span class="math inline">\(|\mathbf{\tilde{x}}_{j^{(\ell)}}^T(\mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}}_{S^{(\ell-1)}})|\leq \epsilon\)</span> <strong>break</strong>,<br />
– Set <span class="math inline">\(S^{(\ell)}=\{j^{(\ell)}\}\cup S^{(\ell-1)}\)</span> and <span class="math inline">\(\mathbf{X}_{S^{(\ell)}}\)</span> the column subset of <span class="math inline">\(\mathbf{X}\)</span> with all columns <span class="math inline">\(j\in S^{(\ell)}\)</span>,<br />
– Set <span class="math inline">\(\hat{\boldsymbol{\beta}}_{S^{(\ell)}}=\left(\mathbf{X}_{S^{(\ell)}}^T\mathbf{X}_{S^{(\ell)}}\right)^{-1}\mathbf{X}_{S^{(\ell)}}^T\mathbf{y}\)</span>.</p>
<hr />
<p>One can see that in the search loop (stage no 4), the variable to enter is chosen as the one to maximize the partial correlation with the response, conditioned on the current variables (selected until the previous step). Indeed <span class="math inline">\(\mathbf{\tilde{x}}_{j^{(\ell)}}^T(\mathbf{y}-\mathbf{X}\hat{\boldsymbol{\beta}}_{S^{(\ell-1)}})=\)</span> <span class="math inline">\(\mathbf{\tilde{x}}_{j^{(\ell)}}^T\left(\mathbf{y}-\mathbf{X}_{S^{(\ell-1)}}\left(\mathbf{X}_{S^{(\ell-1)}}^T\mathbf{X}_{S^{(\ell-1)}}\right)^{-1}\mathbf{X}_{S^{(\ell-1)}}^T\mathbf{y}\right)=\)</span> <span class="math inline">\(\mathbf{\tilde{x}}_{j^{(\ell)}}^T\left(\mathbf{I}-\mathbf{H}_{S^{(\ell-1)}}\right)\mathbf{y}\)</span>, and hence <span class="math inline">\(\mathbf{\tilde{x}}_{j^{(\ell)}}\)</span> is selected to maximize the drop in the residual sum of squares based on <span class="math inline">\(S^{(\ell)}\)</span>. Therefore, the OMP can be used in conjunction with a <em>model validity</em> estimator based on the residual sum of squares (L2 norm loss function) such as the <span class="math inline">\(C_p\)</span>, i.e. instead of of <span class="math inline">\(\epsilon\)</span> in the OMP.</p>
<p>There is therefore no difference between a stepwise forward regression and the OMP in terms of the variables that are chosen at each step. The only difference is that forward stepwise regression is associated with testing or optimizing a model validity criterion, while the OMP is associated to another criterion <span class="math inline">\(\epsilon\)</span>.</p>
</div>
<div id="forward-stagewise-regression" class="section level3">
<h3><span class="header-section-number">3.2.4</span> Forward stagewise regression</h3>
<p>Forward stagewise regression is a <em>smoother</em> variation of stepwise regression that has been overlooked for a while but is coming back in high dimensional settings. It is also based on partial correlations, but at each step the chosen variable is not entered <em>fully</em>. It starts with all coefficients <span class="math inline">\(\beta_j\)</span> equal to zero, and iteratively updates the <span class="math inline">\(\beta_j\)</span> by a small amount <span class="math inline">\(\epsilon\)</span> of the variable <span class="math inline">\(X_j\)</span> that achieves the maximal absolute correlation with the current residuals.</p>
<hr />
<p><strong>Forward Stagewise Regression algorithm</strong>:<br />
1. Inputs: <span class="math inline">\(\mathbf{X}=[\mathbf{x}_{1},\dots,\mathbf{x}_{p}]\)</span>, of size <span class="math inline">\(n\times p\)</span>, <span class="math inline">\(\mathbf{y}\)</span>, <span class="math inline">\(\epsilon&gt;0\)</span>, <span class="math inline">\(\Delta\)</span><br />
2. Outputs: <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span><br />
3. Initial steps:<br />
- standardize all covariates <span class="math inline">\(\rightarrow \mathbf{\tilde{x}}_{j},j=1,\ldots,p\)</span><br />
- Set <span class="math inline">\(\hat{\boldsymbol{\beta}}^{(0)}=\mathbf{0}\)</span><br />
- Set <span class="math inline">\(\mathbf{e}_0=\mathbf{y}-\bar{y}\)</span>, <span class="math inline">\(\bar{y}=1/n\sum_{i=1}^n y_i\)</span><br />
4. For <span class="math inline">\(k =1, 2, \ldots\)</span>, do:<br />
- <span class="math inline">\(j=\arg\max_{j}|\mathbf{\tilde{x}}_j^T\mathbf{e}_{k-1}|\)</span>,<br />
- if <span class="math inline">\(|\mathbf{\tilde{x}}_j^T\mathbf{e}_{k-1}|\leq \Delta\)</span> then stop - <span class="math inline">\(\delta_j=\epsilon\;\mbox{sign}\left(\mathbf{\tilde{x}}_j^T\mathbf{e}_{k-1}\right)\)</span><br />
- Set <span class="math inline">\(\hat{\beta}_l^{(k+1)}\leftarrow \beta_l^{(k)}+\delta_jI(l=j), l=1,\ldots,p\)</span><br />
- Set <span class="math inline">\(\mathbf{e}_k\leftarrow \mathbf{e}_{k-1}-\delta_j\mathbf{\tilde{x}}_j\)</span></p>
<hr />
<p>There is no clear rule for setting <span class="math inline">\(\epsilon\)</span> and <span class="math inline">\(\Delta\)</span>, which makes the forward stagewise regression algorithm more an <em>exploratory</em> method. Obviously, <span class="math inline">\(\Delta\)</span> should be small (e.g. <span class="math inline">\(0.0001\)</span>), while the smaller <span class="math inline">\(\epsilon\)</span>, the more steps are needed to reach the end of the algorithm. There is actually a strong connection between the sequence of forward stagewise estimates and the solution path of the lasso; see <span class="citation">Efron et al. (<a href="#ref-EfHaJoTi:04">2004</a>)</span>, and also <span class="citation">Hastie et al. (<a href="#ref-HaTaTiWa:07">2007</a>)</span>, <span class="citation">Tibshirani (<a href="#ref-TibR:15">2015</a>)</span>.</p>
<blockquote>
Exercise (optional):<br />
- Show that an ordering procedure based on stagewise regression is biased and calculate this bias for the OLS coefficient of the <span class="math inline">\(x_j\)</span> predictor.<br />
Hint: Use the proof of Proposition I of <span class="citation">Lin, Foster, and Ungar (<a href="#ref-LiFoUn:11">2011</a>)</span>.<br />

</blockquote>
</div>
</div>
<div id="streamwise-regression" class="section level2">
<h2><span class="header-section-number">3.3</span> Streamwise regression</h2>
<div id="introduction-4" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Introduction</h3>
<p>Stepwise forward selection is a procedure that finds a good approximation of the the model (formed by a subset of the available covariates) that minimizes a chosen criterion (e.g. the <span class="math inline">\(C_p\)</span>). It involves however, at each step <span class="math inline">\(k\)</span>, the comparison of <span class="math inline">\(p-k\)</span> models which can be computationally very challenging.</p>
<p>Streamwise regression (see e.g. <span class="citation">Zhou et al. (<a href="#ref-ZhFoStUn:06">2006</a>)</span> and the references therein), is a variation of stepwise regression in which covariates (predictive features) are tested sequentially for addition to the current model (or not). Because it considers each potential covariate only once, it is extremely fast.</p>
<p>The question that arises then is, if stepwise regression is already an approximation to the <em>optimal</em> model in terms of <em>model validity</em>, how can streamwise regression, which is by construction less accurate (in terms of approximating the <em>optimum</em>) still be valid in some sense. There is trade-off between <em>usable</em> or <em>workable</em> procedures and reaching an <em>optimum</em> in high dimensions (where any, even reasonable, procedure can fail). This discussion, still widely open, goes beyond the scope of the focus of this eBook.</p>
<p>Streamwise regression can be bound to hypothesis testing, in that the focus is placed on selecting the largest set of covariates so that their significance (rejecting their associated null hypothesis) is valid up to a <em>global risk</em>. These procedures mimic somehow the situation in which a global model could be estimated (not possible, so far, when <span class="math inline">\(n&lt;p\)</span>) and the useful covariates, in terms of model validity, could be identified as the ones for which <span class="math inline">\(\mathbb{H}_{0j}=0\)</span> can be rejected. Care needs to be taken about not only the FWER (bound to multiple hypothesis testing) and the fact that any theoretical result (bound to the hypothesis to be tested) needs to be implemented using estimated quantities from the sample at hand, hence introducing bias and overfitting.</p>
</div>
<div id="sure-independence-screening" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Sure Independence Screening</h3>
<p><span class="citation">Fan and Lv (<a href="#ref-FaLi:08">2008</a>)</span> introduce the concept of <em>sure screening</em> and propose a sure screening method based on <em>correlation learning</em> which filters out the features that have weak correlation with the response. The resulting method is called <em>sure independence screening</em> (SIS). Sure screening is a property that insures all the important variables survive after variable screening with probability tending to 1.</p>
<p>An important feature in high dimensions is that spurious correlations (sample correlations) between predictors can be very large. This implies that using the (sample) correlations (partial or marginal) induces a form of decisional bias because the sample (observed) quantities are misleading. For example, important predictors can be found to be highly correlated with some unimportant ones, which then makes selection tend to include more predictors than necessary. <span class="citation">Fan and Lv (<a href="#ref-FaLi:08">2008</a>)</span> provide a simulations study in which <span class="math inline">\(p\)</span> independent standard normal predictors are simulated and the maximum absolute sample correlation coefficient between predictors is considered. In the Figure below are represented these quantities computed on <span class="math inline">\(500\)</span> datasets with <span class="math inline">\(n=60\)</span> and <span class="math inline">\(p=1000\)</span> and <span class="math inline">\(p=5000\)</span>.</p>
<blockquote>
<p>Reported from <span class="citation">Fan and Lv (<a href="#ref-FaLi:08">2008</a>)</span> <img src="Figures/FanLi08-Fig1.png" alt="Top" /></p>
</blockquote>
In the SIS framework, all covariates <span class="math inline">\(X_j,j=1,\ldots,p\)</span> are standardized (mean 0, variance 1). It is also assumed that the true model is sparse, i.e. the true dimension <span class="math inline">\(s\)</span> is such that <span class="math inline">\(s&lt;&lt;p\)</span>. Let <span class="math inline">\(\mathcal{M}_s = \{j=1,\ldots,p \; \vert \; \beta_j \neq 0 \}\)</span> be the true sparse model with non-sparsity size <span class="math inline">\(s=\vert \mathcal{M}_s\vert\)</span>. Let also <span class="math inline">\(\mathcal{M}_\gamma\)</span> denote a model selected on the basis of a (streamwise) selection procedure, the sure screening property ensures that
<span class="math display">\[\begin{equation}
P\left(\mathcal{M}_s\ \subseteq \mathcal{M}_\gamma\right)\rightarrow 1
\end{equation}\]</span>
The SIS procedure proposed by <span class="citation">Fan and Lv (<a href="#ref-FaLi:08">2008</a>)</span> ranks the importance of covariates (features) according to their marginal correlation with the response variable and filters out those that have weak marginal correlations with the response variable. Let
<span class="math display">\[\begin{equation}
\boldsymbol{\omega}=(\omega_1,\ldots,\omega_p) = \mathbf{X}^T\mathbf{y}
\end{equation}\]</span>
hence, the vector of marginal correlations of predictors with the response variable (rescaled by the standard deviation of the response <span class="math inline">\(\mathbf{y}\)</span>). For any given <span class="math inline">\(\gamma\in (0,1)\)</span>, the SIS selected model is
<span class="math display">\[\begin{equation}
\mathcal{M}_\gamma=\{j=1,\ldots,p \;\vert \; \vert \omega_j\;\vert \; \mbox{is among the first} \lfloor \gamma n \rfloor \; \mbox{largest}\}
\end{equation}\]</span>
<p>where <span class="math inline">\(\lfloor \gamma n \rfloor\)</span> denotes the integer part of <span class="math inline">\(\gamma n\)</span>. The SIS is actually used to shrink the full model of size <span class="math inline">\(p\)</span> down to a submodel <span class="math inline">\(\mathcal{M}_\gamma\)</span> with size <span class="math inline">\(d =\lfloor \gamma n \rfloor&lt;n\)</span>. There is a link between SIS and <em>ridge regression</em> (<em>Tikhonov regularization</em>), a shrinkage method that will be presented in Chapter 4.</p>
<p><span class="citation">Fan and Lv (<a href="#ref-FaLi:08">2008</a>)</span> set up the conditions (Theorem 1) for the SIS to have the sure screening property, for suitable (technical) choices of <span class="math inline">\(\gamma\)</span>. In particular, <span class="math inline">\(\boldsymbol{\Sigma}^{^1/2}\mathbf{X}, \boldsymbol{\Sigma}=\mbox{cov}\left(\mathbf{X}\right), \mathbf{X}=\left[X_j\right]_{j=1,\ldots,p}\)</span> has a spherically symmetric distribution.</p>
<p>The SIS procedure can be combined with another (subset) selection method: apply the SIS to reduce the dimension, apply a selection method to the reduced dimension to get, say, <span class="math inline">\(\mathcal{M}_{k_1}\)</span> of size of order e.g. <span class="math inline">\(n/log(n)\)</span>. The residuals from a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(\mathbf{X}=[X_j],j\in \mathcal{M}_{k_1}\)</span> are then used as a new response vector and the SIS is applied to the left-aside <span class="math inline">\(p-k_1\)</span> covariates to get <span class="math inline">\(\mathcal{M}_{k_2}\)</span>. The rationale behind is that using the residuals from the previous step, amounts to considering partial correlations which set aside the unimportant covariates that are highly correlated with the response through their associations with the covariates present in the set <span class="math inline">\(\mathcal{M}_{k_1}\)</span> and also makes those important predictors that are missed in the previous step possible to survive. The procedure is iterated until the <span class="math inline">\(L\)</span> disjoint subsets <span class="math inline">\(\mathcal{M}_{k_l},l=1,\ldots,L\)</span> whose union <span class="math inline">\(\mathcal{M}_d\)</span> has maximal size satisfying e.g. <span class="math inline">\(\vert \mathcal{M}_d\vert&lt;n\)</span>. This procedure is called the iterative sure independence screening (ISIS).</p>
<blockquote>
<p>Exercise (solutions presented in <a href="solutions.html#SIS">7.3.2</a>):<br />
After having read the documentation of the R package <em>SIS</em> and installed it, perform the following steps:<br />
a) Load the <em>Leukemia</em> dataset.<br />
b) Split the dataset randomly and create a <em>train</em> and <em>test</em> sample.<br />
c) The functions SIS() performs first a screening procedure based on marginal correlations and then applies a penalized method (<em>Chapter 4</em> of the e-book) to obtain the final model. Choose among all the available options (i.e. in terms both of penalized methods and tuning constants) three candidates and evaluate the predictions of the selected models on the <em>test</em> sample. Which penalized method performs best in this specific example after the SIS?<br />
(optional) Explain why stepwise regression based on partial correlations (for the inclusion criterion) is a special case of ISIS.</p>
</blockquote>
</div>
<div id="pc-simple-algorithm" class="section level3">
<h3><span class="header-section-number">3.3.3</span> PC-simple algorithm</h3>
<p><span class="citation">Bühlmann, Kalisch, and Maathuis (<a href="#ref-BuKaMa:10">2010</a>)</span> use partial correlations to stream through the covariates and develop an algorithm that is a simplified version of the PC algorithm <span class="citation">(Spirtes, Glymour, and Scheines <a href="#ref-SpGlSc:00">2000</a>)</span> used in Bayesian network modeling (Causal Bayesian Network). Moreover, they provide a criterion based on hypothesis testing to decide if a covariate is selected or not.</p>
<p>The PC-simple algorithm is given by the following steps:<br />
1. Define the <em>active set</em> at step m as <span class="math inline">\(\mathcal{M}_m\)</span> and set m = 1.<br />
2. Do correlation screening, and build the active set <span class="math inline">\(\mathcal{M}_1 = \{j=1,\ldots,p \; \vert \; \rho_{X_jY}\neq 0 \}\)</span>.<br />
3. m = m + 1.<br />
4. Construct the <span class="math inline">\(m\)</span> active set <span class="math inline">\(\mathcal{M}_m =\{j \in \mathcal{M}_{m-1}\; \vert \;\rho_{X_jY,\mathbf{X}_{C}}\neq 0, \; \mathbf{X}_{C}=[X_l],l\in\mathcal{M}_{m-1}\setminus\{j\}\}\)</span>.<br />
5. Repeat from step 3 as long as <span class="math inline">\(\vert \mathcal{M}_m\vert \leq m\)</span>.<br />
6. Output: <span class="math inline">\(m_S=\{m\;\vert\; \vert \mathcal{M}_m\vert \leq m\}\)</span>.</p>
<p>The PC-simple algorithm hence starts (i.e. m=1) with a streamwise regression based on marginal correlations which are a particular case of partial correlations with empty conditioning set. In the next step (i.e. m=2) all the possible first order partial correlation, of the variables present in <span class="math inline">\(\mathcal{M}_1\)</span>, are evaluated to exclude all the covariates for with at least one first order partial correlation that is equal to zero. Then a new active set is obtained such that <span class="math inline">\(\mathcal{M}_2\subseteq \mathcal{M}_1\)</span>. Thus, at a given step m, there is an evaluation of the (m-1)-th order partial correlations, of the variables present in <span class="math inline">\(\mathcal{M}_{m-1}\)</span>, to reduce the active set to <span class="math inline">\(\mathcal{M}_m\subseteq \mathcal{M}_{m-1}\)</span>. The procedure stops when an equilibrium is reached, namely when the number of covariates in the active set at step <span class="math inline">\(m\)</span> is the closest (and inferior) to <span class="math inline">\(m\)</span>.</p>
Since partial correlations need to be estimated from the available data, they are subject to sampling error, hence even if <span class="math inline">\(\rho_{\cdot\;\cdot,\cdot}= 0\)</span>, <span class="math inline">\(\hat{\rho}_{\cdot\;\cdot,\cdot}\neq 0\)</span>. <span class="citation">Bühlmann, Kalisch, and Maathuis (<a href="#ref-BuKaMa:10">2010</a>)</span> propose to apply Fisher’s <span class="math inline">\(Z\)</span>-transformation <span class="citation">(Fisher <a href="#ref-Fish:15">1915</a>)</span>
<span class="math display">\[\begin{equation}
Z_{X_jY,\mathbf{X}_C}=\frac{1}{2}\log\left(\frac{1+\hat{\rho}_{X_jY,\mathbf{X}_{C}}}{1-\hat{\rho}_{X_jY,\mathbf{X}_{C}}}\right)
\end{equation}\]</span>
<p>When testing (two-sided) <span class="math inline">\(\mathbb{H}_0:\rho_{X_jY,\mathbf{X}_{C}}= 0\)</span>, <span class="math inline">\(\mathbb{H}_0\)</span> is rejected when <span class="citation">(Fisher <a href="#ref-Fish:21">1921</a>)</span> <span class="math inline">\(\sqrt{n-(m-1) -3} \vert Z_{X_jY,\mathbf{X}_C}\vert &gt; \Phi^{-1}\left(1-\alpha/2\right)\)</span>, where <span class="math inline">\(\alpha\)</span> is the significance level, <span class="math inline">\(\Phi\)</span> is the standard Normal cumulative distribution function and m the current step of the procedure.</p>
Partial correlations can be calculated recursively <span class="citation">(Bühlmann, Kalisch, and Maathuis <a href="#ref-BuKaMa:10">2010</a>)</span> using
<span class="math display">\[\begin{equation}
\hat{\rho}_{X_jY,\mathbf{X}_{C}}=\frac{\hat{\rho}_{X_jY,\mathbf{X}_{C\setminus\{k\}}}-\hat{\rho}_{X_kY,\mathbf{X}_{C\setminus\{k\}}}\hat{\rho}_{X_jX_k,\mathbf{X}_{C\setminus\{k\}}}}{\sqrt{\left(1-\hat{\rho}_{X_kY,\mathbf{X}_{C\setminus\{k\}}}^2\right)\left(1-\hat{\rho}_{X_jX_k,\mathbf{X}_{C\setminus\{k\}}}^2\right)}}
\end{equation}\]</span>
<p>To assess that the (population version) of the PC-simple algorithm identifies the true underlying active set, i.e. <span class="math inline">\(\mathcal{M}_{m_S} = \{j = 1,\ldots,p \; \vert\; \beta_j\neq 0 \}\)</span>, <span class="citation">Bühlmann, Kalisch, and Maathuis (<a href="#ref-BuKaMa:10">2010</a>)</span> make use of the <em>partial faithfulness</em> concept. It applies to models that are such that if <span class="math inline">\(\forall j\)</span> (with <span class="math inline">\(j=1,\ldots,p\)</span>) <span class="math inline">\(\rho_{X_jY,\mathbf{X}_{C}}= 0\)</span>, for some <span class="math inline">\(\mathcal{M}_C\subseteq \mathcal{M}_p\setminus \{j\}\)</span>, implies that <span class="math inline">\(\beta_j=0\)</span> (with <span class="math inline">\(\mathcal{M}_p=\{j=1,\ldots,p\}\)</span>). Partial faithfulness can only be verified when assuming a (joint) distribution for the covariates.</p>
<blockquote>
<p>Exercise (solutions presented in <a href="solutions.html#PC">7.3.3</a>):<br />
First of all build a simulation setting as explained below:<br />
- Generate from a MVN (multivariate normal) a matrix <span class="math inline">\(\mathbf{X_{n*p}}\)</span> with <span class="math inline">\(n = 1000\)</span> and <span class="math inline">\(p = 10\)</span>. Choose the location but set the scale matrix with an autoregressive form <span class="math inline">\(\boldsymbol{\Sigma}=[\sigma_{lm}]_{l,m=1,\ldots,p}\)</span> with <span class="math inline">\(\sigma_{lm} = \rho^{\mid l - m\mid}\)</span>.  - Fix <span class="math inline">\(\rho = 0.5\)</span> and set the seed equal to 11 (i.e. set.seed(11)).<br />
- Choose the generating vector <span class="math inline">\(\boldsymbol{\beta }= [3 \; 1.5 \; 0 \; 2 \; rep(0,6)]\)</span>.<br />
- Generate <span class="math inline">\(\mathbf{\hat{y}}\)</span> thanks to the relation <span class="math inline">\(\mathbf{y} = \mathbf{X_{n*p}} \; \boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span> where <span class="math inline">\(\epsilon_{i}\)</span> is a standard normal. Suppose for simplicity that the errors are uncorrelated.<br />
Now perform the following passages on your simulated data:<br />
a) Find the active set <span class="math inline">\(M_{1}\)</span> using the Fisher’s Z transformation and the associated correlation coefficient test (fix <span class="math inline">\(\alpha = 0.05\)</span> for the rest of the exercise).<br />
b) Calculate all the partial correlations of order 1 (i.e. one variable at the time) of the active set <span class="math inline">\(M_{1}\)</span>, test them and retrieve <span class="math inline">\(M_{2} \subseteq M_1\)</span> which is the new active set.<br />
c) Find the partial correlations of higher order and test them until your reach the condition <span class="math inline">\(M_{m-1} = M_{m}\)</span> which implies the convergence of the PC-simple algorithm. Do you obtain the exact model?</p>
</blockquote>
<blockquote>
<p>Exercise (project):<br />
- Write the PC-simple algorithm in R using the recursive formula for partial correlations.<br />
- Perform a simulation study to evaluate the performance (model selection criteria) both when the covariates are normally distributed and when they are not (consider e.g. factors, i.e. dummies).<br />
- Study the sensitivity of the PC-simple as a function of <span class="math inline">\(\alpha\)</span>.<br />
- Apply the algorithm for variable selection in a chosen dataset.<br />
- Compare with another variable selection method (implemented).</p>
</blockquote>
</div>
</div>
<div id="classification-and-regression-tree-cart" class="section level2">
<h2><span class="header-section-number">3.4</span> Classification And Regression Tree (CART)</h2>
<p>Note: The Section is highly inspired from <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-HaTiFr:09">2009</a>)</span>, chapter 9.</p>
<p>The regression model involves estimating the <em>conditional mean</em> of the response variable <span class="math inline">\(Y\)</span> given a set of predictors <span class="math inline">\(X_j, j=1,\ldots,p\)</span> (collected in the <span class="math inline">\(n\times p\)</span> <em>design matrix</em> <span class="math inline">\(\mathbf{X}\)</span>), i.e. <span class="math inline">\(\mu(\mathbf{X})=\mathbf{X}\beta, \dim(\beta)=p\)</span>. The parameters <span class="math inline">\(\beta\)</span> represent the slopes of the linear regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>The basis of CART <span class="citation">(Breiman <a href="#ref-Brei:84">1984</a>)</span> is to approximate these slopes by partitioning the observed predictors realizations into consecutive sets, for which the observed mean response is computed. More precisely, tree-based methods partition the predictors’ space (feature space) into a set of rectangles, and then fit a simple model (i.e. computes the mean) in each one.</p>
<p>The partitioning is done sequentially, with usually a binary partitioning (i.e. in two consecutive parts), one predictor at the time. The Figure below shows such a sequence in the case of two predictors <span class="math inline">\(X_1, X_2\)</span>, with splitting values <span class="math inline">\(\theta_m\)</span> and (split) regions <span class="math inline">\(R_m, m=1,\ldots,M, M=5\)</span>.</p>
<hr />
<p><strong>Illustration of the recursive binary partitioning</strong><br />
<br></p>
<center>
<img src="Figures/RecursiveBiPart.png" alt="Top" />
</center>
<hr />
<p>A regression tree algorithm has the following characteristics that need to be set:<br />
- The choice of the splitting variable <span class="math inline">\(X_j\)</span> (from which the split is done)in each region)<br />
- criterion for the splitting value <span class="math inline">\(t_m:=\theta_m\)</span><br />
- the model (predictor) that is fitted in each region <span class="math inline">\(R_m\)</span><br />
- the size of the tree <span class="math inline">\(M\)</span><br />
</p>
<div id="regression-tree" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Regression tree</h3>
In the linear regression model, a constant model is fitted in each region <span class="math inline">\(R_m\)</span>, i.e. the <em>mth</em> stage of the algorithm is
<span class="math display">\[\begin{equation}
\hat{\mu}(\mathbf{X})= \sum_{m=1}^M c_m(y_i\vert \mathbf{x}_i\in R_m)
\end{equation}\]</span>
with
<span class="math display">\[\begin{equation}
c_m(y_i\vert \mathbf{x}_i\in R_m)=\frac{1}{n_m}\sum_i^n y_i I(\mathbf{x}_i\in R_m), \\ n_m=\sum_i^n I(\mathbf{x}_i\in R_m)
\end{equation}\]</span>
<p>This amounts to consider, as predictor for the response, the sum of <span class="math inline">\(m\)</span> local predictors computed as the mean responses <span class="math inline">\(c_m(y_i\vert \mathbf{x}_i\in R_m)\)</span> in each region <span class="math inline">\(R_m\)</span>. The slopes <span class="math inline">\(\beta_j\)</span> are then never estimated (even locally) which results in a important gain in computational time.</p>
At stage of the algorithm, two new regions are created, namely one of the former region is split into two parts. This is done for each region. A choice for the covariate <span class="math inline">\(X_j\)</span> to consider for the spiting as well as the splitting value <span class="math inline">\(t_{mj}\)</span> needs to be done, and the criterion is the minimization of the residual sum of squares (RSS)
<span class="math display">\[\begin{equation}
\sum_{k=1}^m\sum_{i \in R_k}\left(y_i-c_k(y_i\vert \mathbf{x}_i\in R_k)\right)^2
\end{equation}\]</span>
Since the contributions to the RSS of each region are computed separately (the <span class="math inline">\(c_k(y_i\vert \mathbf{x}_i\in R_k)\)</span>), the search for an <em>optimum</em> can be conducted region by region, in which both the splitting variable <span class="math inline">\(X_j\)</span> and value <span class="math inline">\(t_{mj}\)</span> are found that minimize the RSS in a given region. More precisely, let, at stage <span class="math inline">\(m\)</span>, <span class="math inline">\(R_{m_1}(j,t_{mj})\)</span> and <span class="math inline">\(R_{m_2}(j,t_{mj})\)</span> the regions resulting from the split of one region on the basis of one of the covariates <span class="math inline">\(X_j\)</span> and according to a splitting scalar <span class="math inline">\(t_{mj}\)</span>, then <span class="math inline">\(j\)</span> and <span class="math inline">\(t_{mj}\)</span> are found using
<span class="math display">\[\begin{eqnarray}
\min_{j,t_{mj}}&amp; \left\{\sum_{i \in R_{m_1}(j,t_{mj})} \left(y_i-c_m(y_i\vert \mathbf{x}_i\in R_{m_1}(j,t_{mj}))\right)^2 \right. \\  &amp; \left.   + 
\sum_{i \in R_{m_2}(j,t_{mj})} \left(y_i-c_m(y_i\vert \mathbf{x}_i \in R_{m_2}(j,t_{mj}))\right)^2\right\}
\end{eqnarray}\]</span>
<p>For the size of the tree, one approach is to grow the tree until each region is very small, say of size <span class="math inline">\(n_m\approx 5\)</span>, and this large tree, say <span class="math inline">\(T_0\)</span>, is then <em>prunned</em> using <em>cost-complexity pruning</em> based on a criterion <span class="math inline">\(C_{\alpha}(T)\)</span>.</p>
Pruning generates a series of trees <span class="math inline">\(T_{k}\subset T_{k-1}\subset \ldots\subset T_{1}\subset T_0\)</span>, obtained (sequentially) removing a subtree from the previous tree. A subtree is removed by removing a splitting (and the following ones) to create a collapsed region, also called leaf. The choice for the subtree to split, hence the resulting tree, say <span class="math inline">\(T\)</span> of size <span class="math inline">\(\vert T\vert\)</span>, is done using a criterion based on the RSS, i.e.
<span class="math display">\[\begin{equation}
C(T)=\sum_{m=1}^{\vert T\vert}\sum_{i\in R_m(T)}\left(y_i-c_m(T)\right)^2 
\end{equation}\]</span>
where <span class="math inline">\(R_m(T)\)</span> is the <span class="math inline">\(m\)</span>th region (leaf) of tree <span class="math inline">\(T\)</span> and <span class="math inline">\(c_m(T)=\frac{1}{n_m}\sum_i^n y_i I(\mathbf{x}_i\in R_m(T))\)</span> is the average response in region <span class="math inline">\(R_m(T)\)</span>. <span class="math inline">\(C(T)\)</span> needs however to be constrained to avoid overfitting (since the largest possible tree is always the one that minimizes <span class="math inline">\(C(T)\)</span>), so that one uses a penalized criterion given by <span class="citation">(Hastie, Tibshirani, and Friedman <a href="#ref-HaTiFr:09">2009</a>)</span>
<span class="math display">\[\begin{equation}
C_{\alpha}(T)=C(T) + \alpha \vert T\vert
\end{equation}\]</span>
<p>Hence, the idea is to find, for each <span class="math inline">\(\alpha_k&gt;\alpha_{k-1}&gt;\ldots &gt; \alpha_1&gt; \alpha_0=0\)</span>, the (unique) subtree <span class="math inline">\(T_{\alpha_k} \subset T_{\alpha_{k-1}}\)</span> that minimizes <span class="math inline">\(C_{\alpha}(T)\)</span>. The tuning parameters <span class="math inline">\(\alpha_k\)</span> govern the trade off between tree size (to avoid overfitting) and its goodness of fit to the data. The sequence of <span class="math inline">\(\alpha_k\)</span> can be constructed starting from <span class="math inline">\(\alpha_1\)</span>, and the sequence stops when a criterion such as tenfold cross-validation is optimized.</p>
<p>One major problem with trees is their high variance. Often a small change in the data can result in a very different series of splits, making interpretation somewhat precarious. Averaging methods such as <em>bagging</em>, i.e. <em>bootstrap aggregating</em>, can be used to reduce the variability of the tree estimator <span class="citation">(Breiman <a href="#ref-Brei:96">1996</a>)</span>. See <span class="citation">Sutton (<a href="#ref-Sutt:05">2005</a>)</span> for a survey.</p>
<blockquote>
<p>Exercise (solutions presented in <a href="solutions.html#RT">7.3.4</a>):<br />
After having read the documentation of the <em>rpart</em> package and having installed it, perform the following steps:  </p>
</blockquote>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Load the <em>Zambia</em> dataset, split it randomly in a <em>train</em> and <em>test</em> sample (common choice is <span class="math inline">\(\frac{2}{3}\)</span> train and <span class="math inline">\(\frac{1}{3}\)</span> test). For simplicity, you can consider only the continuous variables.<br />
</li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Fit a regression tree with the function rpart() and plot the tree. Have a look at <em>rpart.plot</em> package if you want to improve the appearance of the fitted tree.  </li>
</ol>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>After having pruned the tree, evaluate its prediction on the <em>test</em> sample (i.e. use predict() on a tree object.</li>
</ol>
</blockquote>
</div>
<div id="classification-trees" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Classification Trees</h3>
<p>Like for subset selection, if the target is a classification outcome taking values <span class="math inline">\(q=1, 2,\ldots,Q\)</span>, the criterion for splitting (growing) and pruning the tree, , i.e. the RSS, needs to be adapted.</p>
<p>Let <span class="math inline">\(p_{mq}=1/n_m\sum_{\mathbf{x}_i\in R_m}I(y_i= q)\)</span> be the proportion of class <span class="math inline">\(q\)</span> observations in region <span class="math inline">\(R_m\)</span> and let <span class="math inline">\(q_m=\mbox{argmax}_q p_{mq}\)</span>, i.e. the most frequent class in region <span class="math inline">\(R_m\)</span>. Different alternatives to the RSS (i.e. risk measures) exist and include the following ones, specified for the region <span class="math inline">\(R_m\)</span>:<br />
- <em>Misclassification error</em>: <span class="math inline">\(\mbox{err}_m=\frac{1}{n_m}\sum_{i\in R_m}I(y_i\neq q_m)=1-p_{mq_m}\)</span><br />
- <em>Gini index</em>: <span class="math inline">\(\mbox{err}_m=\sum_{q=1}^Qp_{mq}(1-p_{mq})= \sum_{q=1}^Q\sum_{q&#39;\neq q}p_{mq}p_{mq&#39;}\)</span><br />
- <em>Cross-entropy or deviance</em>: <span class="math inline">\(\mbox{err}_m=- \sum_{q=1}^Qp_{mq}\log(p_{mq})\)</span></p>
<p>The total risk or error is obtained as <span class="math inline">\(\sum_{m=1}^R\mbox{err}_m\)</span>. The deviance and the Gini index have the advantage of being differentiable functions, and hence more amenable to numerical optimization.</p>
<blockquote>
<p>Exercise (solutions presented in <a href="solutions.html#CT">7.3.5</a>):<br />
- After having read the documentation of the <em>rpart</em> package and having installed it, perform the following steps:  a) Load the <em>Iris</em> dataset already present in R, split it randomly in a <em>train</em> and <em>test</em> sample (common choice is <span class="math inline">\(\frac{2}{3}\)</span> train and <span class="math inline">\(\frac{1}{3}\)</span> test). </p>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Fit a classification tree with the function rpart() and plot the tree. Have a look at <em>rpart.plot</em> package if you want to improve the appearance of the fitted tree. </li>
</ol>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>After having pruned the tree, evaluate its prediction on the <em>test</em> sample (i.e. use predict() on a tree object). </li>
</ol>
<ul>
<li>(Optional) Analyse the Leukemia dataset by means of the regression tree method. Use the <em>rpart</em> package in R (see also <a href="https://www.statmethods.net/advstats/cart.html" class="uri">https://www.statmethods.net/advstats/cart.html</a>).</li>
</ul>
</blockquote>
<!--
### Properties 

- consistency
- as prob overfitting

-->

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-BeHo:95">
<p>Benjamini, Y., and Y. Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” <em>Journal of the Royal Statistical Society, Ser. B</em> 57: 289–300.</p>
</div>
<div id="ref-Brei:84">
<p>Breiman, L. 1984. <em>Classification and Regression Trees</em>. Chapman &amp; Hall.</p>
</div>
<div id="ref-Brei:96">
<p>Breiman, L. 1996. “Bagging Predictors.” <em>Machine Learning</em> 24: 123–40.</p>
</div>
<div id="ref-BuKaMa:10">
<p>Bühlmann, P., M. Kalisch, and M. H. Maathuis. 2010. “Variable Selection in High-Dimensional Linear Models: Partially Faithful Distributions and the PC-Simple Algorithm.” <em>Biometrika</em> 97: 261–78.</p>
</div>
<div id="ref-EfHaJoTi:04">
<p>Efron, B., T. Hastie, I. Johnstone, and R. Tibshirani. 2004. “Least Angle Regression (with Discussion).” <em>Annals of Statistics</em> 32: 407–99.</p>
</div>
<div id="ref-FaLi:08">
<p>Fan, J., and J. Lv. 2008. “Sure Independence Screening for Ultrahigh Dimensional Feature Space.” <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 70 (5). Wiley Online Library: 849–911.</p>
</div>
<div id="ref-Fish:15">
<p>Fisher, R. A. 1915. “Frequency Distribution of the Values of the Correlation Coeficient in Samples from an Indefinitely Large Population.” <em>Biometrika</em> 10: 507–21.</p>
</div>
<div id="ref-Fish:21">
<p>Fisher, R. 1921. “On the ‘Probable Error’ of a Coefficient of Correlation Deduced from a Small Sample.” <em>Metron</em> 1: 3–32.</p>
</div>
<div id="ref-FoSt:04">
<p>Foster, D., and R Stine. 2004. “Variable Selection in Data Mining: Building a Predictive Model for Bankruptcy.” <em>Journal of the American Statistical Association</em> 99: 303–13.</p>
</div>
<div id="ref-Good:79">
<p>Goodnight, J. H. 1979. “A Tutorial on the SWEEP Operator.” <em>The American Statistician</em> 33: 149–58.</p>
</div>
<div id="ref-GSWaChTi:16">
<p>G’Sell, M. G., S. Wager, A. Chouldechova, and R. Tibshirani. 2016. “Sequential Selection Procedures and False Discovery Rate Control.” <em>Journal of the Royal Statististical Society, B</em> 78: 423–44.</p>
</div>
<div id="ref-HaTaTiWa:07">
<p>Hastie, T., J. Taylor, R. Tibshirani, and G. Walther. 2007. “Forward Stagewise Regression and the Monotone Lasso.” <em>Electronic Journal of Statistics</em> 1: 1–29.</p>
</div>
<div id="ref-HaTiFr:09">
<p>Hastie, Trevor, Tibshirani Robert, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Second Edition. Springer Series in Statistics. Springer.</p>
</div>
<div id="ref-LiFoUn:11">
<p>Lin, D., D. P. Foster, and L. H. Ungar. 2011. “VIF Regression: A Fast Regression Algorithm for Large Data.” <em>Journal of the American Statistical Association</em> 106: 232–47.</p>
</div>
<div id="ref-SpGlSc:00">
<p>Spirtes, P., C. Glymour, and R. Scheines. 2000. <em>Causation, Prediction, and Search</em>. MIT Press.</p>
</div>
<div id="ref-Sutt:05">
<p>Sutton, C. D. 2005. “11 - Classification and Regression Trees, Bagging, and Boosting.” In <em>Data Mining and Data Visualization</em>, edited by C.R. Rao, E.J. Wegman, and J.L. Solka, 24:303–29. Handbook of Statistics. Elsevier.</p>
</div>
<div id="ref-TibR:15">
<p>Tibshirani, R. J. 2015. “A General Framework for Fast Stagewise Algorithms.” <em>Journal of Machine Learning Research</em> 16: 2543–88.</p>
</div>
<div id="ref-zhang2009">
<p>Zhang, T. 2009. “On the Consistency of Feature Selection Using Greedy Least Squares Regression.” <em>Journal of Machine Learning Research</em> 10: 555–68.</p>
</div>
<div id="ref-ZhFoStUn:06">
<p>Zhou, J., D. P. Foster, R. A. Stine, and L. H. Ungar. 2006. “Streamwise Feature Selection.” <em>Journal of Machine Learning Research</em> 7: 1861–85.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="assessing-the-validity-of-a-model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="shrinkage-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
