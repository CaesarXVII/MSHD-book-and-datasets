<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Model Selection in High Dimensions</title>
  <meta name="description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook.">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="Model Selection in High Dimensions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Model Selection in High Dimensions" />
  
  <meta name="twitter:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  

<meta name="author" content="Maria-Pia Victoria-Feser (professor), Cesare Miglioli and Guillaume Blanc (teaching assistants)">


<meta name="date" content="2018-03-09">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  

<link rel="next" href="assessing-the-validity-of-a-model.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#read-this-part-first"><i class="fa fa-check"></i><b>1.1</b> Read this part first</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#content-choice-and-structure"><i class="fa fa-check"></i><b>1.2</b> Content choice and structure</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#bibliography"><i class="fa fa-check"></i><b>1.2.1</b> Bibliography</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#useful-links"><i class="fa fa-check"></i><b>1.2.2</b> Useful links</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#using-r"><i class="fa fa-check"></i><b>1.3</b> Using R</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#useful-r-packages"><i class="fa fa-check"></i><b>1.3.1</b> Useful R packages</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#managing-data"><i class="fa fa-check"></i><b>1.3.2</b> Managing Data</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#loading-data-from-an-r-package"><i class="fa fa-check"></i><b>1.3.3</b> Loading data from an R package</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#loading-data-from-a-local-file"><i class="fa fa-check"></i><b>1.3.4</b> Loading data from a local file</a></li>
<li class="chapter" data-level="1.3.5" data-path="index.html"><a href="index.html#loading-data-from-an-online-file"><i class="fa fa-check"></i><b>1.3.5</b> Loading data from an online file</a></li>
<li class="chapter" data-level="1.3.6" data-path="index.html"><a href="index.html#loading-data-from-an-online-database-using-a-mysql-query-optional"><i class="fa fa-check"></i><b>1.3.6</b> Loading data from an online database using a mySQL query (Optional)</a></li>
<li class="chapter" data-level="1.3.7" data-path="index.html"><a href="index.html#data-wrangling"><i class="fa fa-check"></i><b>1.3.7</b> Data Wrangling</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#writing-reports"><i class="fa fa-check"></i><b>1.4</b> Writing reports</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#r-markdown"><i class="fa fa-check"></i><b>1.4.1</b> R Markdown</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4.2</b> GitHub</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.5</b> Examples</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#data-on-malnutrition-in-zambia"><i class="fa fa-check"></i><b>1.5.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#prognostic-factors-in-childhood-leukemia"><i class="fa fa-check"></i><b>1.5.2</b> Prognostic Factors in Childhood Leukemia</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#r-package-quantmod"><i class="fa fa-check"></i><b>1.5.3</b> R package quantmod</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#fundamental-statistical-concepts"><i class="fa fa-check"></i><b>1.6</b> Fundamental statistical concepts</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#sample-and-population"><i class="fa fa-check"></i><b>1.6.1</b> Sample and population</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#models-and-risk"><i class="fa fa-check"></i><b>1.6.2</b> Models and risk</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#estimators-and-associated-variability"><i class="fa fa-check"></i><b>1.6.3</b> Estimators and associated variability</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#simulating-the-population-using-resampling-techniques"><i class="fa fa-check"></i><b>1.6.4</b> Simulating the population using resampling techniques</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#model-selection"><i class="fa fa-check"></i><b>1.6.5</b> Model Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html"><i class="fa fa-check"></i><b>2</b> Assessing the validity of a model</a><ul>
<li class="chapter" data-level="2.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#cross-validation"><i class="fa fa-check"></i><b>2.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="2.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#covariance-penalties-criteria"><i class="fa fa-check"></i><b>2.3</b> Covariance Penalties Criteria</a><ul>
<li class="chapter" data-level="2.3.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-2"><i class="fa fa-check"></i><b>2.3.1</b> Introduction</a></li>
<li class="chapter" data-level="2.3.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mallows-c_p"><i class="fa fa-check"></i><b>2.3.2</b> Mallows <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="2.3.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#efrons-q-class"><i class="fa fa-check"></i><b>2.3.3</b> Efron’s <span class="math inline">\(q\)</span>-class</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#information-theory-and-bayesian-criteria"><i class="fa fa-check"></i><b>2.4</b> Information Theory and Bayesian Criteria</a><ul>
<li class="chapter" data-level="2.4.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#aic-akaike-information-criterion"><i class="fa fa-check"></i><b>2.4.1</b> AIC: Akaike Information Criterion</a></li>
<li class="chapter" data-level="2.4.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#bic-bayesian-information-criterion"><i class="fa fa-check"></i><b>2.4.2</b> BIC: Bayesian Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mean-squared-error-based-criteria"><i class="fa fa-check"></i><b>2.5</b> Mean Squared Error Based Criteria</a><ul>
<li class="chapter" data-level="2.5.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#steins-unbiased-risk-estimator-sure"><i class="fa fa-check"></i><b>2.5.1</b> Stein’s unbiased risk estimator (SURE)</a></li>
<li class="chapter" data-level="2.5.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-focused-information-criterion-fic"><i class="fa fa-check"></i><b>2.5.2</b> The Focused Information Criterion (FIC)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-measures"><i class="fa fa-check"></i><b>2.6</b> Classification measures</a><ul>
<li class="chapter" data-level="2.6.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-logistic-model"><i class="fa fa-check"></i><b>2.6.1</b> The logistic model</a></li>
<li class="chapter" data-level="2.6.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#prediction-error-measures-for-binary-classification"><i class="fa fa-check"></i><b>2.6.2</b> Prediction error measures for Binary classification</a></li>
<li class="chapter" data-level="2.6.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-error-estimation"><i class="fa fa-check"></i><b>2.6.3</b> Classification error estimation</a></li>
<li class="chapter" data-level="2.6.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-roc-curve"><i class="fa fa-check"></i><b>2.6.4</b> The ROC curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html"><i class="fa fa-check"></i><b>3</b> Ordering the variables</a><ul>
<li class="chapter" data-level="3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#stepwise-forward-regression"><i class="fa fa-check"></i><b>3.2</b> Stepwise forward regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#partial-correlations"><i class="fa fa-check"></i><b>3.2.1</b> Partial correlations</a></li>
<li class="chapter" data-level="3.2.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#selection-by-hypothesis-testing"><i class="fa fa-check"></i><b>3.2.2</b> Selection by hypothesis testing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Model Selection in High Dimensions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Model Selection in High Dimensions</h1>
<h4 class="author"><em>Maria-Pia Victoria-Feser (professor), Cesare Miglioli and Guillaume Blanc (teaching assistants)</em></h4>
<h4 class="date"><em>2018-03-09</em></h4>
</div>
<div id="introduction" class="section level1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<div id="read-this-part-first" class="section level2">
<h2><span class="header-section-number">1.1</span> Read this part first</h2>
<p>Anyone is invited to use any part of this eBook as long as credit is given. To cite this book, please use:</p>
<ul>
<li>Victoria-Feser, M.-P. (2018). <em>A Lecture in Model Selection in High Dimensions</em>, Research Center for Statistics, GSEM, University of Geneva, Switzerland.</li>
</ul>
<p>If you use this eBook as a reference for a course, please inform the author.</p>
<p>(<a href="mailto:maria-pia.victoriafeser@unige.ch">maria-pia.victoriafeser@unige.ch</a>).</p>
<p>The content of this eBook is dynamic and changes as the lectures take place. Students participating to the classes can contribute to the content, with for example the analysis of real data sets, the resolution of exercises, simulations to explore methods in particular settings, etc. Their contribution is acknowledge where it is due.</p>
<p>The first acknowledgements go to Cesare Miglioli and Guillaume Blanc, Ph. D. Students and the Research Center for Statistics, University of Geneva, for their invaluable contribution in setting up the first version of this eBook.</p>
</div>
<div id="content-choice-and-structure" class="section level2">
<h2><span class="header-section-number">1.2</span> Content choice and structure</h2>
<p>The content of this e-book is intended for graduate and doctoral students in statistics and related fields interested in the statistical approach of <em>model selection in high dimensions</em>.</p>
<p>Model selection in high dimensions is an active subject of research, ranging from machine learning and/or artificial intelligence algorithms, to statistical inference, and sometimes a mix of the two. We focus on the frequentist approach to model selection in view of presenting methods that have the necessary properties for out-of-sample (or population) <em>validity</em>, within an as large as possible theoretical framework that enables the measurement of different aspects of the <em>validity</em> concept. We therefore anchor the content into an <em>inferential statistics</em> approach, essentially for causal models.</p>
<p>More specifically, the focus of model selection in high dimensions is presented into two main headings, one on statistical methods or criteria for measuring the statistical <em>validity</em>, and the other one on fast algorithms in high dimensional settings, both in the number of <em>observation</em> and in the number of <em>inputs</em>, that avoid the simultaneous comparison of all <em>possible models</em>.</p>
<p>Even within this focus, the set of available methods is still very rich, so that only a selection of the available methods is presented.</p>
<p>Each presentation is accompanied with practical exercises using <a href="https://www.r-project.org/">R</a>. We highly recommend downloading <a href="https://www.rstudio.com/">RStudio’s IDE</a> which is an ideal working environment for statistical analyses.</p>
<div id="bibliography" class="section level3">
<h3><span class="header-section-number">1.2.1</span> Bibliography</h3>
<p><strong>(to be completed)</strong></p>
<ul>
<li><em>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</em>. Bradley Efron &amp; Trevor Hastie, Cambridge University Press, 2016.</li>
</ul>
<p><a href="https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf" class="uri">https://web.stanford.edu/~hastie/CASI_files/PDF/casi.pdf</a></p>
<ul>
<li><em>An Introduction to Statistical Learning: with Applications in R</em>. Gareth James, Daniela Witten, Trevor Hastie &amp; Robert Tibshirani, Springer, 2013.</li>
</ul>
<p><a href="http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf">http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf</a></p>
<ul>
<li><em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Trevor Hastie, Robert Tibshirani &amp; Jerome Friedman, Springer, 2009.</li>
</ul>
<p><a href="https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf" class="uri">https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf</a></p>
<ul>
<li><p><em>Model selection and model averaging</em>. Gerda Claeskens and Nils Lid Hjort, Cambridge University Press, 2008.</p></li>
<li><p><em>Regression and Time Series Model Selection</em>. Allan D R McQuarrie and Chih-Ling Tsai, World Scientific, 1998.</p></li>
</ul>
</div>
<div id="useful-links" class="section level3">
<h3><span class="header-section-number">1.2.2</span> Useful links</h3>
<p><strong>(to be completed)</strong></p>
<!--- Project: https://www.dropbox.com/sh/ijh1bbxc49jztcl/AABdSzpcG4lGvo1ye8ZWtJjKa?dl=0 (to eliminate before publishing)-->
<ul>
<li><a href="https://chamilo.unige.ch/">Chamilo</a> (Search for <em>Model Selection in High Dimensions</em> to register)</li>
<li><a href="https://github.com/CaesarXVII/Model-Selection-in-High-Dimensions">Github repository of the course</a></li>
<li><a href="https://www.r-project.org/">R project</a></li>
<li><a href="https://www.rstudio.com/">R Studio</a></li>
<li><a href="https://smac-group.github.io/ds/">An Introduction to Statistical Programming Methods with R</a></li>
<li><a href="https://github.com/">GitHub</a></li>
<li><a href="https://archive.ics.uci.edu/ml/index.php">UCI repository for datasets</a></li>
<li><a href="https://en.wikipedia.org/wiki/Database">Database definition</a></li>
<li><a href="https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf">Data Wrangling cheatsheet</a></li>
<li><a href="https://archive-ouverte.unige.ch/unige:29628">Malnutrition in Zambia</a>, p. 64</li>
<li><a href="https://github.com/CaesarXVII/MSHD-book-and-datasets/blob/master/datasets/malnutrion_zambia_cleaned.Rda">Course Datasets - Malnutrition in Zambia</a></li>
<li><a href="https://www.cancer.org/cancer/leukemia-in-children/detection-diagnosis-staging/prognostic-factors.html">American Cancer Association on Leukemia prognostic factors</a></li>
<li><a href="http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv">Data on Leukemia in Children</a></li>
<li><a href="https://www.quantmod.com/">quantmod package in R</a></li>
<li><a href="https://en.wikipedia.org/wiki/Theil_index">Theil inequality index</a></li>
<li><a href="http://www.deeplearningbook.org/">Deep Learning</a></li>
<li><a href="https://tadpole.grand-challenge.org/">Alzeihmer data</a></li>
</ul>
</div>
</div>
<div id="using-r" class="section level2">
<h2><span class="header-section-number">1.3</span> Using R</h2>
<p>There are many available classes, textbooks, e-books, etc. on how to get acquainted with a quite sophisticated usage of the most commonly used statistical software <a href="https://www.r-project.org/">R</a>. The choice of the R editor depends on how R is used and for this class, we propose the open source editor <a href="https://www.rstudio.com/">RStudio</a>. We also highly recommend the introduction proposed in <a href="https://smac-group.github.io/ds/index.html#r-and-rstudio">SMAC</a>, which will constitute the basic knowledge from which this class starts.</p>
<div id="useful-r-packages" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Useful R packages</h3>
<p>The R packages that will be used throughout this class are the following (to be completed):</p>
<!-- Here we should add all the packages needed also for Guillaume exercises, even R Markdown is a package! -->
<ul>
<li>rmarkdown</li>
<li>quantmod</li>
<li>plotly</li>
<li>tidyr</li>
<li>dplyr</li>
<li>RODBC</li>
<li>pool</li>
<li>RMySQL</li>
<li>foreign</li>
<li>leaps</li>
<li>glmulti</li>
<li>MuMIn</li>
<li>caret</li>
<li>mvtnorm</li>
<li>MASS</li>
<li>tictoc</li>
<li>pROC</li>
<li>glmnet</li>
</ul>
<p>To install a package, use the R command <code>install.packages(&quot;chosen.package.name&quot;)</code> (see also <a href="https://cran.r-project.org/web/packages/" class="uri">https://cran.r-project.org/web/packages/</a>). Visit the <a href="https://cran.r-project.org/">CRAN R project</a> to get useful information about all the available R packages (<a href="https://cran.r-project.org/web/packages/available_packages_by_name.html" class="uri">https://cran.r-project.org/web/packages/available_packages_by_name.html</a>)</p>
<p>To install all packages at once, run the following code with administrator rights.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">packages.required &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;rmarkdown&quot;</span>, <span class="st">&quot;quantmod&quot;</span>, <span class="st">&quot;plotly&quot;</span>, <span class="st">&quot;tidyr&quot;</span>, <span class="st">&quot;dplyr&quot;</span>, <span class="st">&quot;RODBC&quot;</span>, <span class="st">&quot;pool&quot;</span>, <span class="st">&quot;RMySQL&quot;</span>, 
                       <span class="st">&quot;foreign&quot;</span>, <span class="st">&quot;leaps&quot;</span>, <span class="st">&quot;glmulti&quot;</span>, <span class="st">&quot;MuMIn&quot;</span>, <span class="st">&quot;caret&quot;</span>,<span class="st">&quot;mvtnorm&quot;</span>, <span class="st">&quot;MASS&quot;</span>, <span class="st">&quot;tictoc&quot;</span>,<span class="st">&quot;pROC&quot;</span>,<span class="st">&quot;glmnet&quot;</span>)
packages.missing  &lt;-<span class="st"> </span>packages.required[!(packages.required %in%<span class="st"> </span><span class="kw">rownames</span>(<span class="kw">installed.packages</span>()))]
if(<span class="kw">length</span>(packages.missing)!=<span class="dv">0</span>) <span class="kw">install.packages</span>(packages.missing)</code></pre></div>
</div>
<div id="managing-data" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Managing Data</h3>
<p>(by G. Blanc)</p>
<p>Data are nowadays continuously produced and readily available from internet platforms. This has become necessary since very often personal computer memory is not sufficient to store (high dimensional) data locally. It is therefore important to be able to import data into R for data analysis in an (almost) automatic fashion.</p>
<p>Colloquially, `loading’ a dataset means storing it into your computer’s Random-Access-Memory (RAM), which allows for a fast access of the CPU to the data. The RAM is extremely fast<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, but is typically of limited amount compared to what can be stored in a hard-drive or a SSD (usually 4 to 16 Gbs in a typical consumer-grade computer). The data stored in your computer’s RAM is called <em>volatile</em>: the information it contains will disappear once the computer is shut down. This is the reason why the datasets must be loaded into R at the beginning of each session.</p>
<p>In this section, you will learn different ways to load the data that you may encounter in the future, depending on the context and the size of the databases: from an R package, from a local data file, from an online data file, and from an online database. In this latter case, datasets are obtained by way of SQL queries via a remote connection.</p>
<p>Follow along and load all the datasets as in the text: they will be used for the following exercises.</p>
</div>
<div id="loading-data-from-an-r-package" class="section level3">
<h3><span class="header-section-number">1.3.3</span> Loading data from an R package</h3>
<p>(by G. Blanc)</p>
<p>Some packages have their own data included, and R-base indeed includes a well known selection, for instance the <em>iris</em> dataset. You can load it using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load the iris dataset</span>
iris &lt;-<span class="st"> </span>iris</code></pre></div>
<p>and display its structure with:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Display iris&#39; structure</span>
<span class="kw">str</span>(iris)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>
<p>The dataset <em>iris</em> is now ready for analysis.</p>
</div>
<div id="loading-data-from-a-local-file" class="section level3">
<h3><span class="header-section-number">1.3.4</span> Loading data from a local file</h3>
<p>(by G. Blanc)</p>
<p>Typical file formats include <em>.txt</em> and <em>.csv</em>, or <em>.data</em>. Download the file <em>wine.data</em> from <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/wine/" class="uri">http://archive.ics.uci.edu/ml/machine-learning-databases/wine/</a> to a folder on your computer, say in <em>./datasets/</em>.</p>
<p>You can load the data using the <code>read.table</code> function, which takes as further input <code>sep = &quot;,&quot;</code> to indicate that the variables in each line are separated by a comma. These variables will be organized as columns in the dataset.Every line of the file becomes a row in the dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wine &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;./datasets/wine.data&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>)</code></pre></div>
</div>
<div id="loading-data-from-an-online-file" class="section level3">
<h3><span class="header-section-number">1.3.5</span> Loading data from an online file</h3>
<p>(by G. Blanc)</p>
<p>The end result will be the same as above, but the process is less tedious, provided you have an internet connection. Simply load the data using the complete url as an input for <code>read.table</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">wine &lt;-<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data&quot;</span>, <span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>)</code></pre></div>
<p>A quick look at the structure of the dataset shows that the columns do not have names that describe their content. Let’s change that:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># format the data.frame:</span>

<span class="kw">colnames</span>(wine) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Class&quot;</span>, <span class="st">&quot;Alcohol&quot;</span>, <span class="st">&quot;Malic_acid&quot;</span>, <span class="st">&quot;Ash&quot;</span>, 
                    <span class="st">&quot;Alcalinity_of_ash&quot;</span>, <span class="st">&quot;Magnesium&quot;</span>, <span class="st">&quot;Total_phenols&quot;</span>, 
                    <span class="st">&quot;Flavanoids&quot;</span>, <span class="st">&quot;Nonflavanoid_phenols&quot;</span>, <span class="st">&quot;Proanthocynins&quot;</span>,
                    <span class="st">&quot;Color_intensity&quot;</span>, <span class="st">&quot;Hue&quot;</span>, <span class="st">&quot;OD280vsOD315&quot;</span>, <span class="st">&quot;Proline&quot;</span>)
wine$Class &lt;-<span class="st"> </span><span class="kw">as.factor</span>(wine$Class)</code></pre></div>
<p>The dataset <em>wine</em> is now ready for analysis.</p>
</div>
<div id="loading-data-from-an-online-database-using-a-mysql-query-optional" class="section level3">
<h3><span class="header-section-number">1.3.6</span> Loading data from an online database using a mySQL query (Optional)</h3>
<p>(by G. Blanc)</p>
<p>The most common way to store massive, related data from different sources is to use relational databases. These consists of multiple datasets (tables), that may be related in some specific way. For instance, an online shop may have a table listing all of its registered clients, and another table listing all the orders made by the clients. In this example, the clients will have a <em>unique identifier</em> that will establish the relation between the two databases.</p>
<p>A <em>database</em> is an organized collection of data. A <em>relational database</em> (see also <a href="https://en.wikipedia.org/wiki/Database" class="uri">https://en.wikipedia.org/wiki/Database</a>), more restrictively, is a collection of schema, tables, queries, reports, views, and other elements. A <em>database-management system</em> (<em>DBMS</em>) is a computer-software application that interacts with end-users (you), other applications that you may develop (e.g., an <em>Rmarkdown</em> document), and the database itself to capture and analyze data.</p>
<p>Relational databases organize data into one or more <em>tables</em> of columns and rows, with a <em>unique key</em> identifying each row. Generally, each table/relation represents one “entity type” (such as customer or product). The rows represent instances of that type of entity (such as “Lee” or “chair”) and the columns represent values attributed to that instance (such as “address” or “price”).</p>
<p>The databases can be stored - offline, on a non-volatile memory (for instance your hard drive or SSD drive), or - online, which require an internet connection to access.</p>
<p>You will usually need credentials (a username and a password) to access either databases.</p>
<!-- #### Why relational databases instead of a "flat file"? -->
<p>As we will see, there are many advantages of relational databases which explain their almost universal use when it comes to storing massive amount of data online. The main advantages are that they:</p>
<ul>
<li>avoid data duplication,</li>
<li>avoid inconsistent records,</li>
<li>allow easily to change or add/remove the data,</li>
<li>are more secure.</li>
</ul>
<!--#### Loading data from a database-->
<p>To access the data store on the database, you will need to establish a <em>connection</em>, for which we will need RStudio’s <code>pool</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pool)

<span class="co"># Establish a connection to the external database</span>
con &lt;-<span class="st"> </span><span class="kw">dbPool</span>(
  <span class="dt">drv =</span> RMySQL::<span class="kw">MySQL</span>(),
  <span class="dt">dbname =</span> <span class="st">&quot;shinydemo&quot;</span>,
  <span class="dt">host =</span> <span class="st">&quot;shiny-demo.csa7qlmguqrf.us-east-1.rds.amazonaws.com&quot;</span>,
  <span class="dt">username =</span> <span class="st">&quot;guest&quot;</span>,
  <span class="dt">password =</span> <span class="st">&quot;guest&quot;</span>
)</code></pre></div>
<p>We can explore the database and list the tables that it contains using <code>dbListTables</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Explore the tables available in the database:</span>
<span class="kw">dbListTables</span>(con)</code></pre></div>
<pre><code>## [1] &quot;City&quot;            &quot;Country&quot;         &quot;CountryLanguage&quot;</code></pre>
<p>Remember that each table is a different dataset. Some databases are very big, in the <em>BigData</em> sense: think millions, even billions of entries. SQL queries allow to cherry pick the data we need, without having to download the whole dataset (which would be in some cases unfeasible). In a typical use, we would then query the data that we need, and no more. Since this is not a <code>mySQL</code> course and our datasets are of reasonable size, we will simply download the three datasets entirely using the <code>SQL</code>query <code>SELECT * FROM (Table)</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Download the three datasets</span>
City &lt;-<span class="st"> </span>DBI::<span class="kw">dbGetQuery</span>(con, <span class="st">&quot;select * from City&quot;</span>)
Country &lt;-<span class="st"> </span>DBI::<span class="kw">dbGetQuery</span>(con, <span class="st">&quot;select * from Country&quot;</span>)
CountryLanguage &lt;-<span class="st"> </span>DBI::<span class="kw">dbGetQuery</span>(con, <span class="st">&quot;select * from CountryLanguage&quot;</span>)
<span class="kw">poolClose</span>(con)</code></pre></div>
<blockquote>
Exercise:<br />
- Load the iris dataset from the R-base package<br />
- Load the wine dataset from a local file and from an online file using the URL directly<br />
- Load the datasets City, Country, and CountryLanguage by connecting to an online database<br />

</blockquote>
</div>
<div id="data-wrangling" class="section level3">
<h3><span class="header-section-number">1.3.7</span> Data Wrangling</h3>
<p>(by G. Blanc)</p>
<p>Data wrangling means to manipulate and prepare a dataset in such a way, that it becomes amenable to analysis. Minimally, a numerical dataset is stored as a Matrix object, a type optimized for computations. Preferably, however, a dataset is stored as a dataframe object. A dataframe is technically a list of columns, each containing data of a given type (e.g., integer, numerical, character, factor).</p>
<p>Many packages, and indeed R-base itself, are optimized to have the data organized in the following way:</p>
<ul>
<li>each <em>row</em> represent one observation</li>
<li>each <em>column</em> represents a variable (or `feature’)</li>
</ul>
<p>There are two packages dedicated to data wrangling in R:</p>
<ul>
<li><code>dplyr</code> is a grammar of data wrangling, which focuses on efficient and elegant coding</li>
<li><code>data.table</code> is computationally extremely fast to manipulate very large datasets</li>
</ul>
<p>Both create objects that are extensions of dataframes: <code>dplyr</code> uses a <code>tibble</code>, and <code>data.table</code> uses a <code>data.table</code>. We do not recommend to use <code>data.table</code> in this course, as it has a steeper learning curve. Using <code>dplyr</code> is not necessary either to complete the course; however, it will save you time in the end and is a worthwhile investment. For a quick reference, download the official <a href="https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf">cheatsheet</a>.</p>
<blockquote>
<p>Exercise:</p>
</blockquote>
<!-- The purpose of the following exercises is to refresh/teach you R skills that will save you time in the future. Your grade will not depend on your ability to use the techniques showcased here. -->
<p>We will assume that the datasets iris, wine, City, Country, and CountryLanguage of the above section have all been loaded in R, as well as the package dplyr.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)</code></pre></div>
<blockquote>
<ol style="list-style-type: decimal">
<li>Comment on the type of variables of the iris dataset. Write a code that computes the mean of each variable, grouped by Species. Do it first using <code>R-base</code> only, and optionally, do it using <code>dplyr</code>.</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">variables.to.average &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)
species.types &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;setosa&quot;</span>, <span class="st">&quot;versicolor&quot;</span>, <span class="st">&quot;virginica&quot;</span>)  <span class="co"># levels(iris$Species)</span>
<span class="co">#</span>
<span class="co"># Method 1A: without dplyr using a for loop; outputs a matrix </span>
species.means1 &lt;-<span class="st"> </span><span class="ot">NULL</span>
for(type in species.types){
  is.species &lt;-<span class="st"> </span>iris$Species==type
  species.subset &lt;-<span class="st"> </span>iris[is.species, variables.to.average]
  species.means1 &lt;-<span class="st"> </span><span class="kw">rbind</span>(species.means1, <span class="kw">colMeans</span>(species.subset))
}
<span class="kw">rownames</span>(species.means1) &lt;-<span class="st"> </span>species.types
<span class="co">#</span>
<span class="co"># Method 1B: more compact and efficient; outputs a matrix</span>
species.means2 &lt;-<span class="st"> </span><span class="kw">t</span>(<span class="kw">sapply</span>(species.types, function(species){
  <span class="kw">colMeans</span>(iris[iris$Species==species, variables.to.average])
}))
<span class="co">#</span>
<span class="co"># Method 2: using dplyr; outputs a dataframe</span>
 species.means3 &lt;-<span class="st"> </span>iris %&gt;%<span class="st"> </span><span class="kw">group_by</span>(Species) %&gt;%<span class="st"> </span><span class="kw">summarise_all</span>(mean)
 species.means3</code></pre></div>
<pre><code>## # A tibble: 3 x 5
##   Species    Sepal.Length Sepal.Width Petal.Length Petal.Width
##   &lt;fct&gt;             &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;
## 1 setosa             5.01        3.43         1.46       0.246
## 2 versicolor         5.94        2.77         4.26       1.33 
## 3 virginica          6.59        2.97         5.55       2.03</code></pre>
<blockquote>
<ol start="2" style="list-style-type: decimal">
<li>Using the method of your choice, plot the boxplots of alcohol levels of the three classes of wine from the wine dataset.</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(Alcohol ~<span class="st"> </span>Class, <span class="dt">data=</span>wine, <span class="dt">xlab=</span><span class="st">&quot;Class of wine&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Alcohol concentration&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<blockquote>
<ol start="3" style="list-style-type: decimal">
<li>Create 6 boxplots displaying the alcohol level of wine by class (3 levels) and by degree of color intensity (2 levels: above or below 5.0). Optional: compute the corresponding means using dplyr.</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Create a new binary variable, called Color_intensity_factor to denote whether the color is mild or intense.</span>
wine$Color_intensity_factor &lt;-<span class="st"> </span><span class="kw">factor</span>(wine$Color_intensity&gt;<span class="dv">5</span>, <span class="dt">levels=</span><span class="kw">c</span>(<span class="ot">FALSE</span>, <span class="ot">TRUE</span>), <span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&quot;mild&quot;</span>, <span class="st">&quot;intense&quot;</span>))
<span class="co"># Create the boxplots using the standard formulae</span>
<span class="kw">boxplot</span>(Alcohol ~<span class="st"> </span>Color_intensity_factor +<span class="st"> </span>Class, <span class="dt">data=</span>wine, <span class="dt">xlab=</span><span class="st">&quot;Class of wine&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Alcohol concentration&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#</span>
<span class="co"># Compute the alcohol means of each subgroup using dplyr</span>
wine %&gt;%<span class="st"> </span><span class="kw">group_by</span>(Class, Color_intensity_factor) %&gt;%<span class="st"> </span><span class="kw">summarise_at</span>(<span class="st">&quot;Alcohol&quot;</span>, mean)</code></pre></div>
<pre><code>## # A tibble: 6 x 3
## # Groups:   Class [?]
##   Class Color_intensity_factor Alcohol
##   &lt;fct&gt; &lt;fct&gt;                    &lt;dbl&gt;
## 1 1     mild                      13.4
## 2 1     intense                   13.9
## 3 2     mild                      12.3
## 4 2     intense                   12.4
## 5 3     mild                      13.0
## 6 3     intense                   13.2</code></pre>
<blockquote>
<ol start="4" style="list-style-type: decimal">
<li>Consider the three datasets City, Country, and CountryLanguage. Merge the three datasets into a single one using the method of your choice, such that no information is lost. Compare the total number of entries in the three relational datasets, to that of the unique dataset. What do you notice? [Hint: notice that “country” is the common relation among the three datasets]</li>
</ol>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># We observe that &quot;country&quot; is the common relation among all datasets. </span>
<span class="co"># Let&#39;s try to merge all datasets into a single one using this key</span>
<span class="co">#</span>
<span class="co"># Merge all cities with country attributes</span>
data.merged &lt;-<span class="st"> </span><span class="kw">left_join</span>(City, Country, <span class="dt">by=</span><span class="kw">c</span>(<span class="st">&quot;CountryCode&quot;</span> =<span class="st"> &quot;Code&quot;</span>)) %&gt;%<span class="st">   </span><span class="kw">left_join</span>(CountryLanguage, <span class="dt">by=</span><span class="st">&quot;CountryCode&quot;</span>)
<span class="co"># we observe that all the data pertaining to the countries appear multiple times. #The total number of data &quot;cells&quot; of the merged data set is 27031*22 = 594682, as compared to the original 24656 data &quot;cells&quot;, that is about 24 times bigger to store the same information.</span></code></pre></div>
</div>
</div>
<div id="writing-reports" class="section level2">
<h2><span class="header-section-number">1.4</span> Writing reports</h2>
<p>In this Section, information is provided about one convenient way to produce reports when working in teams. To be able to participate in the construction of this eBook, only text (including R chunks) in RMarkdown will be accepted.</p>
<div id="r-markdown" class="section level3">
<h3><span class="header-section-number">1.4.1</span> R Markdown</h3>
<p>RMarkdown is a framework that provides a literate programming format for data science. It can be used to save and execute R code within R Studio and also as a simple formatting syntax for authoring HTML, PDF, ODT, RTF, and MS Word documents as well as seamless transitions between available formats. For example this eBook is written using R Markdown. We recommend the introduction proposed in <a href="https://smac-group.github.io/ds/rmarkdown.html" class="uri">https://smac-group.github.io/ds/rmarkdown.html</a> to rapidly get acquainted with the use of R Markdown.</p>
<blockquote>
<p>Exercises with Iris dataset (see Loading data from an R package)</p>
</blockquote>
<blockquote>
<ul>
<li>Create an .rmd file from R Studio classic interface and look at the basic notions explained in the new document</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Create an histogram of the sepal width of Iris Setosa without showing both the code and the graph. Then, in another code chunk, show only the graph (without the code) and change the height or width of the histogram as you prefer</li>
</ul>
</blockquote>
<p><img src="_main_files/figure-html/unnamed-chunk-16-1.png" width="480" /></p>
<blockquote>
<ul>
<li>Write the formula, both inline and with a Latex environment (e.g. equation), of the conditional probability of observing an Iris Virginica given that the sepal width is greater than 3. Display both the code and the conditional probability</li>
</ul>
</blockquote>
<!-- These are the solutions for the formulas. We just need to uncomment them when needed -->
<p>Inline formula with Dollar operator:</p>
<p><span class="math inline">\(P(\; virginica / \; sepal.width &gt; 3) = \frac{P(virginica \; \&amp; \; sepal.width &gt; 3)}{P(sepal.width &gt; 3)} = 0.254\)</span></p>
<p>Latex enviroment <em>equation</em>:</p>
<span class="math display">\[\begin{equation*}
    P(\; virginica / \; sepal.width &gt; 3) = \frac{P(virginica \; \&amp; \; sepal.width &gt; 3)}{P(sepal.width &gt; 3)} = 0.254
\end{equation*}\]</span>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sub_joint =<span class="st"> </span><span class="kw">subset</span>(iris,iris$Species ==<span class="st"> &quot;virginica&quot;</span> &amp;<span class="st"> </span>iris$Sepal.Width &gt;<span class="st"> </span><span class="dv">3</span>)

sub_marg =<span class="st"> </span>iris$Sepal.Width[iris$Sepal.Width &gt;<span class="st"> </span><span class="dv">3</span>]

result =<span class="st"> </span><span class="kw">dim</span>(sub_joint)[<span class="dv">1</span>]/<span class="kw">length</span>(sub_marg)

result</code></pre></div>
<pre><code>## [1] 0.2537313</code></pre>
</div>
<div id="github" class="section level3">
<h3><span class="header-section-number">1.4.2</span> GitHub</h3>
<p>GitHub (<a href="https://github.com/" class="uri">https://github.com/</a>) is a development platform designed to host and review code, manage projects, and build software alongside millions of other developers. An introduction to the use of Github for managing projects (e.g. a data analysis project), we recommend <a href="https://smac-group.github.io/ds/github.html" class="uri">https://smac-group.github.io/ds/github.html</a>.</p>
<p>Students following this course will be stimulated to complete the exercises and practicals and provide their solutions that will be published in the eBook. GitHub provides a platform for team work that is strongly encouraged.</p>
<blockquote>
<p>Exercises</p>
</blockquote>
<blockquote>
<ul>
<li>Create a free GitHub account on <a href="https://github.com/" class="uri">https://github.com/</a></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Read chapter 3 of the GitHub Guide <a href="https://smac-group.github.io/ds/github.html" class="uri">https://smac-group.github.io/ds/github.html</a></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Install a version of Git (from <a href="https://git-scm.com/downloads" class="uri">https://git-scm.com/downloads</a>) which is compatible with the OS of your computer (e.g. Windows/Mac/Linux/Solaris). Once you have downloaded and installed Git, the first thing you should do is to configure it by setting your username and email address (see first point).</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Watch the video in Section 3.3 of the GitHub Guide on the workflow within R Studio</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Create a new R Studio project, following the steps highlighted in the video, and take the URL from the GitHub repository of the course <a href="https://github.com/CaesarXVII/Model-Selection-in-High-Dimensions" class="uri">https://github.com/CaesarXVII/Model-Selection-in-High-Dimensions</a></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Modify the file (add the name of practical1.rmd file) as you like (e.g. try to solve an exercise). Then <em>commit</em> the changes and <em>push</em> it to the remote repository of the course. Do not forget to click on <em>pull</em> every time you access to your R Studio project to retrieve the updated version of all the files of the course repository</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>In order to properly execute your <em>commits</em>, you need to be added as a collaborator of the project. It is sufficient to send an email to <a href="mailto:*cesare.miglioli@etu.unige.ch">*cesare.miglioli@etu.unige.ch</a>* with your GitHub name in it from your unige mail account and you will be set as a collaborator.</li>
</ul>
</blockquote>
</div>
</div>
<div id="examples" class="section level2">
<h2><span class="header-section-number">1.5</span> Examples</h2>
<div id="data-on-malnutrition-in-zambia" class="section level3">
<h3><span class="header-section-number">1.5.1</span> Data on Malnutrition in Zambia</h3>
<p>Childhood malnutrition is considered to be one of the worst health problems in developing countries <span class="citation">(United Nations Children’s Fund <a href="#ref-unicef1998">1998</a>)</span>. Both a manifestation and a cause of poverty, malnutrition is thought to contribute to over a third of death in children under five years old globally <span class="citation">(United Nations Children’s Fund <a href="#ref-unicef2012">2012</a>)</span>. Moreover, it is well established in the medical literature that maternal and child under nutrition have considerable consequences for adult health and human capital (see e.g. <span class="citation">Victora et al. (<a href="#ref-victora2008maternal">2008</a>)</span> and the references therein). Such conditions are, for example, associated with less schooling, reduced economic productivity, and for women lower offspring birth weight. It has also been reported that lower birth weight and under nutrition in childhood have an influence on cancer occurrence and are risk factors for high glucose concentrations, blood pressure, and harmful lipid profiles. See also <a href="https://archive-ouverte.unige.ch/unige:29628" class="uri">https://archive-ouverte.unige.ch/unige:29628</a>, p. 64.</p>
<p>Under nutrition is generally assessed by comparing anthropometric indicators such as height or weight at a certain age to a reference population. A well established measurement for the study of acute malnutrition is given by (see cite {who1995physical} for details):</p>
<span class="math display">\[\begin{equation}
    Y_i = \frac{H_{i,j} - \mu_j}{\sigma_{j}}
    \label{eq:Zscore}
\end{equation}\]</span>
<p>where <span class="math inline">\(H_{i,j}\)</span>, <span class="math inline">\(\mu_j\)</span> and <span class="math inline">\(\sigma_j\)</span> denote, respectively, the height of the <span class="math inline">\(i^{\text{th}}\)</span> child at age <span class="math inline">\(j\)</span>, the median height of a child of the same age in the reference population and the associated standard deviation. Several factors are assumed to have a determinant influence on under nutrition.</p>
<p>Consider the dataset <em>Zambia.SAV</em> available at <a href="https://github.com/CaesarXVII/MSHD-book-and-datasets/blob/master/datasets/Zambia.SAV">Course Datasets - Malnutrition in Zambia</a> containing variables assumed to be potential causes for childhood malnutrition, i.e.</p>
<ul>
<li>breastfeeding duration (month);</li>
<li>age of the child (month);</li>
<li>age of the mother (years);</li>
<li>Body Mass Index (BMI) of the mother (kg/meter<span class="math inline">\(^2\)</span>);</li>
<li>height of the mother (meter);</li>
<li>weight of the mother (kg);</li>
<li>region of residence (9 levels: Central, Copperbelt, Eastern, Luapula, Lusaka, Northern, Northwestern, Southern and Western);</li>
<li>mother’s highest education level attended (4 levels: No education, Primary, Secondary and Higher);</li>
<li>wealth index factor score;</li>
<li>weight of child at birth (kg) ;</li>
<li>sex of the child;</li>
<li>interval between the current birth and the previous birth (month); and</li>
<li>main source of drinking water (8 levels: Piped into dwelling, Piped to yard/plot, Public tap/standpipe, Protected well, Unprotected well, River/dam/lake/ponds/stream/canal/ irrigation channel, Bottled water, Other).</li>
</ul>
<blockquote>
<p>Exercise:</p>
</blockquote>
<blockquote>
<ul>
<li>Load the dataset and build the variables so that they can be used for a regression analysis.</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(foreign)  <span class="co"># install foreign package if you do not have it yet</span>

<span class="co"># See section 1.6.2 e-book for information on the dataset.</span>

<span class="co"># dat = read.spss(&quot;Zambia.SAV&quot;, add.undeclared.levels = &quot;no&quot;)</span>

dat =<span class="st"> </span><span class="kw">read.spss</span>(<span class="st">&quot;Zambia.SAV&quot;</span>)

<span class="co"># Construct system matrix</span>

<span class="co"># The idea behind this exercise is to be aware that data cleaning is most of the times the real issue </span>
<span class="co"># with a real problem. It is sensitive to say that 80% of the work is cleaning and only 20% is modeling.</span>

<span class="co"># Extract response variable i.e. HW70 Height for age standard deviation (according to WHO)</span>
y =<span class="st"> </span>dat$HW70
y[y ==<span class="st"> </span><span class="dv">9996</span>] =<span class="st"> </span><span class="ot">NA</span>
y[y ==<span class="st"> </span><span class="dv">9997</span>] =<span class="st"> </span><span class="ot">NA</span>
y[y ==<span class="st"> </span><span class="dv">9998</span>] =<span class="st"> </span><span class="ot">NA</span>
y[y ==<span class="st"> </span><span class="dv">9999</span>] =<span class="st"> </span><span class="ot">NA</span>

<span class="co"># Revert tranformation (i.e. z-score)</span>
y =<span class="st"> </span>y/<span class="dv">100</span>

<span class="co"># Variable 1: The calculated months of breastfeeding gives the duration of breastfeeding</span>
x1 =<span class="st"> </span>dat$M5
x1[x1 ==<span class="st"> </span><span class="dv">94</span>] =<span class="st"> </span><span class="dv">0</span>
x1[x1 ==<span class="st"> </span><span class="dv">97</span>] =<span class="st"> </span><span class="ot">NA</span>
x1[x1 ==<span class="st"> </span><span class="dv">98</span>] =<span class="st"> </span><span class="ot">NA</span>
x1[x1 ==<span class="st"> </span><span class="dv">99</span>] =<span class="st"> </span><span class="ot">NA</span>
x1[x1 &gt;<span class="st"> </span><span class="dv">40</span>] =<span class="st"> </span><span class="ot">NA</span>

<span class="co"># Variable 2: Age in months of the child</span>
x2 =<span class="st"> </span>dat$HW1

<span class="co"># Variable 3: Age of the mother at birth</span>
x3 =<span class="st"> </span>dat$V012 -<span class="st"> </span>dat$B8
x3[x3&gt;<span class="dv">45</span>] =<span class="st"> </span><span class="ot">NA</span>

<span class="co"># Variable 4: Body mass index (BMI) of the mother</span>
x4 =<span class="st"> </span>dat$V445

x4 =<span class="st"> </span>x4/<span class="dv">100</span>  <span class="co"># no sense without this division</span>

<span class="co"># Variable 5: Height of the mother in meters</span>
x5 =<span class="st"> </span>dat$V438
x5[x5 ==<span class="st"> </span><span class="dv">9998</span>] =<span class="st"> </span><span class="ot">NA</span>
x5[x5 ==<span class="st"> </span><span class="dv">9999</span>] =<span class="st"> </span><span class="ot">NA</span>
x5[x5 &lt;<span class="st"> </span><span class="dv">1300</span>] =<span class="st"> </span><span class="ot">NA</span>
x5[x5 &gt;<span class="st"> </span><span class="dv">1900</span>] =<span class="st"> </span><span class="ot">NA</span>

x5 =<span class="st"> </span>x5/<span class="dv">1000</span>  <span class="co"># it was in mm, we need to transform from original</span>

<span class="co"># Variable 6: Weight of the mother in kilograms</span>
x6 =<span class="st"> </span>dat$V437

x6=x6/<span class="dv">10</span> <span class="co"># we need to go back to Kg</span>

<span class="co"># Variable 7: De facto region of residence</span>

<span class="co"># Creating dummies (i.e. indicator functions) for each level of an existing factor enables</span>
<span class="co"># to check the coefficients of each level in a possible future model estimation</span>

x7 =<span class="st"> </span><span class="kw">as.factor</span>(dat$V101)


x7 =<span class="st"> </span><span class="kw">model.matrix</span>(~x7<span class="dv">-1</span>)

<span class="kw">dim</span>(x7)

<span class="co"># Variable 8: Mother highest education level attended</span>
x8 =<span class="st"> </span><span class="kw">as.factor</span>(dat$V106)
x8 =<span class="st"> </span><span class="kw">model.matrix</span>(~x8<span class="dv">-1</span>)

<span class="kw">dim</span>(x8)

<span class="co"># Variable 9: Wealth index factor score</span>
x9 =<span class="st"> </span>dat$V191

<span class="co"># Variable 10: Weight of child at birth given in kilograms with three implied decimal places</span>
x10 =<span class="st"> </span>dat$M19
x10[x10 ==<span class="st"> </span><span class="dv">9996</span>] =<span class="st"> </span><span class="ot">NA</span>
x10[x10 ==<span class="st"> </span><span class="dv">9997</span>] =<span class="st"> </span><span class="ot">NA</span>
x10[x10 ==<span class="st"> </span><span class="dv">9998</span>] =<span class="st"> </span><span class="ot">NA</span>
x10[x10 ==<span class="st"> </span><span class="dv">9999</span>] =<span class="st"> </span><span class="ot">NA</span>
x10 =<span class="st"> </span>x10/<span class="dv">1000</span>

<span class="co"># Variable 11: Child Sex</span>
x11 =<span class="st"> </span>dat$B4

<span class="co"># Variable 12: Preceding birth interval is calculated as the difference in months between the current birth and the previous birth</span>
x12 =<span class="st"> </span>dat$B11
x12[x12 &gt;<span class="st"> </span><span class="dv">125</span>] =<span class="st"> </span><span class="ot">NA</span>

<span class="co"># Variable 13: Drinking Water</span>
x13 =<span class="st"> </span>dat$V113
x13 =<span class="st"> </span><span class="kw">model.matrix</span>(~x13<span class="dv">-1</span>)
x13 =<span class="st"> </span>x13[,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">9</span>,<span class="dv">13</span>,<span class="dv">17</span>,<span class="dv">18</span>)]

<span class="kw">dim</span>(x13)

<span class="kw">levels</span>(x13)

mat.sys =<span class="st"> </span><span class="kw">na.omit</span>(<span class="kw">cbind</span>(y,x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13))
<span class="kw">dim</span>(mat.sys)[<span class="dv">2</span>]


<span class="co"># Number of regressor</span>
p =<span class="st"> </span><span class="kw">dim</span>(mat.sys)[<span class="dv">2</span>]

<span class="co"># Construct X and Y</span>
y =<span class="st"> </span>mat.sys[,<span class="dv">1</span>]
X =<span class="st"> </span>mat.sys[,<span class="dv">2</span>:p]

<span class="co"># Create a dataframe</span>

data_zambia =<span class="st"> </span><span class="kw">cbind</span>(y,X)

data_zambia =<span class="st"> </span><span class="kw">data.frame</span>(data_zambia)</code></pre></div>
<blockquote>
<ul>
<li>Associate proper names to each variable (hint: look at the previous comments in the r chunk).</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colnames</span>(data_zambia) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Height for age sd&quot;</span>, <span class="st">&quot;Breastfeeding duration (months)&quot;</span>,<span class="st">&quot;Age of the child (months)&quot;</span>, <span class="st">&quot;Age of the mother (years)&quot;</span>, <span class="st">&quot;BMI mother&quot;</span>, <span class="st">&quot;Heigth mother (meter)&quot;</span>, <span class="st">&quot;Weight mother (kg)&quot;</span>, <span class="st">&quot;Region:Central&quot;</span>, <span class="st">&quot;Region:Copperbelt&quot;</span>, <span class="st">&quot;Region:Eastern&quot;</span>, <span class="st">&quot;Region:Luapula&quot;</span>, <span class="st">&quot;Region:Lusaka&quot;</span>, <span class="st">&quot;Region:Northern&quot;</span>, <span class="st">&quot;Region:Northwestern&quot;</span>, <span class="st">&quot;Region:Southern&quot;</span>, <span class="st">&quot;Region:Western&quot;</span>, <span class="st">&quot;Ed:No education&quot;</span>, <span class="st">&quot;Ed:Primary&quot;</span>, <span class="st">&quot;Ed:Secondary&quot;</span>, <span class="st">&quot;Ed:Higher&quot;</span>, <span class="st">&quot;Wealth index factor score&quot;</span>, <span class="st">&quot;Child weight at birth (kg)&quot;</span>, <span class="st">&quot;Child sex&quot;</span>, <span class="st">&quot;Interval between births&quot;</span>,<span class="st">&quot;Water:Piped into dwelling&quot;</span>, <span class="st">&quot;Water:Piped to yard/plot&quot;</span>, <span class="st">&quot;Water:Public tap/standpipe&quot;</span>, <span class="st">&quot;Water:Protected well&quot;</span>, <span class="st">&quot;Water:Unprotected well&quot;</span>, <span class="st">&quot;Water:River/dam/lake/ponds/stream/canal/irrigation channel&quot;</span>, <span class="st">&quot;Water:Bottled water&quot;</span>, <span class="st">&quot;Water:Other&quot;</span>)</code></pre></div>
<blockquote>
<ul>
<li>Perform a linear regression on all the available variables.</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">attach</span>(data_zambia)

lm_zambia =<span class="st"> </span><span class="kw">lm</span>(<span class="st">`</span><span class="dt">Height for age sd</span><span class="st">`</span> ~<span class="st"> </span>. -<span class="st">`</span><span class="dt">Region:Central</span><span class="st">`</span>-<span class="st"> `</span><span class="dt">Ed:No education</span><span class="st">`</span>, <span class="dt">data =</span> data_zambia)

<span class="co"># We take off two levels to avoid multicollinearity. This should always be done when you create dummies.</span>
<span class="kw">summary</span>(lm_zambia) <span class="co"># read the output understand the benchmark of the factor</span>

lm_zambia_full =<span class="st"> </span><span class="kw">lm</span>(<span class="st">`</span><span class="dt">Height for age sd</span><span class="st">`</span> ~<span class="st"> </span>. , <span class="dt">data =</span> data_zambia)

<span class="kw">summary</span>(lm_zambia_full) <span class="co">#here it is R who choses the benchmark for the factors (i.e. NA variables)</span>

<span class="kw">detach</span>(data_zambia)</code></pre></div>
<blockquote>
<ul>
<li>Reduce the number of covariates (e.g. using the <em>t</em>-test) and add some interactions. Perform a linear regression on the new dataset.</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">attach</span>(data_zambia)

<span class="co"># Eliminate variables with t-test in a stepwise manner (fixed alfa = 0.05 in this case)</span>

model_zambia_reduced =<span class="st"> </span><span class="kw">lm</span>(<span class="st">`</span><span class="dt">Height for age sd</span><span class="st">`</span> ~<span class="st"> </span>., <span class="dt">data =</span> data_zambia[,<span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">9</span>:<span class="dv">16</span>,<span class="dv">21</span>:<span class="dv">23</span>)])


<span class="kw">summary</span>(model_zambia_reduced) <span class="co"># notice what is happening to the age of the mother variable </span>


<span class="co"># Introduce one interaction in the reduced model. We start with the childsex factor.</span>

model_zambia_int =<span class="st"> </span><span class="kw">lm</span>(<span class="st">`</span><span class="dt">Height for age sd</span><span class="st">`</span> ~<span class="st"> </span>. +<span class="st"> `</span><span class="dt">Breastfeeding duration (months)</span><span class="st">`</span>*<span class="st">`</span><span class="dt">Child sex</span><span class="st">`</span>, <span class="dt">data =</span> data_zambia[,<span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">9</span>:<span class="dv">16</span>,<span class="dv">21</span>:<span class="dv">23</span>)])


<span class="kw">summary</span>(model_zambia_int) <span class="co">#We take out the interaction from the model as it is not significant</span>

#### Remember: the hierarchical effect states that anytime you add an interaction also the marginal effects

#### should be part of your model

<span class="kw">detach</span>(data_zambia)</code></pre></div>
<blockquote>
<ul>
<li>Other available procedures for a first model selection in this specific case:</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># (1) VIF (variance inflation factor) for avoiding multicollinearity, </span>

<span class="co"># (2) Automatic Stepwise procedures (e.g. forward and backward) </span>

<span class="co"># (3) Exhaustive search (See practical 3 exercises)</span>

<span class="co"># Example with an automatic stepwise procedure</span>

<span class="kw">help</span>(<span class="st">&quot;step&quot;</span>)

stepwise_procedue =<span class="st"> </span><span class="kw">step</span>(lm_zambia_full,<span class="dt">direction =</span> <span class="st">&quot;backward&quot;</span>) <span class="co">#or forward</span>

<span class="co"># This procedure evaluates, given a criterion, a sequence of variables stopping when</span>
<span class="co"># the criterion is increasing</span></code></pre></div>
<blockquote>
<ul>
<li>Analyse your chosen estimated model with a residual analysis (e.g. residuals vs fitted plot, normal QQ plot etc.).</li>
</ul>
</blockquote>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Validate your model looking at residuals vs fitted plot and normal QQ plot</span>

<span class="kw">plot</span>(model_zambia_reduced, <span class="dt">which =</span> <span class="dv">1</span>)  <span class="co"># Residuals vs fitted: no particular structure</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(model_zambia_reduced, <span class="dt">which =</span> <span class="dv">2</span>) </code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-23-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Normal QQ plot: We observe right tail which is not compatible with a normal assumption</span></code></pre></div>
</div>
<div id="prognostic-factors-in-childhood-leukemia" class="section level3">
<h3><span class="header-section-number">1.5.2</span> Prognostic Factors in Childhood Leukemia</h3>
<p>(by C. Miglioli)</p>
<p>Factors that can affect a child’s outlook (prognosis) suffering e.g. from Leukemia are called prognostic factors. They help doctors decide whether a child with leukemia should receive standard treatment or more intensive treatment. Prognostic factors seem to be more determinant in acute lymphocytic leukemia (ALL) than in acute myelogenous leukemia (AML). See <a href="https://www.cancer.org/cancer/leukemia-in-children/detection-diagnosis-staging/prognostic-factors.html" class="uri">https://www.cancer.org/cancer/leukemia-in-children/detection-diagnosis-staging/prognostic-factors.html</a> for a detailed explanation.</p>
<p>The <em>leukemia_big.csv</em> dataset contains gene expression measurements on 72 leukemia patients: 47 ALL (i.e. acute lymphocytic leukemia) and 25 AML (i.e. acute myelogenous leukemia). These data arise from the landmark of <span class="citation">Golub et al. (<a href="#ref-golub1999molecular">1999</a>)</span> Science paper and exhibit an important statistical challenge because <span class="math inline">\(p &gt;&gt; n\)</span> as we deal with 72 patients and 7128 measurements.</p>
<blockquote>
<p>Exercises</p>
</blockquote>
<ul>
<li>Load the data from the URL <a href="http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv" class="uri">http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv</a></li>
</ul>
<div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">leukemia_big &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;http://web.stanford.edu/~hastie/CASI_files/DATA/leukemia_big.csv&quot;</span>)</code></pre></div>
</div>
<ul>
<li>Create the response variable y according to the number of ALL and AML patients. In the same fashion create the matrix X of independent variables.</li>
</ul>
<p>See <a href="https://web.stanford.edu/~hastie/CASI_files/DATA/leukemia.html" class="uri">https://web.stanford.edu/~hastie/CASI_files/DATA/leukemia.html</a> for further details.</p>
<div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">leukemia_mat =<span class="st"> </span><span class="kw">as.matrix</span>(leukemia_big)

<span class="kw">dim</span>(leukemia_mat)

leukemia_mat =<span class="st"> </span><span class="kw">t</span>(leukemia_mat) <span class="co">#this is the design matrix for the analysis</span>

<span class="co"># Generate the 0 and 1 values for the two different categories: there are 20 ALL, 14 AML, 27 ALL and</span>

<span class="co"># 11 AML for a total of 47 ALL and 25 AML.</span>

<span class="co"># Given the above excerpt from the cancer society, I have decided to code ALL as 1 and AML as 0 since</span>

<span class="co"># doctors are interested in knowing the characteristics which differentiate ALL from AML in order to</span>

<span class="co"># understand if we can use standard treatment or a more aggressive one.</span>

y =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">20</span>),<span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">14</span>), <span class="kw">rep</span>(<span class="dv">1</span>,<span class="dv">27</span>), <span class="kw">rep</span>(<span class="dv">0</span>,<span class="dv">11</span>))  <span class="co">#the response vector</span>

<span class="kw">length</span>(y)

X =<span class="st"> </span>leukemia_mat

<span class="kw">dim</span>(X)</code></pre></div>
</div>
<ul>
<li>Choose the correct exponential family for this situation and perform a GLM on the data. Comment on the results that you obtain.</li>
</ul>
<!-- Since p>>n there are problems -->
<div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">model_glm =<span class="st"> </span><span class="kw">glm</span>(<span class="dt">formula =</span> y ~<span class="st"> </span>X,<span class="dt">family =</span> <span class="st">&quot;binomial&quot;</span>)


<span class="kw">summary</span>(model_glm)  <span class="co">#singularity issues in the IWLS algorithm of GLM. It is impossible to invert the matrix.</span>

<span class="co"># The binary Lasso is a possible way to solve the issue and have an actual estimate. See glmnet package.</span></code></pre></div>
</div>
</div>
<div id="r-package-quantmod" class="section level3">
<h3><span class="header-section-number">1.5.3</span> R package quantmod</h3>
<p>The quantmod package for R (<a href="https://www.quantmod.com/" class="uri">https://www.quantmod.com/</a>) <em>is designed to assist the quantitative trader in the development, testing, and deployment of statistically based trading models</em>. We are here interested in the easy and rapid access to data.</p>
<p><em>It is possible with one quantmod function to load data from a variety of sources, including Yahoo! Finance (OHLC data), Federal Reserve Bank of St. Louis FRED® (11,000 economic series), Google Finance (OHLC data), Oanda, The Currency Site (FX and Metals)</em>, etc. Below are some examples on how to load financial data and perform some simple data analysis. A <em>getting started guide to quantmod</em> can be found at <a href="https://www.quantmod.com/examples/intro/" class="uri">https://www.quantmod.com/examples/intro/</a>.</p>
<p>The first step is to install the <em>quantmod</em> package (only once), using the <em>install.packages</em> in R. Also install the <em>plotly</em> package for nice plots. Then try the following:</p>
<!-- Below you will find the code for avoiding the pop up of comments in the output: basically you need to set results=hide,comment=FALSE and warning=FALSE (this last part only for packages loadings that are full of warnings). 
Of course then you need to split up the chunks in three different parts (i.e. you want the graphs to be shown and not to be hidden) that also look nicer graphically. -->
<!-- Put as a comment in order to avoid lengthy compilation... -->
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Load quantmod</span>
<span class="co"># library(quantmod)</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Download data</span>
<span class="co"># today &lt;- Sys.Date()</span>
<span class="co"># three_month_ago &lt;- seq(today, length = 2, by = &quot;-3 month&quot;)[2]</span>
<span class="co"># getSymbols(&quot;AAPL&quot;, from = three_month_ago, to = today)</span>
<span class="co"># getSymbols(&quot;NFLX&quot;, from = three_month_ago, to = today)</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Produce a </span>
<span class="co"># candleChart(NFLX, theme=&#39;white&#39;)</span>
<span class="co"># candleChart(AAPL, theme=&#39;white&#39;)</span></code></pre></div>
</div>
</div>
<div id="fundamental-statistical-concepts" class="section level2">
<h2><span class="header-section-number">1.6</span> Fundamental statistical concepts</h2>
<div id="sample-and-population" class="section level3">
<h3><span class="header-section-number">1.6.1</span> Sample and population</h3>
<p>If data are collected, stored, analysed, it is because the are supposed to provide information that cannot be otherwise available. Most of the information that is sought concerns sufficiently general phenomena that, in fact, nobody know (or will ever know) exactly. As an illustration, take the example of a teacher that computes the average score of the last math test in his class, what he gets is the <em>exact</em> information about the average score for that particular class and particular test, at the particular moment when the test took place <em>and</em> when the teacher marked the copies. Any other <em>inference</em> from the available information (the <em>sample</em>) to another context is subject to <em>sampling variability</em> and hence is <em>not exact</em>. If the teacher uses the average score to somehow evaluate the difficulty of his math test, then what he has observed within his class is only a part of the truth. For that purpose (evaluating the difficulty of the math test), he should let all the possible students (the <em>population</em>) pass the test and compute the average of the resulting scores of all of them. This is of course not possible, but statistical methodology can help in <em>targetting</em> the question of interest summarized here by the scores average, by providing, for example, a finite set of possible values for the <em>true average</em>, the one computed <em>virtually</em> on the population, also called a <em>parameter</em>.</p>
</div>
<div id="models-and-risk" class="section level3">
<h3><span class="header-section-number">1.6.2</span> Models and risk</h3>
<p>When the sample <em>per se</em> is not the target (i.e. in most of the cases), then one enters into the process of <em>inference</em>: what can we say about what happens in the <em>population</em>, given a sample of <em>data</em>, supposedly carrying enough information for that purpose? A fundamental aspect of statistical inference is the ability of constructing (manageable) measures of variability to <em>any</em> data treatment operated in order to produce <em>information</em> that is used, in its context, to e.g.:</p>
<ul>
<li>understand the phenomenon under investigation,</li>
<li>to predict,</li>
<li>to evaluate research hypotheses,</li>
<li>etc.</li>
</ul>
<p>The inference concept implies two subsequent questions that are at the core of statistics. On the one hand, one has to define what is the <em>population information</em> of interest, and, on the second hand, one has to provide an <em>inferential risk</em>, i.e. a measure of risk associated to any inference made from the sample to the population. The first concept can be associated, very broadly, to the <em>model</em>, i.e. a set of input (a priori) information that serves to formalize the information of interest. The second concept which is a direct consequence of a function of the sampling variability, can be associated to a (set of) <em>propability</em>, a fundamental measure in statistics. Sometimes, and even more and more often, the two concepts are untangled, in the sense that the <em>model</em> can be very flexible (it is actually a set of models) and a risk measure is used to somehow define a (or a drastically reduced set of) model. This <em>vas-et-vient</em> process could be used to define <em>model selection</em> in statistics.</p>
<p>Finally, while the model (or the set of models) is, in general, set a priori, the inferential risk needs to be <em>estimated</em> from the available information, i.e. the sample itself. For that purpose, the fundamental instrument is probability theory.</p>
</div>
<div id="estimators-and-associated-variability" class="section level3">
<h3><span class="header-section-number">1.6.3</span> Estimators and associated variability</h3>
<p>Consider the simplest decisional setting, i.e. confidence intervals for population parameters. We adopt here a frequentist approach. Population parameters can be quantities of interest, e.g. the population mean, the population proportion (for something specific), the population probability (e.g. of being bankrupt or of surviving a given treatment), or more elaborate quantities such as inequality or poverty measures (see e.g. <span class="citation">Cowell (<a href="#ref-Cowe:11">2011</a>)</span>).</p>
Very generally, consider an estimator <span class="math inline">\(\hat{\theta}\)</span> from a population parameter <span class="math inline">\(\theta\in\Theta\subseteq\mathbb{R}^p\)</span> that is computed on a sample <span class="math inline">\(F^{(n)}\)</span><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> generated for a (family of) model <span class="math inline">\(F\)</span>. The latter can be parametric, non parametric or semi-parametric. We can write <span class="math inline">\(\hat{\theta}(F)\)</span>, i.e. the estimator as a functional (or function of a distribution) of <span class="math inline">\(F\)</span>; in particular, we can write <span class="math inline">\(\hat{\theta}(F^{(n)})\)</span>. For example, the sample mean, an estimator of the population mean <span class="math inline">\(\mu\)</span>, can be written as:
<span class="math display">\[\begin{equation}
\hat{\mu}\left(F^{(n)}\right)=\int x dF^{(n)}(x)=\frac{1}{n}\sum_{i=1}^n x_i
\end{equation}\]</span>
<p>An estimator is first chosen for the population parameter and then a confidence interval is built (estimated from the data) that depends on some underlying assumptions about the data generating process. This requires calculating the properties of estimators <span class="math inline">\(\hat{\theta}(F^{(n)})\)</span> at distribution <span class="math inline">\(F\)</span> (which is unknown, only assumed). To do so, there are several strategies which include (see <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span>, chapter 2):</p>
<ul>
<li><p><em>The plug-in principle</em>: The variance (or any other moment of the distribution) of <span class="math inline">\(\hat{\theta}(F^{(n)})\)</span> is expressed (theoretically) as a function of population parameters (e.g. the population mean, variance, higher moments) and the population parameters in the formula are replaced by estimators computed from the sample. General results on classes of estimators such as the maximum likelihood estimator (MLE) or <span class="math inline">\(M\)</span>-estimators <span class="citation">(Huber and Ronchetti <a href="#ref-HuRo:09">2009</a>)</span> can be used for the <em>plug-in principle</em>.</p></li>
<li><p><em>Taylor-series approximations</em>: Let <span class="math inline">\(T(\hat{\theta})\)</span> be a function of interest of <span class="math inline">\(\hat{\theta}(F^{(n)})\)</span>, one can use local linear approximations, method that is also sometimes known as the <em>delta method</em>. One considers the linear expansion of <span class="math inline">\(T(\hat{\theta})\)</span> around <span class="math inline">\(T(\theta)\)</span> and uses the approximation <span class="math inline">\(T(\hat{\theta})=T(\theta)+\partial/\partial\theta^T T(\theta)\left(\hat{\theta}-\theta\right)\)</span> together with the plug-in principle to get an estimator of <span class="math inline">\(T(\hat{\theta})\)</span>. For example, the variance of <span class="math inline">\(T(\hat{\theta})\)</span> can be estimated by <span class="math inline">\(\left(\partial/\partial\theta^T T(\theta)\right)\text{var}(\hat{\theta})\left(\partial/\partial\theta^T T(\theta)\right)^T\)</span> in which the unknown <span class="math inline">\(\theta\)</span> is replaced by its estimated value. Using the functional notation, approximations can be found using von Mises expansions, together with Gâteaux differentials for multidimensional functionals (see e.g. <span class="citation">Fernholz (<a href="#ref-Fern:01">2001</a>)</span>).</p></li>
<li><p><em>Simulation and the bootstrap</em>: The basic idea is to implement the <em>infinite sequence of future trials</em> using simulated samples in <em>almost</em> infinite quantities. An estimator <span class="math inline">\(\hat{F}\)</span> of <span class="math inline">\(F\)</span> is first chosen, then samples <span class="math inline">\(F_k^{(n)},k=1,\ldots,B\)</span> (<span class="math inline">\(B\)</span> is the <em>almost</em> infinite quantity) are simulated from <span class="math inline">\(\hat{F}\)</span> to compute estimates <span class="math inline">\(\hat{\theta}^{(k)}:=\hat{\theta}(F_k^{(n)})\)</span>. This produces an estimate for the distribution of <span class="math inline">\(\hat{\theta}\)</span> that can be used to compute the required quantities such mean, variance, quantiles, etc. Simulation based inference is quite different to traditional methods based on the plug-in principle in that an estimator is sought for <span class="math inline">\(F\)</span> instead of estimators for population parameters.</p></li>
</ul>
<p>The natural question to ask at this point is <strong>what is the <em>best</em> approach to measure <em>risk</em> (sampling error) associated to a <em>statistic</em> (a functional of the sample distribution)?</strong> In a frequentist paradigm, one can rely on concepts such as the minimum variance or mean squared error of the resulting estimator (an asymptotic concept also called efficiency) and/or the rate of convergence of the resulting estimator (related to the asymptotic concept of consistency), i.e. how does the estimator converge to the corresponding population quantity as a function of the sample size <span class="math inline">\(n\)</span>. There is no unifying theory providing an <em>optimality</em> result for all settings, rather general results for classes of models, estimators and/or simulation-based methods.</p>
</div>
<div id="simulating-the-population-using-resampling-techniques" class="section level3">
<h3><span class="header-section-number">1.6.4</span> Simulating the population using resampling techniques</h3>
<p>(See <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span>, chapter 10)</p>
<p>Generally speaking, resampling techniques allow to <em>simulate</em> the population, or more precisely, the sampling mechanism of an <em>infinte</em> number of trials (samples). The <em>unspoiled</em> and unique proxy for the population (model) is the sample <span class="math inline">\(F^{(n)}\)</span>. For finite populations of size <span class="math inline">\(N\)</span> (that can be huge), a sample of independently drawn observations (i.e. the <em>iid</em> case) can be seen as one realization of <span class="math inline">\(n\)</span> draws from a multinomial distribution with <span class="math inline">\(N\)</span> equally probable (<span class="math inline">\(1/N\)</span>) outcomes corresponding to the <span class="math inline">\(N\)</span> population values. This suggests that a suitable proxy to this data generating mechanism (in the <em>iid</em> case) is to proceed with <span class="math inline">\(n\)</span> draws from a multinomial distribution with <span class="math inline">\(n\)</span> equally probable (<span class="math inline">\(1/n\)</span>) outcomes corresponding to the <span class="math inline">\(n\)</span> sample values. This is what the non parametric Bootstrap <span class="citation">(Efron <a href="#ref-Efro:79">1979</a>)</span> does.</p>
<!-- Cesare's comment: I propose to use this format: gray for exercises which are not in the practicals and

that students may do as a project or to get bonus. On the other hand in black all exercises with already 

solution given by us -->
<blockquote>
<p>Exercise (optional):<br />
The aim here is to reproduce the non parametric Bootstrap and compare it with Monte Carlo simulations.<br />
Use as an example the Theil Inequality Index (see e.g. <a href="https://en.wikipedia.org/wiki/Theil_index" class="uri">https://en.wikipedia.org/wiki/Theil_index</a>) together with the Generalized Beta Distribution of the second kind.<br />
For the sampling part, control the seed (see <code>help(set.seed)</code> in R).<br />
Use both the <code>sample</code> command in R and the Uniform distribution to produce the random draws.<br />
- Produce (with both methods) the Bootstrap estimate of the sampling distribution of the Theil Inequality Index (of size 1000) for one chosen set of values for the parameters of the Generalized Beta Distribution of the second kind. Check that both methods produce exactly the same distribution’s estimate.<br />
- Do the same using a Monte Carlo experiment and compare the outcome with the Bootstrap estimate (control the seed).</p>
</blockquote>
The jackknife (<span class="citation">Quenouille (<a href="#ref-Quen:56">1956</a>)</span>, <span class="citation">Tukey (<a href="#ref-Tuke:58">1958</a>)</span>) was a first step toward simulation-based inference, developed to compute standard errors. It actually produces <span class="math inline">\(n\)</span> <em>systematic</em> (not randomly drawn) samples <span class="math inline">\(F_{(i)}^{(n)},i=1,\ldots,n\)</span>, each one of size <span class="math inline">\(n-1\)</span>, obtained by successively removing one observation at the time. For the functional <span class="math inline">\(\hat{\theta}(F^{(n)})\)</span>, the (unbiased) Jackknife standard error (SE), say <span class="math inline">\(T\)</span>, is given by (see <span class="citation">Efron and Stein (<a href="#ref-EfSt:81">1981</a>)</span>)
<span class="math display">\[\begin{equation}
T(\hat{\theta})=\left[\frac{n-1}{n}\sum_{i=1}^n\left(\hat{\theta}\left(F_{(i)}^{(n)}\right)-\bar{\hat{\theta}}\right)\right]^{1/2}
\end{equation}\]</span>
<p>with <span class="math inline">\(\bar{\hat{\theta}}=1/n\sum_{i=1}^n\hat{\theta}\left(F_{(i)}^{(n)}\right)\)</span>. The jackknife can be seen as a linear approximation of the bootstrap, hence less appropriate for unsmooth estimators (e.g. quantiles).</p>
<blockquote>
Exercise (optional)<br />
- Use the sample experimental setting as in the exercise for the Boostrap and compute the sampling distribution estimate of of the Theil Inequality Index using the jacknife. Compare the results with the ones based on the Bootsrap and Monte Carlo simulations.<br />

</blockquote>
<p>An alternative and natural estimator for <span class="math inline">\(F\)</span> is to assume that <span class="math inline">\(F\)</span> belongs to a family of (parametric) distributions (models), indexed by a parameter vector <span class="math inline">\(\theta\)</span>, i.e. the set <span class="math inline">\(\{F_{\theta}, \theta\in\Theta\subseteq\mathbb{R}^p \}\)</span> and, using the plug-in principle, one gets <span class="math inline">\(\hat{F}=F_{\hat{\theta}}\)</span>. This requires some attention for the choice of <span class="math inline">\(\hat{\theta}\)</span> which ad minima should be consistent. The MLE (computed on the original sample <span class="math inline">\(F^{(n)}\)</span>) is a suitable candidate for consistency, but also for efficiency. To produce <span class="math inline">\(B\)</span> samples of size <span class="math inline">\(n\)</span>, there exists various (implemented) random generators, which are in principle based on a random generation of Uniform(0,1) realizations <span class="math inline">\(u_i,\ldots,u_n\)</span>, from which a sample is obtained via <span class="math inline">\(x_i=F_{\hat{\theta}}^{-1}(u_i), i=1,\ldots,n\)</span>.</p>
<p>Parametric families act as regularizers, smoothing out the raw data and de-emphasizing extreme observations. They are particularly appreciated when studying rare events, like probabilities of extremes, in finance, insurance and with natural phenomena (tides, temperatures, earthquakes, etc.). The obvious drawback is that they need to be specified <em>a priori</em>, but the family can be sufficiently large. In this case, model selection becomes an important step into model building, with an obvious impact on inference.</p>
<blockquote>
Exercise (optional):<br />
There are different ways to simulate samples in the linear regression model case. In the non parametric case, the design matrix is kept fixed or, alternatively, the raws are drawn together with the response. It is not yet clear which method is the most suitable in therms of statistical properties of resulting procedures like significance testing. In the parametric case, only the random part is simulated, i.e. the residuals (with <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\sigma}\)</span>). There is also a semi-parametric version which consists in resampling the residuals.<br />
- With the Malnutrition in Zambia dataset, considering the (complete) linear model without interactions, compute 95% confidence intervals (percentile method) for the slope parameter of the <em>breastfeeding duration</em> variable, using the four different resampling schemes for the linear regression model. Compare.<br />
Hint: Read chapter 11 of <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span>.<br />

</blockquote>
</div>
<div id="model-selection" class="section level3">
<h3><span class="header-section-number">1.6.5</span> Model Selection</h3>
<p>Model selection is a broad concept. For example, for a family of models <span class="math inline">\(F_{\mathbf{\theta}}\)</span>, there exists an (almost) infinite number of different ones according to the value of <span class="math inline">\(\mathbf{\theta}\)</span>. Hence, estimation (parametric or non parametric) is also a form of model selection since it allows, from the sample, to reduce the set of potential models.</p>
<p>Another form of model selection involves, simultaneously, the specification of the parameter’s set <span class="math inline">\(\mathbf{\theta}\)</span> (e.g. what is <span class="math inline">\(p\)</span>) and, within this specification, a reduced set of potential values. This process, like estimation, is by nature inferential, since the only available information is the sample. A trade-off needs then to be made between the <em>model complexity</em> (e.g. <span class="math inline">\(p\)</span>) and the <em>model adequacy</em> (e.g. its fit to the data). The choice of the measure associated to the trade-off is also important.</p>
<p>While model adequacy is an obvious objective to achieve, reducing model complexity is a more subtle, but also an important feature. The reasons include (see also <span class="citation">James et al. (<a href="#ref-JaWiHaTi:13">2013</a>)</span>, Section 2.1.3):</p>
<ul>
<li><p>When we are mainly interested in inference, then restrictive models are much more interpretable.</p></li>
<li><p>When the objective is prediction only and the interpretability of the predictive model is simply not of interest, we might expect that it will be best to use the most flexible model available. Surprisingly, this is not always the case, since more accurate predictions are obtained using a less complex models (see Section 2.1)</p></li>
</ul>
<p>One can separate model selection procedures in three broad categories which are: - <em>Subset Selection</em>: This approach involves identifying a subset of the <span class="math inline">\(p\)</span> <em>predictors</em> (i.e. a non zero subset of <span class="math inline">\(\mathbf{\theta}\)</span>) that we believe to be related to the response. - <em>Shrinkage</em>: This approach involves fitting a model involving all <span class="math inline">\(p\)</span> predictors (or parameters in <span class="math inline">\(\mathbf{\theta}\)</span>). However, the estimated parameters are shrunk towards zero relative in the estimation procedure. This shrinkage is also known as regularization and has the effect of reducing sampling error and, depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero, leading to a form of subset selection. - <em>Dimension Reduction</em>: This approach involves reducing the <span class="math inline">\(p\)</span> predictors into a <span class="math inline">\(q\)</span>-dimensional subspace, where <span class="math inline">\(q &lt;p\)</span>, using a trade-off between information loss due to the dimension reduction and model complexity. The <span class="math inline">\(q\)</span> (orthogonal) axes of the subspace are then used as predictors to fit the model.</p>
<p>Obviously, subset selection and shrinkage are methods that target the model interpretability objective, while dimension reduction might be more appropriate for pure prediction. It is however not clear that, in terms of <em>out-of-sample</em> prediction error, one set of approaches is better than the others. In this course we will mainly focus on subset selection while also presenting shrinkage methods.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-unicef1998">
<p>United Nations Children’s Fund. 1998. “The State of the World’s Children 1998: Focus on Nutrition.” UNICEF, New York, USA.</p>
</div>
<div id="ref-unicef2012">
<p>United Nations Children’s Fund. 2012. “The State of the World’s Children 2012: Children in an Urban World.” UNICEF, New York, USA.</p>
</div>
<div id="ref-victora2008maternal">
<p>Victora, C. G., L. Adair, C. Fall, P. C. Hallal, R. Martorell, L. Richter, and H. S. Sachdev. 2008. “Maternal and Child Undernutrition: Consequences for Adult Health and Human Capital.” <em>Lancet</em> 371 (9609). Elsevier: 340.</p>
</div>
<div id="ref-golub1999molecular">
<p>Golub, Todd R, Donna K Slonim, Pablo Tamayo, Christine Huard, Michelle Gaasenbeek, Jill P Mesirov, Hilary Coller, et al. 1999. “Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring.” <em>Science</em> 286 (5439). American Association for the Advancement of Science: 531–37.</p>
</div>
<div id="ref-Cowe:11">
<p>Cowell, F. A. 2011. <em>Measuring Inequality</em>. Third Edition. Oxford: Oxford University Press.</p>
</div>
<div id="ref-EfHa:16">
<p>Efron, B., and T. Hastie. 2016. <em>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</em>. Cambridge University Press.</p>
</div>
<div id="ref-HuRo:09">
<p>Huber, P. J., and E. M. Ronchetti. 2009. <em>Robust Statistics</em>. Second Edition. Wiley Series in Probability and Statistics. John Wiley &amp; Sons.</p>
</div>
<div id="ref-Fern:01">
<p>Fernholz, L. 2001. “On Multivariate Higher Order von Mises Expansions.” <em>Metrika</em> 53: 123–40.</p>
</div>
<div id="ref-Efro:79">
<p>Efron, B. 1979. “Bootstrap Methods: Another Look at the Jacknife.” <em>Annst</em> 7: 1–26.</p>
</div>
<div id="ref-Quen:56">
<p>Quenouille, Maurice H. 1956. “Notes on Bias in Estimation.” <em>Biometrika</em> 43: 353–60.</p>
</div>
<div id="ref-Tuke:58">
<p>Tukey, John W. 1958. “Bias and Confidence in Not Quite Large Samples (Abstract).” <em>The Annals of Mathematical Statistics</em> 29: 614.</p>
</div>
<div id="ref-EfSt:81">
<p>Efron, B., and C. Stein. 1981. “The Jackknife Estimate of Variance.” <em>Annals of Statististics</em> 9: 586–96.</p>
</div>
<div id="ref-JaWiHaTi:13">
<p>James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. <em>An Introduction to Statistical Learning: With Applications in R</em>. New York: Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="assessing-the-validity-of-a-model.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
