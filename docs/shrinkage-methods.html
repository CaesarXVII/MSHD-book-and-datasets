<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Model Selection in High Dimensions</title>
  <meta name="description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Model Selection in High Dimensions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Model Selection in High Dimensions" />
  
  <meta name="twitter:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  

<meta name="author" content="Maria-Pia Victoria-Feser (professor), Cesare Miglioli and Guillaume Blanc (teaching assistants)">


<meta name="date" content="2019-02-23">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ordering-the-variables.html">
<link rel="next" href="properties-of-model-selection-criteria.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#read-this-part-first"><i class="fa fa-check"></i><b>1.1</b> Read this part first</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#content-choice-and-structure"><i class="fa fa-check"></i><b>1.2</b> Content choice and structure</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#bibliography"><i class="fa fa-check"></i><b>1.2.1</b> Bibliography</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#useful-links"><i class="fa fa-check"></i><b>1.2.2</b> Useful links</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#using-r"><i class="fa fa-check"></i><b>1.3</b> Using R</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#useful-r-packages"><i class="fa fa-check"></i><b>1.3.1</b> Useful R packages</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#managing-data"><i class="fa fa-check"></i><b>1.3.2</b> Managing Data</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#loading-data-from-an-r-package"><i class="fa fa-check"></i><b>1.3.3</b> Loading data from an R package</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#loading-data-from-a-local-file"><i class="fa fa-check"></i><b>1.3.4</b> Loading data from a local file</a></li>
<li class="chapter" data-level="1.3.5" data-path="index.html"><a href="index.html#loading-data-from-an-online-file"><i class="fa fa-check"></i><b>1.3.5</b> Loading data from an online file</a></li>
<li class="chapter" data-level="1.3.6" data-path="index.html"><a href="index.html#loading-data-from-an-online-database-using-a-mysql-query-optional"><i class="fa fa-check"></i><b>1.3.6</b> Loading data from an online database using a mySQL query (Optional)</a></li>
<li class="chapter" data-level="1.3.7" data-path="index.html"><a href="index.html#data-wrangling"><i class="fa fa-check"></i><b>1.3.7</b> Data Wrangling</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#writing-reports"><i class="fa fa-check"></i><b>1.4</b> Writing reports</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#r-markdown"><i class="fa fa-check"></i><b>1.4.1</b> R Markdown</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4.2</b> GitHub</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.5</b> Examples</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#data-on-malnutrition-in-zambia"><i class="fa fa-check"></i><b>1.5.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#prognostic-factors-in-childhood-leukemia"><i class="fa fa-check"></i><b>1.5.2</b> Prognostic Factors in Childhood Leukemia</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#gene-expression-in-prostate-cancer"><i class="fa fa-check"></i><b>1.5.3</b> Gene Expression in Prostate Cancer</a></li>
<li class="chapter" data-level="1.5.4" data-path="index.html"><a href="index.html#gene-expression-ratios-in-lung-cancer-and-mesothelioma"><i class="fa fa-check"></i><b>1.5.4</b> Gene Expression Ratios in Lung Cancer and Mesothelioma</a></li>
<li class="chapter" data-level="1.5.5" data-path="index.html"><a href="index.html#r-package-quantmod"><i class="fa fa-check"></i><b>1.5.5</b> R package quantmod</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#fundamental-statistical-concepts"><i class="fa fa-check"></i><b>1.6</b> Fundamental statistical concepts</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#sample-and-population"><i class="fa fa-check"></i><b>1.6.1</b> Sample and population</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#models-and-risk"><i class="fa fa-check"></i><b>1.6.2</b> Models and risk</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#estimators-and-associated-variability"><i class="fa fa-check"></i><b>1.6.3</b> Estimators and associated variability</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#simulating-the-population-using-resampling-techniques"><i class="fa fa-check"></i><b>1.6.4</b> Simulating the population using resampling techniques</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#model-selection"><i class="fa fa-check"></i><b>1.6.5</b> Model Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html"><i class="fa fa-check"></i><b>2</b> Assessing the validity of a model</a><ul>
<li class="chapter" data-level="2.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#cross-validation"><i class="fa fa-check"></i><b>2.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="2.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#covariance-penalties-criteria"><i class="fa fa-check"></i><b>2.3</b> Covariance Penalties Criteria</a><ul>
<li class="chapter" data-level="2.3.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-2"><i class="fa fa-check"></i><b>2.3.1</b> Introduction</a></li>
<li class="chapter" data-level="2.3.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mallows-c_p"><i class="fa fa-check"></i><b>2.3.2</b> Mallows <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="2.3.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#efrons-q-class"><i class="fa fa-check"></i><b>2.3.3</b> Efron’s <span class="math inline">\(q\)</span>-class</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#information-theory-and-bayesian-criteria"><i class="fa fa-check"></i><b>2.4</b> Information Theory and Bayesian Criteria</a><ul>
<li class="chapter" data-level="2.4.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#aic-akaike-information-criterion"><i class="fa fa-check"></i><b>2.4.1</b> AIC: Akaike Information Criterion</a></li>
<li class="chapter" data-level="2.4.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#bic-bayesian-information-criterion"><i class="fa fa-check"></i><b>2.4.2</b> BIC: Bayesian Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mean-squared-error-based-criteria"><i class="fa fa-check"></i><b>2.5</b> Mean Squared Error Based Criteria</a><ul>
<li class="chapter" data-level="2.5.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#steins-unbiased-risk-estimator-sure"><i class="fa fa-check"></i><b>2.5.1</b> Stein’s unbiased risk estimator (SURE)</a></li>
<li class="chapter" data-level="2.5.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-focused-information-criterion-fic"><i class="fa fa-check"></i><b>2.5.2</b> The Focused Information Criterion (FIC)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-measures"><i class="fa fa-check"></i><b>2.6</b> Classification measures</a><ul>
<li class="chapter" data-level="2.6.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-logistic-model"><i class="fa fa-check"></i><b>2.6.1</b> The logistic model</a></li>
<li class="chapter" data-level="2.6.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#prediction-error-measures-for-binary-classification"><i class="fa fa-check"></i><b>2.6.2</b> Prediction error measures for Binary classification</a></li>
<li class="chapter" data-level="2.6.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-error-estimation"><i class="fa fa-check"></i><b>2.6.3</b> Classification error estimation</a></li>
<li class="chapter" data-level="2.6.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-roc-curve"><i class="fa fa-check"></i><b>2.6.4</b> The ROC curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html"><i class="fa fa-check"></i><b>3</b> Ordering the variables</a><ul>
<li class="chapter" data-level="3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#stepwise-forward-regression"><i class="fa fa-check"></i><b>3.2</b> Stepwise forward regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#partial-correlations"><i class="fa fa-check"></i><b>3.2.1</b> Partial correlations</a></li>
<li class="chapter" data-level="3.2.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#selection-by-hypothesis-testing"><i class="fa fa-check"></i><b>3.2.2</b> Selection by hypothesis testing</a></li>
<li class="chapter" data-level="3.2.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#orthogonal-matching-pursuit"><i class="fa fa-check"></i><b>3.2.3</b> Orthogonal matching pursuit</a></li>
<li class="chapter" data-level="3.2.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#forward-stagewise-regression"><i class="fa fa-check"></i><b>3.2.4</b> Forward stagewise regression</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#streamwise-regression"><i class="fa fa-check"></i><b>3.3</b> Streamwise regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-4"><i class="fa fa-check"></i><b>3.3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#sure-independence-screening"><i class="fa fa-check"></i><b>3.3.2</b> Sure Independence Screening</a></li>
<li class="chapter" data-level="3.3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#pc-simple-algorithm"><i class="fa fa-check"></i><b>3.3.3</b> PC-simple algorithm</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-and-regression-tree-cart"><i class="fa fa-check"></i><b>3.4</b> Classification And Regression Tree (CART)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#regression-tree"><i class="fa fa-check"></i><b>3.4.1</b> Regression tree</a></li>
<li class="chapter" data-level="3.4.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-trees"><i class="fa fa-check"></i><b>3.4.2</b> Classification Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html"><i class="fa fa-check"></i><b>4</b> Shrinkage Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#ridge-regression"><i class="fa fa-check"></i><b>4.2</b> Ridge regression</a></li>
<li class="chapter" data-level="4.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-lasso-estimator"><i class="fa fa-check"></i><b>4.3</b> The lasso estimator</a></li>
<li class="chapter" data-level="4.4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#alternative-penalized-regression-methods"><i class="fa fa-check"></i><b>4.4</b> Alternative penalized regression methods</a><ul>
<li class="chapter" data-level="4.4.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-adaptive-and-relaxed-lasso"><i class="fa fa-check"></i><b>4.4.1</b> The adaptive and relaxed lasso</a></li>
<li class="chapter" data-level="4.4.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-elastic-net"><i class="fa fa-check"></i><b>4.4.2</b> The elastic net</a></li>
<li class="chapter" data-level="4.4.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-nonnegative-garotte"><i class="fa fa-check"></i><b>4.4.3</b> The nonnegative garotte</a></li>
<li class="chapter" data-level="4.4.4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#non-convex-penalties"><i class="fa fa-check"></i><b>4.4.4</b> Non convex penalties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html"><i class="fa fa-check"></i><b>5</b> Properties of model selection criteria</a><ul>
<li class="chapter" data-level="5.1" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#introduction-6"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#selection-consistency"><i class="fa fa-check"></i><b>5.2</b> Selection consistency</a></li>
<li class="chapter" data-level="5.3" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#selection-efficiency"><i class="fa fa-check"></i><b>5.3</b> Selection efficiency</a></li>
<li class="chapter" data-level="5.4" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#the-oracle-property"><i class="fa fa-check"></i><b>5.4</b> The oracle property</a></li>
<li class="chapter" data-level="5.5" data-path="properties-of-model-selection-criteria.html"><a href="properties-of-model-selection-criteria.html#probability-of-overfitting"><i class="fa fa-check"></i><b>5.5</b> Probability of overfitting</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="post-selection-inference.html"><a href="post-selection-inference.html"><i class="fa fa-check"></i><b>6</b> Post-Selection Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="post-selection-inference.html"><a href="post-selection-inference.html#introduction-7"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="post-selection-inference.html"><a href="post-selection-inference.html#inference-via-the-nonparametric-bootstrap"><i class="fa fa-check"></i><b>6.2</b> Inference via the nonparametric Bootstrap</a></li>
<li class="chapter" data-level="6.3" data-path="post-selection-inference.html"><a href="post-selection-inference.html#improving-on-the-bootstrap-smoothed-bootstrap-or-bagging"><i class="fa fa-check"></i><b>6.3</b> Improving on the Bootstrap: Smoothed Bootstrap or Bagging</a></li>
<li class="chapter" data-level="6.4" data-path="post-selection-inference.html"><a href="post-selection-inference.html#post-selection-significance-testing"><i class="fa fa-check"></i><b>6.4</b> Post selection significance testing</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i><b>7</b> Solutions</a><ul>
<li class="chapter" data-level="7.1" data-path="solutions.html"><a href="solutions.html#chapter-1"><i class="fa fa-check"></i><b>7.1</b> Chapter 1</a><ul>
<li class="chapter" data-level="7.1.1" data-path="solutions.html"><a href="solutions.html#zam"><i class="fa fa-check"></i><b>7.1.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="7.1.2" data-path="solutions.html"><a href="solutions.html#leuk"><i class="fa fa-check"></i><b>7.1.2</b> Prognostic Factors in Childhood Leukemia</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="solutions.html"><a href="solutions.html#chapter-2"><i class="fa fa-check"></i><b>7.2</b> Chapter 2</a><ul>
<li class="chapter" data-level="7.2.1" data-path="solutions.html"><a href="solutions.html#cv"><i class="fa fa-check"></i><b>7.2.1</b> Cross-validation</a></li>
<li class="chapter" data-level="7.2.2" data-path="solutions.html"><a href="solutions.html#aic"><i class="fa fa-check"></i><b>7.2.2</b> Akaike Information Criterion</a></li>
<li class="chapter" data-level="7.2.3" data-path="solutions.html"><a href="solutions.html#roc"><i class="fa fa-check"></i><b>7.2.3</b> ROC curves</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="solutions.html"><a href="solutions.html#chapter-3"><i class="fa fa-check"></i><b>7.3</b> Chapter 3</a><ul>
<li class="chapter" data-level="7.3.1" data-path="solutions.html"><a href="solutions.html#HT"><i class="fa fa-check"></i><b>7.3.1</b> Selection by Hypothesis Testing</a></li>
<li class="chapter" data-level="7.3.2" data-path="solutions.html"><a href="solutions.html#SIS"><i class="fa fa-check"></i><b>7.3.2</b> Sure Independence Screening</a></li>
<li class="chapter" data-level="7.3.3" data-path="solutions.html"><a href="solutions.html#PC"><i class="fa fa-check"></i><b>7.3.3</b> PC-simple algorithm</a></li>
<li class="chapter" data-level="7.3.4" data-path="solutions.html"><a href="solutions.html#RT"><i class="fa fa-check"></i><b>7.3.4</b> Regression Tree</a></li>
<li class="chapter" data-level="7.3.5" data-path="solutions.html"><a href="solutions.html#CT"><i class="fa fa-check"></i><b>7.3.5</b> Classification Tree</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="solutions.html"><a href="solutions.html#chapter-4"><i class="fa fa-check"></i><b>7.4</b> Chapter 4</a><ul>
<li class="chapter" data-level="7.4.1" data-path="solutions.html"><a href="solutions.html#Ridge"><i class="fa fa-check"></i><b>7.4.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="7.4.2" data-path="solutions.html"><a href="solutions.html#lasso"><i class="fa fa-check"></i><b>7.4.2</b> Lasso</a></li>
<li class="chapter" data-level="7.4.3" data-path="solutions.html"><a href="solutions.html#nonconvex"><i class="fa fa-check"></i><b>7.4.3</b> Non Convex Penalties</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Model Selection in High Dimensions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="shrinkage-methods" class="section level1">
<h1><span class="header-section-number">4</span> Shrinkage Methods</h1>
<div id="introduction-5" class="section level2">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
Shrinkage methods denote estimation methods under constraints that <em>shrink</em> estimators toward a given value. The origin goes back to the James–Stein estimator <span class="citation">(James and Stein <a href="#ref-JaSt:61">1961</a>)</span>. The rationale is that one can sacrifice <em>bias</em> for <em>variance</em>, i.e. optimize in some sense the Mean Squared Error (MSE), which, for an estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is
<span class="math display">\[\begin{equation}
\mbox{MSE}\left(\hat{\boldsymbol{\beta}}\right)=\mathbb{E}\left[\left\Vert\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right\Vert^2\right]=\mathbb{E}\left[\left(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right)^T\left(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right)\right]
\end{equation}\]</span>
<p>(see <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span>, Section 7.1, for details on the James–Stein estimator)</p>
<p>For the least squares (LS) estimator of the linear model <span class="math inline">\(\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}\)</span>, <span class="math inline">\(\boldsymbol{\varepsilon}\sim N\left(\mathbf{0},\sigma^2\mathbf{I}\right)\)</span>, and supposing without loss of generality that <span class="math inline">\(\mathbf{y}\)</span> is centered and the covariates (columns in <span class="math inline">\(\mathbf{X}\)</span>) are standardized, we have <span class="math inline">\(\mbox{MSE}\left(\hat{\boldsymbol{\beta}}_{LS}\right)=\sigma^2\mbox{tr}\left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\right]\)</span>, which obviously grows with the dimension <span class="math inline">\(p\)</span>.</p>
In terms of out-of-sample prediction error (PE), we have the following result
<span class="math display">\[\begin{eqnarray}
\mbox{PE}\left(\mathbf{x}_0,\hat{\boldsymbol{\beta}}\right)
&amp;=&amp;\sigma^2+\mathbb{E}_{Y\vert\mathbf{X}=\mathbf{x}_0}\left[\left(Y-\mathbf{x}_0\hat{\boldsymbol{\beta}}\right)\right]^2
+\mathbb{E}_{Y\vert \mathbf{X}=\mathbf{x}_0}\left[\left(Y-\mathbf{x}_0\hat{\boldsymbol{\beta}}\right)^2\right] \\
&amp;=&amp; \sigma^2+\mbox{Bias}^2\left(\mathbf{x}_0,\hat{\boldsymbol{\beta}}\right)+
\mbox{var}\left(\mathbf{x}_0,\hat{\boldsymbol{\beta}}\right)
\end{eqnarray}\]</span>
<p>As model becomes more complex (more terms included), on one hand, local structure/curvature can be picked up, but on the other hand, coefficient estimates suffer from high variance as more terms are included in the model (see overfitting phenomenon).</p>
<blockquote>
<p>Reported from <a href="http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf" class="uri">http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf</a> <img src="Figures/BiasVarTradeOff.png" alt="Top" /></p>
</blockquote>
<p>Hence, introducing a little bias in the estimators might lead to a substantial decrease in variance, and hence to a substantial decrease in PE. The idea is then not to let the <span class="math inline">\(\hat{\beta}_j\)</span> take too large values in order to avoid too large variances. To control the variance, the slope coefficients are <em>regularized</em> in that a constraint is added to the optimization problem (e.g. LS) that controls how large the coefficients can be.</p>
</div>
<div id="ridge-regression" class="section level2">
<h2><span class="header-section-number">4.2</span> Ridge regression</h2>
The first proposed regularization method is the <em>ridge</em> constraint (or Tikhonov regularization <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization" class="uri">https://en.wikipedia.org/wiki/Tikhonov_regularization</a>), which amounts to set the constraint <span class="math inline">\(\sum_{i=1}^p\beta_j^2&lt;t_{\lambda}\)</span>. In terms of optimization, for a given value of <span class="math inline">\(\lambda\)</span>, the ridge estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}(\lambda)\)</span> is given by
<span class="math display">\[\begin{equation}
\hat{\boldsymbol{\beta}}(\lambda)=\mbox{argmin}_{\boldsymbol{\beta}}\left\Vert\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\right\Vert^2-\lambda\Vert\boldsymbol{\beta}\Vert^2
\end{equation}\]</span>
The solution is
<span class="math display">\[\begin{eqnarray}
\hat{\boldsymbol{\beta}}(\lambda)&amp;=&amp;\left(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I}\right)^{-1}\mathbf{X}^T\mathbf{y}\\
&amp;=&amp; \left(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I}\right)^{-1}\left(\mathbf{X}^T\mathbf{X}\right)\hat{\boldsymbol{\beta}}_{LS}
\end{eqnarray}\]</span>
<p>Hence, the variance of <span class="math inline">\(\hat{\boldsymbol{\beta}}_{LS}\)</span> is <em>shrunk</em> by an amount of <span class="math inline">\(\left(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I}\right)^{-1}\left(\mathbf{X}^T\mathbf{X}\right)\)</span> which depends on <span class="math inline">\(\lambda\)</span>. With <span class="math inline">\(\lambda=0\)</span>, there is no shrinkage and <span class="math inline">\(\hat{\boldsymbol{\beta}}(0)=\hat{\boldsymbol{\beta}}_{LS}\)</span>, while when <span class="math inline">\(\lambda\rightarrow\infty\)</span>, <span class="math inline">\(\hat{\boldsymbol{\beta}}(\lambda)\rightarrow\mbox{0}\)</span>, the maximal shinkage.</p>
<blockquote>
<p>Reported from <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span>, Section 7.2: Ridge regression applied to the diabetes data <img src="Figures/RidgeEstimExample.png" alt="Top" /></p>
</blockquote>
<p>Ridge estimators have lower variance the the LS, but this does not guarantee that the corresponding predictions <span class="math inline">\(\hat{\boldsymbol{\mu}}\left(\lambda\right)=\mathbf{X}\hat{\boldsymbol{\beta}}(\lambda)\)</span> are more accurate than the ones obtained by the LS <span class="math inline">\(\hat{\boldsymbol{\mu}}(0)=\mathbf{X}\hat{\boldsymbol{\beta}}(0)\)</span>, since <span class="math inline">\(\hat{\boldsymbol{\beta}}(\lambda)\)</span> is biased towards <span class="math inline">\(\mathbf{0}\)</span>. However, in high dimensions, the prior belief is that most of the <span class="math inline">\(\beta_j\)</span> lie near zero, and biasing the LS estimator toward zero then becomes a necessity.</p>
<p>Another notable advantage of the ridge estimator is that the inclusion of <span class="math inline">\(\lambda\)</span> makes the optimization problem solvable even if <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is not invertible (e.g. <span class="math inline">\(\mathbf{X}\)</span> is not of full rank when <span class="math inline">\(p&gt;n\)</span>). This was the original motivation for ridge regression as proposed by <span class="citation">Hoerl and Kennard (<a href="#ref-HoKe:70">1970</a>)</span> who also recommended to select <span class="math inline">\(\lambda\)</span> <em>graphically</em>. Standard practice now is to use (10-fold) cross-validation.</p>
<blockquote>
<p>Exercise (solutions presented in <a href="solutions.html#Ridge">7.4.1</a>):<br />
- Use the function <em>glmnet()</em> to perform a Ridge regression on <em>Zambia</em> dataset, plot the values as a function of <span class="math inline">\(\lambda\)</span> and comment on the results.<br />
- Use the function <em>cv.glmnet()</em> to perform model selection based on 10-fold Cross Validation (i.e. method to select the <span class="math inline">\(\lambda\)</span> parameter), plot the results and comment the graph that you obtain. Which values of <span class="math inline">\(\lambda\)</span> are shown by default?<br />
- Use the function <em>predict()</em> to retrieve the final model estimates and perform a simple linear model on the same covariates, what can you conclude?</p>
</blockquote>
</div>
<div id="the-lasso-estimator" class="section level2">
<h2><span class="header-section-number">4.3</span> The lasso estimator</h2>
<p>Instead of regularizing the estimator using <span class="math inline">\(\sum_{i=1}^p\beta_j^2=\Vert\boldsymbol{\beta}\Vert^2&lt;t_{\lambda}\)</span>, one could use other <span class="math inline">\(l_q\)</span>-norms, i.e. <span class="math inline">\(\Vert\boldsymbol{\beta}\Vert_{q}=\sum_{i=1}^p\vert\beta_j\vert^{q}&lt;t_{\lambda}\)</span>. In particular, when <span class="math inline">\(q=0\)</span>, we have <span class="math inline">\(\Vert\boldsymbol{\beta}\Vert_{0}=\sum_{i=1}^p\vert\beta_j\vert^{0}&lt;t_{\lambda}\)</span> with, by definition, <span class="math inline">\(\sum_{i=1}^p\vert\beta_j\vert^{0}=\sum_{i=1}^pI\left(\beta_j\neq 0\right)\)</span>, the number of slope coefficients different from <span class="math inline">\(0\)</span>, and hence corresponds to subset selection.</p>
<span class="citation">Tibshirani (<a href="#ref-Tibs:96">1996</a>)</span> proposed to use <span class="math inline">\(q=1\)</span> leading to the very famous lasso estimator. The great advantage of the lasso, on top of the fact that the resulting optimization problem
<span class="math display">\[\begin{equation}
\hat{\boldsymbol{\beta}}(\lambda)=\mbox{argmin}_{\boldsymbol{\beta}}\left\Vert\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\right\Vert^2-\lambda\Vert\boldsymbol{\beta}\Vert_1
\end{equation}\]</span>
<p>is convex (since both the loss <span class="math inline">\(\left\Vert\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\right\Vert^2\)</span> and the constraint <span class="math inline">\(\Vert\boldsymbol{\beta}\Vert_1=\sum_{j=1}^p\vert\beta_j\vert\)</span> are convex in <span class="math inline">\(\boldsymbol{\beta}\)</span>), is that the shrinking forces a set of <span class="math inline">\(\beta_j\)</span> to exactly <span class="math inline">\(0\)</span>. Hence, the lasso estimator provides simultaneously a regularized estimator and model (subset) selection.</p>
<blockquote>
<p>Reported from <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span>, Section 16.2 <img src="Figures/LassoRidgeIllustration.png" alt="Top" /></p>
</blockquote>
<p>As for the ridge estimator, <span class="math inline">\(\lambda\)</span> needs to be chosen and, for model selection purposes (or out-of-sample prediction error optimization), a criteria that assesses the model validity can be used. The most comon choices are the <span class="math inline">\(C_p\)</span> and 10-fold CV. Practically, for <em>well</em> chosen values of <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\hat{\boldsymbol{\beta}}(\lambda)\)</span> and the corresponding model validity criterion are computed. <span class="math inline">\(\lambda\)</span> is chosen as to optimize (minimize) the later.</p>
<p>To compute the entire lasso path (i.e. for all values of <span class="math inline">\(\lambda\)</span> that <em>add</em> one <span class="math inline">\(\beta_j\neq 0\)</span> at each step), <span class="citation">Efron et al. (<a href="#ref-EfHaJoTi:04">2004</a>)</span> propose the Least Angle Regression algorithm (LARS), which is based on correlations between the covariates to enter and the residuals from the previous fit. LARS is introduced as a more <em>democratic</em> version of forward stepwise regression. Indeed, while forward stepwise regression builds a model sequentially, adding one variable at a time and updating the LS estimator to include all the active variables, the LARS only enters <em>as much</em> of a covariate as it <em>deserves</em>. At the first step it identifies the variable most correlated with the response, and moves the coefficient of this variable continuously toward (not fully) its LS estimate. The residuals are updated and this causes the correlations with other variables to evolve. As soon as another variable reaches the same correlation with the residual as the ones in the active set, this variable then joins the active set.</p>
<blockquote>
<p>Reported from <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-HaTiFr:09">2009</a>)</span>, Section 3.4. Note that <span class="math inline">\(\langle \mbox{x}_j,\mathbf{r}\rangle= \mbox{x}_j^T\mathbf{r}\)</span>. <img src="Figures/LARS.png" alt="Top" /></p>
</blockquote>
<p>In step 3 and 4, the values of the slopes in the active set, say <span class="math inline">\(\mathcal{A}\)</span>, are augmented as <span class="math inline">\(\boldsymbol{\beta}_{\mathcal{A}}(\epsilon)=\boldsymbol{\beta}_{\mathcal{A}}+\epsilon\cdot\delta_{\mathcal{A}}\)</span> for small values of <span class="math inline">\(\epsilon\)</span>, with <span class="math inline">\(\delta_{\mathcal{A}}=\left(\mathbf{X}^T_{\mathcal{A}}\mathbf{X}_{\mathcal{A}}\right)^{-1}\mathbf{X}^T_{\mathcal{A}}\mathbf{r}_{\mathcal{A}}\)</span>, <span class="math inline">\(\mathbf{r}_{\mathcal{A}}=\mathbf{y}-\mathbf{X}_{\mathcal{A}}\boldsymbol{\beta}_{\mathcal{A}}\)</span>.</p>
<p>The LARS bears very close similarities with forward stagewise regression (see <span class="citation">Hastie et al. (<a href="#ref-HaTaTiWa:07">2007</a>)</span>, <span class="citation">Tibshirani (<a href="#ref-TibR:15">2015</a>)</span> and <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span>, Section 16.4). Hence, if the covariates are correlated, the lasso path might not produce the suitable model selection path (risk of overfitting).</p>
<blockquote>
<p>Exercise (solutions presented in <a href="solutions.html#lasso">7.4.2</a>):<br />
- Use the function <em>glmnet()</em> to perform a lasso on <em>Zambia</em> dataset, plot the values as a function of <span class="math inline">\(\lambda\)</span> and comment on the results.<br />
- Use the function <em>cv.glmnet()</em> to perform model selection based on 10-fold Cross Validation (i.e. method to select the <span class="math inline">\(\lambda\)</span> parameter), plot the results and comment the graph that you obtain. Which values of <span class="math inline">\(\lambda\)</span> are shown by default? What can you conclude on the choice of <span class="math inline">\(\lambda\)</span> in terms of model selection?<br />
- Use the function <em>predict()</em> to retrieve the final model chosen by 10-fold CV (given lasso ordering) and perform a linear model on the covariates present in the final model. What can you conclude observing the estimates?</p>
</blockquote>
</div>
<div id="alternative-penalized-regression-methods" class="section level2">
<h2><span class="header-section-number">4.4</span> Alternative penalized regression methods</h2>
<div id="the-adaptive-and-relaxed-lasso" class="section level3">
<h3><span class="header-section-number">4.4.1</span> The adaptive and relaxed lasso</h3>
Because the lasso finds a suitable compromise between estimation bias and variance, the price to pay is that it generally provides (asymptotically) biased slope estimators. It can be shown that the bias is larger for (true) large slope coefficients (in absolute value). Ideally then, one should consider, instead of the lasso penalty <span class="math inline">\(\lambda\Vert\boldsymbol{\beta}\Vert_1\)</span>, more generally <span class="math inline">\(\sum_{j=1}^p\lambda_j\vert\beta_j\vert\)</span>. <span class="citation">Zou (<a href="#ref-ZouH:06">2006</a>)</span> propose the <em>adaptive lasso</em> with <span class="math inline">\(\lambda_j=\lambda w_j^{\gamma}\)</span> and
<span class="math display">\[\begin{equation}
w_j^{\gamma}=\left\vert \hat{\beta}_{j}^{(LS)}\right\vert^{-\gamma}, \; \gamma&gt;0
\end{equation}\]</span>
<p>Hence, <span class="math inline">\(w_j\)</span> assigns smaller weights to larger slope coefficients, and when <span class="math inline">\(\hat{\beta}_{j}^{(LS)}\rightarrow 0\)</span>, <span class="math inline">\(w_j \rightarrow \infty\)</span> so that the adaptive lasso estimate <span class="math inline">\(\hat{\beta}_j \rightarrow 0\)</span>. Actually, <span class="math inline">\(\hat{\boldsymbol{\beta}}^{(LS)}\)</span> can be replaced by any (<span class="math inline">\(\sqrt{n}\)</span>) consistent estimator of <span class="math inline">\(\boldsymbol{\beta}\)</span>. In practice, when <span class="math inline">\(p&gt;n\)</span>, the ridge estimator is a suitable candidate.</p>
The corresponding optimization problem is
<span class="math display">\[\begin{equation}
\hat{\boldsymbol{\beta}}(\lambda,\gamma)=\mbox{argmin}_{\boldsymbol{\beta}}\left\Vert\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\right\Vert^2-\lambda\sum_{j=1}^pw_j^{\gamma}\vert\beta_j\vert
\end{equation}\]</span>
and remains a convex optimization problem. Actually, the LARS algorithm can be used to obtain the adaptive losso path, by applying it to the scaled covariates <span class="math inline">\(\tilde{\mathbf{x}}_j=\mathbf{x}_j/w_j^{\gamma}\)</span>. Namely, let <span class="math inline">\(\tilde{\mathbf{X}}=\left[\tilde{\mathbf{x}}_j\right]_{j=1,\ldots,p}\)</span> and
<span class="math display">\[\begin{equation}
\tilde{\boldsymbol{\beta}}(\lambda)=\mbox{argmin}_{\boldsymbol{\beta}}\left\Vert\mathbf{y}-\tilde{\mathbf{X}}\boldsymbol{\beta}\right\Vert^2-\lambda\Vert\boldsymbol{\beta}\Vert_1
\end{equation}\]</span>
<p>the adpative lasso estimator is <span class="math inline">\(\left[\hat{\beta}_j(\lambda,\gamma)\right]_{j=1,\ldots,p}\)</span> with <span class="math inline">\(\hat{\beta}_j(\lambda,\gamma)=\tilde{\beta}_j(\lambda)/w_j^{\gamma}\)</span>.</p>
<p>As with the lasso, the question of the choice of the tunning parameters is crucial. Since there are two, ideally all combiations of <span class="math inline">\((\lambda,\gamma)\)</span> should be evaluated. Hence, for a given <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\lambda\)</span> is optimized using an out-of-sample validation measure such as 10-fold CV or the <span class="math inline">\(C_p\)</span> to get <span class="math inline">\(\lambda_{\gamma}\)</span> and in a second layer, <span class="math inline">\(\gamma\)</span> is also optimized (actually <span class="math inline">\((\gamma,\lambda_{\gamma})\)</span> is optimized in <span class="math inline">\(\gamma\)</span>) again using an out-of-sample validation measure such as 10-fold CV or the <span class="math inline">\(C_p\)</span>. The choice for the latter does not have to be the same for <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\gamma\)</span>.</p>
The adaptive lasso is a <em>two-stage approach</em>, with at the first stage a chosen initial estimator which produces a single set of weights <span class="math inline">\(w_j^{\gamma}\)</span> which are held constant (for fixed <span class="math inline">\(\gamma\)</span>) across all values of <span class="math inline">\(\lambda\)</span>. An alternative approach, known as a <em>pathwise approach</em> is to let the weights change with <span class="math inline">\(\lambda\)</span>: <span class="math inline">\(w_j^{\gamma\lambda}:=w_j^{\gamma\lambda}\left(\hat{\beta}_j\right)\)</span>. A notable example is the <em>relaxed</em> lasso of <span class="citation">Meinshausen (<a href="#ref-Mein:07">2007</a>)</span> which, for <span class="math inline">\(\lambda\geq 0\)</span> and <span class="math inline">\(0&lt;\phi\leq 1\)</span> is given by
<span class="math display">\[\begin{equation}
\hat{\boldsymbol{\beta}}(\lambda,\phi)=\mbox{argmin}_{\boldsymbol{\beta}}\left\Vert\mathbf{y}-\sum_{j=1}^p\mathbf{x}_j{\beta}_jI\left(j\in \mathcal{A}_{\lambda}\right)\right\Vert^2-\phi\lambda\Vert\boldsymbol{\beta}\Vert_1
\end{equation}\]</span>
<p>with <span class="math inline">\(\mathcal{A}_{\lambda}\)</span> the set of indicators of the covariates in the active set produced by the corresponding <span class="math inline">\(\lambda\)</span>. The later hence controls the variable selection part, as in the lasso, while the <em>relaxation</em> parameter <span class="math inline">\(\phi\)</span> controls the shrinkage of coefficients. If <span class="math inline">\(\phi= 1\)</span>, the lasso and relaxed lasso estimators are identical, and for <span class="math inline">\(\phi&lt;1\)</span>, the shrinkage of coefficients in the selected model is reduced compared to the lasso. When <span class="math inline">\(\phi\rightarrow 0\)</span>, <span class="math inline">\(\hat{\beta}_j(\lambda,\phi)\rightarrow\hat{\beta}_j^{(LS/\mathcal{A}_{\lambda})}\)</span>, where the <span class="math inline">\(\hat{\beta}_j^{(LS/\mathcal{A}_{\lambda})}\)</span> is the LS obtained on the subset <span class="math inline">\(\mathcal{A}_{\lambda}\)</span>. Note that if <span class="math inline">\(\phi=0\)</span> one gets the LS on the full model (the effect of <span class="math inline">\(\lambda\)</span> as variable selection tunning parameters disappears).</p>
<p>To obtain the relaxed lasso solutions, one first sets <span class="math inline">\(\phi=1\)</span> and obtains the lasso path, then for each submodel of the lasso path, the LARS algorithm is run by varying the penalty <span class="math inline">\(\phi\lambda\)</span> through <span class="math inline">\(\phi\)</span>. There is in general no need to do the full LARS in the second stage so the compuation can be nearly as fast as a for the lasso. Again, the tunning parameters <span class="math inline">\((\lambda,\phi)\)</span> can be determined by 10-fold CV.</p>
</div>
<div id="the-elastic-net" class="section level3">
<h3><span class="header-section-number">4.4.2</span> The elastic net</h3>
When there is strong correlation among the covariates, the lasso estimator will somehow arbitrarily choose among a set of strong but correlated variables. The ridge penalty, on the other hand, tends to shrink the coefficients of correlated variables toward each other. The elastic net penalty proposed by <span class="citation">Zou and Hastie (<a href="#ref-zou2005regularization">2005</a>)</span> is a comprimise between the lasso and ridge regression and the estimators are the solution of the optimization problem
<span class="math display">\[\begin{equation}
\hat{\boldsymbol{\beta}}(\lambda)=\mbox{argmin}_{\boldsymbol{\beta}}\left\Vert\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\right\Vert^2-\alpha\lambda\Vert\boldsymbol{\beta}\Vert_1-\frac{1}{2}(1-\alpha)\lambda\Vert\boldsymbol{\beta}\Vert^2
\end{equation}\]</span>
<p>The elastic net solution contains in general more active covariates with smaller coefficients. The introduction of the ridge penalty has the effect of reducing the variance of lasso estimates at the cost of further increasing their bias. Adding a ridge penalty is not always universally benefficial, as the bias can dominate the variance. However, since like ridge regression, the elastic net shrinks the coefficients of correlated variables toward each other, it tends to select correlated variables in groups, which might be an advantage in for example biological studies.</p>
</div>
<div id="the-nonnegative-garotte" class="section level3">
<h3><span class="header-section-number">4.4.3</span> The nonnegative garotte</h3>
Based on the argument that shrinking decreases the estimation/prediction variance, <span class="citation">Breiman (<a href="#ref-Brei:95">1995</a>)</span> proposes to directly define shrunk estimators as
<span class="math display">\[\begin{equation}
\hat{\beta}_j(\lambda)=u_j(\lambda)\hat{\beta}_j^{(LS)}
\end{equation}\]</span>
with shrinking factors (or <em>garotte</em>) <span class="math inline">\(\mathbf{u}(\lambda)=[u_j(\lambda)]_{j=1,\ldots,p}\)</span> obtained as
<span class="math display">\[\begin{eqnarray}
&amp; \mathbf{u}(\lambda)=\mbox{argmin}_{\mathbf{u}=[u_1\ldots u_p]}\left\Vert\mathbf{y}-\mathbf{X}\mathbf{B}\mathbf{u}\right\Vert^2-2\lambda\sum_{j=1}^pu_j \\
&amp; \mbox{s.t. } u_j\geq 0 \forall j
\end{eqnarray}\]</span>
<p>with <span class="math inline">\(\mathbf{B}=\mbox{diag}\left(\hat{\beta}_j^{(LS)}\right)\)</span>. The penalty corresponds to the constraint <span class="math inline">\(\sum_{j=1}^pu_j\leq t_{\lambda}\)</span>. For sufficiently large <span class="math inline">\(\lambda\)</span> some of the <span class="math inline">\(u_j\)</span> will be set to 0, so that the nonnegative garotte estimator performs simultaneously shinking and subset selection.</p>
<p><span class="citation">Yuan and Lin (<a href="#ref-YuYi:07">2007</a>)</span> propose to use a slightly modified version of the LARS to obtain the solution path for the nonnegative garotte estimators. Indeed, since <span class="math inline">\(\mathbf{B}\)</span> is provided in an initial stage, then one can use the LARS (adapted to the positivity constraints) in which <span class="math inline">\(\mathbf{X}\rightarrow \tilde{\mathbf{X}}=\mathbf{X}\mathbf{B}\)</span> and <span class="math inline">\(\boldsymbol{\beta}\rightarrow \mathbf{u}\)</span>, <span class="math inline">\(\lambda\rightarrow 2\lambda\)</span>.</p>
<p>The adaptive lasso with <span class="math inline">\(\gamma=1\)</span> corresponds to the nonnegative garotte (when adding the positivity constraints). Indeed, we have <span class="math inline">\(u_j=\vert\beta_j\vert/\vert\beta_j^{(LS)}\vert\)</span> with the constraints <span class="math inline">\(\beta_j\beta_j^{(LS)}\geq 0\)</span>, <span class="math inline">\(\forall j\)</span>, so that <span class="math inline">\(\mathbf{X}\mathbf{B}\mathbf{u}=\mathbf{X}\boldsymbol{\beta}\)</span>.</p>
Given the form of the nonnegation garotte estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}(\lambda)=\left[\hat{\beta}_j(\lambda)\right]_{j=1,\ldots,p}\)</span>, it is straightforward to derive Efron’s covariance penalty estimator of out-of-sample prediction error. Indeed, <span class="math inline">\(\hat{\mathbf{y}}=\mathbf{X}\mathbf{B}\mathbf{u}=\mathbf{X}\mathbf{U}\hat{\boldsymbol{\beta}}^{(LS)}=\mathbf{X}\mathbf{U}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\mathbf{y}\)</span>, with <span class="math inline">\(\mathbf{U}=\mbox{diag}\left(u_j\right)\)</span>, and
<span class="math display">\[\begin{equation}
\mbox{tr}\left(\mbox{cov}(\hat{\mathbf{Y}},\mathbf{Y})\right)=
\sigma^2\mbox{tr}\left(\mathbf{X}\mathbf{U}\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\mathbf{X}^T\right)=\sigma^2\mbox{tr}\left(\mathbf{U}\right)=\sigma^2\sum_{j=1}^pu_j;
\end{equation}\]</span>
<p>by using the <em>cyclic property</em> of the trace operator (see also <span class="citation">Xiong (<a href="#ref-Xion:10">2010</a>)</span>). Therefore, one can determine the <span class="math inline">\(u_j\)</span> by minimizing the estimated out-of-sample prediction error, i.e. <span class="math inline">\(\mbox{min}_{\mathbf{u}} \Vert \mathbf{y}−\mathbf{X}\mathbf{U}\hat{\boldsymbol{\beta}}^{(LS)}\Vert+2\sigma^2 \sum_{j=1}^pu_j\)</span>, s.t. <span class="math inline">\(u_j \geq 0\)</span>, <span class="math inline">\(\forall j\)</span>, which yields the nonnegative garotte estimator with tuning parameter <span class="math inline">\(\lambda = \sigma^2\)</span>. In practice, <span class="math inline">\(\sigma^2\)</span> is replaced by a consistent estimator <span class="math inline">\(\hat{\sigma}^2\)</span>; for a review, see e.g. <span class="citation">Reid, Tibshirani, and Friedman (<a href="#ref-ReTiFr:16">2016</a>)</span>.</p>
</div>
<div id="non-convex-penalties" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Non convex penalties</h3>
<p>The adaptive lasso consists of a two-stage approach involving an initial estimator to reduce bias for large regression coefficients. An alternative single-stage approach is to use a penalty that <em>tapers off</em> as <span class="math inline">\(\beta_j\)</span> becomes larger in absolute value. Unlike the absolute value penalty employed by the lasso, a tapering penalty function cannot be convex. Such functions are often referred to as <em>folded</em> concave, in that it is symmetric around <span class="math inline">\(\beta_j=0\)</span>. The penalty function, say <span class="math inline">\(\sum_{j=1}^pP(\beta_j\vert \lambda,\gamma)\)</span>, then involves a tuning parameter <span class="math inline">\(\gamma\)</span> that controls the concavity of the penalty (i.e., how rapidly the penalty tapers off). Using a penalty that does not taper off (e.g. the lasso penalty) is called <em>soft thresholding</em>, while using subset (forward stepwise) selection (i.e. the <span class="math inline">\(l_0\)</span> norm) is called <em>hard thresholding</em>.</p>
A popular penalty is the <em>smoothly clipped absolute deviations</em> (SCAD) penalty of <span class="citation">Fan and Li (<a href="#ref-fanandli2001">2001</a>)</span>, given by
<span class="math display">\[\begin{eqnarray}
P(\beta_j\vert \lambda,\gamma)=\left\{\begin{array}{ll}
\lambda\vert\beta_j\vert &amp; \mbox{if } \;\vert\beta_j\vert\leq\lambda \\
\frac{2\gamma\lambda\vert\beta_j\vert-\beta_j^2-\lambda^2}{2(\gamma-1)} &amp;
\mbox{if } \;\lambda&lt;\vert\beta_j\vert\leq\gamma\lambda \\
\frac{\lambda^2(\gamma+1)}{2} &amp; \mbox{if } \;\vert\beta_j\vert&gt;\gamma\lambda
\end{array}
\right.
\end{eqnarray}\]</span>
for <span class="math inline">\(\gamma&gt;2\)</span>. The SCAD coincides with the lasso until <span class="math inline">\(\vert\beta_j\vert=\lambda\)</span>, then smoothly transits to a quadratic function until <span class="math inline">\(\vert\beta_j\vert = \gamma\lambda\)</span>, after which it remains constant. The derivative of the SCAD penalty is
<span class="math display">\[\begin{eqnarray}
\frac{\partial}{\partial\beta_j}P(\beta_j\vert \lambda,\gamma)=\left\{\begin{array}{ll}
\lambda &amp; \mbox{if } \;\vert\beta_j\vert\leq\lambda \\
\frac{\gamma\lambda-\vert\beta_j\vert}{\gamma-1} &amp;
\mbox{if } \;\lambda&lt;\vert\beta_j\vert\leq\gamma\lambda \\
0 &amp; \mbox{if } \;\vert\beta_j\vert&gt;\gamma\lambda
\end{array}
\right.
\end{eqnarray}\]</span>
<p>The SCAD penalty retains the penalization rate (and bias) of the lasso for small coefficients, but continuously relaxes the rate of penalization (and bias) as the absolute value of the coefficient increases. Hence the overall bias is reduced.</p>
Another population non convex penalty has been proposed by <span class="citation">Zhang (<a href="#ref-Zhan:10">2010</a>)</span> as the <em>Minimax Concave Penalty</em> (MCP), given by
<span class="math display">\[\begin{eqnarray}
P(\beta_j\vert \lambda,\gamma)=\left\{\begin{array}{ll}
\lambda\vert\beta_j\vert - \frac{\beta_j^2}{2\gamma}&amp; \mbox{if } \;\vert\beta_j\vert\leq \gamma\lambda \\
\frac{\lambda^2\gamma}{2} &amp; \mbox{if } \;\vert\beta_j\vert&gt;\gamma\lambda
\end{array}
\right.
\end{eqnarray}\]</span>
for <span class="math inline">\(\gamma&gt;1\)</span>. Its derivative is
<span class="math display">\[\begin{eqnarray}
\frac{\partial}{\partial\beta_j}P(\beta_j\vert \lambda,\gamma)=\left\{\begin{array}{ll}
\mbox{sign}\left(\beta_j\right)\left(\lambda - \frac{\vert\beta_j\vert}{\gamma}\right)&amp; \mbox{if } \;\vert\beta_j\vert\leq\gamma\lambda \\
0 &amp; \mbox{if } \;\vert\beta_j\vert&gt;\gamma\lambda
\end{array}
\right.
\end{eqnarray}\]</span>
<p>As with the SCAD, the MCP starts out by applying the same rate of penalization as the lasso, then smoothly relaxes the rate down to zero as the absolute value of the coefficient increases. It does however so immediately while with SCAD the rate remains flat for a while before decreasing.</p>
<blockquote>
<p>Reported from <span class="citation">Zhang (<a href="#ref-Zhan:10">2010</a>)</span>, Figure 1. Penalty functions (right panel) and their derivatives (left panel) as a function of <span class="math inline">\(\vert\beta_j\vert\)</span>, for the lasso (<span class="math inline">\(t\)</span>), SCAD and MCP with <span class="math inline">\(\gamma=2.5\)</span>. <img src="Figures/NonCovexPenalties.png" alt="Top" /></p>
</blockquote>
<p>As <span class="math inline">\(\gamma\rightarrow\infty\)</span>, both the MCP and SCAD penalties converge to the <span class="math inline">\(l_1\)</span> penalty. As <span class="math inline">\(\gamma\)</span> approaches its minimum value, the bias is minimized. The price to pay is the <em>instability</em> of the resulting MCP and SCAD estimators, in the sense that as the penalty becomes more concave (i.e. for smaller <span class="math inline">\(\gamma\)</span>), there is a greater chance for the optimization problem to admit multiple local minima.</p>
<p>Among others, <span class="citation">Zhang (<a href="#ref-Zhan:10">2010</a>)</span> and <span class="citation">Breheny and Huang (<a href="#ref-breheny2011">2011</a>)</span> propose algorithms to find the SCAD and MCP estimates, or more generally, penalized regression estimators with non convex penalties. Moreover, as with the elastic net, one can add a ridge penalty to nonconvex penalties such as MCP and SCAD.</p>
<blockquote>
Exercise (solutions presented in <a href="solutions.html#nonconvex">7.4.3</a>):<br />
After having read the documentation of the R package <em>ncvreg</em> and installed it, perform the following steps:<br />

</blockquote>
<blockquote>
<ol style="list-style-type: lower-alpha">
<li>Fix the generating vector <span class="math inline">\(\boldsymbol{\beta}=(4,2,-4,-2,0,0,\ldots,0)\)</span> and set the seed equal to 11 (i.e. set.seed(11)).<br />
</li>
</ol>
</blockquote>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Generate from a MVN (multivariate normal) a matrix <span class="math inline">\(\mathbf{X_{n*p}}\)</span> with <span class="math inline">\(n = 200\)</span> and <span class="math inline">\(p = 1000\)</span>. You can choose the location vector as you wish but set the scale matrix with an autoregressive form <span class="math inline">\(\boldsymbol{\Sigma}=[\sigma_{lm}]_{l,m=1,\ldots,p}\)</span> with <span class="math inline">\(\sigma_{lm} = \rho^{\mid l - m\mid}\)</span>. </li>
</ol>
</blockquote>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>For each <span class="math inline">\(\boldsymbol{\rho} = [0 \; 0.2 \; 0.5]\)</span> generate <span class="math inline">\(\mathbf{\hat{y}}\)</span> thanks to the relation <span class="math inline">\(\mathbf{y} = \mathbf{X_{n*p}} \; \boldsymbol{\beta} + \boldsymbol{\epsilon}\)</span> where <span class="math inline">\(\epsilon_{i}\)</span> is a standard normal. Suppose for simplicity that the errors are uncorrelated.<br />
</li>
</ol>
</blockquote>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li>Compare the solution paths (graphically as a function of <span class="math inline">\(\lambda\)</span>) for the lasso, SCAD and MCP by fixing several values for <span class="math inline">\(\gamma\)</span> (choose e.g. <span class="math inline">\(\gamma=(1.5, 2, 3, 3.7, 5)\)</span>) for each value of <span class="math inline">\(\rho\)</span> indicated at point c.</li>
</ol>
</blockquote>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-JaSt:61">
<p>James, W., and C. Stein. 1961. “Estimation with Quadratic Loss.” In <em>Proc. 4th Berkeley Symposium on Mathematical Statistics and Probability, Vol. I</em>, 361–79. University of California Press.</p>
</div>
<div id="ref-EfHa:16">
<p>Efron, B., and T. Hastie. 2016. <em>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</em>. Cambridge University Press.</p>
</div>
<div id="ref-HoKe:70">
<p>Hoerl, A. E., and R. Kennard. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” <em>Technometrics</em> 12: 55–67.</p>
</div>
<div id="ref-Tibs:96">
<p>Tibshirani, R. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Jrssb</em> 58: 267–88.</p>
</div>
<div id="ref-EfHaJoTi:04">
<p>Efron, B., T. Hastie, I. Johnstone, and R. Tibshirani. 2004. “Least Angle Regression (with Discussion).” <em>Annals of Statistics</em> 32: 407–99.</p>
</div>
<div id="ref-HaTiFr:09">
<p>Hastie, Trevor, Tibshirani Robert, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Second Edition. Springer Series in Statistics. Springer.</p>
</div>
<div id="ref-HaTaTiWa:07">
<p>Hastie, T., J. Taylor, R. Tibshirani, and G. Walther. 2007. “Forward Stagewise Regression and the Monotone Lasso.” <em>Electronic Journal of Statistics</em> 1: 1–29.</p>
</div>
<div id="ref-TibR:15">
<p>Tibshirani, R. J. 2015. “A General Framework for Fast Stagewise Algorithms.” <em>Journal of Machine Learning Research</em> 16: 2543–88.</p>
</div>
<div id="ref-ZouH:06">
<p>Zou, H. 2006. “The Adaptive Lasso and Its Oracle Properties.” <em>Journal of the American Statistical Association</em> 101: 1418–29.</p>
</div>
<div id="ref-Mein:07">
<p>Meinshausen, N. 2007. “Relaxed Lasso.” <em>Computational Statistics and Data Analysis</em> 52: 374–93.</p>
</div>
<div id="ref-zou2005regularization">
<p>Zou, H., and T. Hastie. 2005. “Regularization and Variable Selection via the Elastic Net.” <em>Journal of the Royal Statistical Society: Series B</em> 67 (2). Wiley Online Library: 301–20.</p>
</div>
<div id="ref-Brei:95">
<p>Breiman, L. 1995. “Better Subset Regression Using the Nonnegative Garrote.” <em>Technometrics</em> 37: 373–84.</p>
</div>
<div id="ref-YuYi:07">
<p>Yuan, M., and Yi Lin. 2007. “On the Non-Negative Garrotte Estimator.” <em>Journal of the Royal Statististical Sociaty, Series B</em> 69: 143–61.</p>
</div>
<div id="ref-Xion:10">
<p>Xiong, S. 2010. “Some Notes on the Nonnegative Garrote.” <em>Technometrics</em> 52 (3): 349–61.</p>
</div>
<div id="ref-ReTiFr:16">
<p>Reid, S., R. Tibshirani, and J. Friedman. 2016. “A Study of Error Variance Eestimation in Lasso Regression.” <em>Statistica Sinica</em> 26: 35–67.</p>
</div>
<div id="ref-fanandli2001">
<p>Fan, J., and R. Li. 2001. “Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties.” <em>Journal of the American Statistical Association</em> 96 (456). Taylor &amp; Francis: 1348–60.</p>
</div>
<div id="ref-Zhan:10">
<p>Zhang, C.-H. 2010. “Nearly Unbiased Variable Selection Under Minimax Concave Penalty.” <em>The Annals of Statistics</em> 38: 894–942.</p>
</div>
<div id="ref-breheny2011">
<p>Breheny, P., and J. Huang. 2011. “Coordinate Descent Algorithms for Nonconvex Penalized Regression, with Applications to Biological Feature Selection.” <em>The Annals of Applied Statistics</em> 5: 232–53.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ordering-the-variables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="properties-of-model-selection-criteria.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
