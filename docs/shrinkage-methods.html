<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Model Selection in High Dimensions</title>
  <meta name="description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="Model Selection in High Dimensions" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Model Selection in High Dimensions" />
  
  <meta name="twitter:description" content="This is the content of the course Model Selection. The output format for this project is bookdown::gitbook." />
  

<meta name="author" content="Maria-Pia Victoria-Feser (professor), Cesare Miglioli and Guillaume Blanc (teaching assistants)">


<meta name="date" content="2018-04-12">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="ordering-the-variables.html">
<link rel="next" href="solutions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#read-this-part-first"><i class="fa fa-check"></i><b>1.1</b> Read this part first</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#content-choice-and-structure"><i class="fa fa-check"></i><b>1.2</b> Content choice and structure</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#bibliography"><i class="fa fa-check"></i><b>1.2.1</b> Bibliography</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#useful-links"><i class="fa fa-check"></i><b>1.2.2</b> Useful links</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#using-r"><i class="fa fa-check"></i><b>1.3</b> Using R</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#useful-r-packages"><i class="fa fa-check"></i><b>1.3.1</b> Useful R packages</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#managing-data"><i class="fa fa-check"></i><b>1.3.2</b> Managing Data</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#loading-data-from-an-r-package"><i class="fa fa-check"></i><b>1.3.3</b> Loading data from an R package</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#loading-data-from-a-local-file"><i class="fa fa-check"></i><b>1.3.4</b> Loading data from a local file</a></li>
<li class="chapter" data-level="1.3.5" data-path="index.html"><a href="index.html#loading-data-from-an-online-file"><i class="fa fa-check"></i><b>1.3.5</b> Loading data from an online file</a></li>
<li class="chapter" data-level="1.3.6" data-path="index.html"><a href="index.html#loading-data-from-an-online-database-using-a-mysql-query-optional"><i class="fa fa-check"></i><b>1.3.6</b> Loading data from an online database using a mySQL query (Optional)</a></li>
<li class="chapter" data-level="1.3.7" data-path="index.html"><a href="index.html#data-wrangling"><i class="fa fa-check"></i><b>1.3.7</b> Data Wrangling</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#writing-reports"><i class="fa fa-check"></i><b>1.4</b> Writing reports</a><ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#r-markdown"><i class="fa fa-check"></i><b>1.4.1</b> R Markdown</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4.2</b> GitHub</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.5</b> Examples</a><ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#data-on-malnutrition-in-zambia"><i class="fa fa-check"></i><b>1.5.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#prognostic-factors-in-childhood-leukemia"><i class="fa fa-check"></i><b>1.5.2</b> Prognostic Factors in Childhood Leukemia</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#r-package-quantmod"><i class="fa fa-check"></i><b>1.5.3</b> R package quantmod</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#fundamental-statistical-concepts"><i class="fa fa-check"></i><b>1.6</b> Fundamental statistical concepts</a><ul>
<li class="chapter" data-level="1.6.1" data-path="index.html"><a href="index.html#sample-and-population"><i class="fa fa-check"></i><b>1.6.1</b> Sample and population</a></li>
<li class="chapter" data-level="1.6.2" data-path="index.html"><a href="index.html#models-and-risk"><i class="fa fa-check"></i><b>1.6.2</b> Models and risk</a></li>
<li class="chapter" data-level="1.6.3" data-path="index.html"><a href="index.html#estimators-and-associated-variability"><i class="fa fa-check"></i><b>1.6.3</b> Estimators and associated variability</a></li>
<li class="chapter" data-level="1.6.4" data-path="index.html"><a href="index.html#simulating-the-population-using-resampling-techniques"><i class="fa fa-check"></i><b>1.6.4</b> Simulating the population using resampling techniques</a></li>
<li class="chapter" data-level="1.6.5" data-path="index.html"><a href="index.html#model-selection"><i class="fa fa-check"></i><b>1.6.5</b> Model Selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html"><i class="fa fa-check"></i><b>2</b> Assessing the validity of a model</a><ul>
<li class="chapter" data-level="2.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-1"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#cross-validation"><i class="fa fa-check"></i><b>2.2</b> Cross-Validation</a></li>
<li class="chapter" data-level="2.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#covariance-penalties-criteria"><i class="fa fa-check"></i><b>2.3</b> Covariance Penalties Criteria</a><ul>
<li class="chapter" data-level="2.3.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#introduction-2"><i class="fa fa-check"></i><b>2.3.1</b> Introduction</a></li>
<li class="chapter" data-level="2.3.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mallows-c_p"><i class="fa fa-check"></i><b>2.3.2</b> Mallows <span class="math inline">\(C_p\)</span></a></li>
<li class="chapter" data-level="2.3.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#efrons-q-class"><i class="fa fa-check"></i><b>2.3.3</b> Efron’s <span class="math inline">\(q\)</span>-class</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#information-theory-and-bayesian-criteria"><i class="fa fa-check"></i><b>2.4</b> Information Theory and Bayesian Criteria</a><ul>
<li class="chapter" data-level="2.4.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#aic-akaike-information-criterion"><i class="fa fa-check"></i><b>2.4.1</b> AIC: Akaike Information Criterion</a></li>
<li class="chapter" data-level="2.4.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#bic-bayesian-information-criterion"><i class="fa fa-check"></i><b>2.4.2</b> BIC: Bayesian Information Criterion</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#mean-squared-error-based-criteria"><i class="fa fa-check"></i><b>2.5</b> Mean Squared Error Based Criteria</a><ul>
<li class="chapter" data-level="2.5.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#steins-unbiased-risk-estimator-sure"><i class="fa fa-check"></i><b>2.5.1</b> Stein’s unbiased risk estimator (SURE)</a></li>
<li class="chapter" data-level="2.5.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-focused-information-criterion-fic"><i class="fa fa-check"></i><b>2.5.2</b> The Focused Information Criterion (FIC)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-measures"><i class="fa fa-check"></i><b>2.6</b> Classification measures</a><ul>
<li class="chapter" data-level="2.6.1" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-logistic-model"><i class="fa fa-check"></i><b>2.6.1</b> The logistic model</a></li>
<li class="chapter" data-level="2.6.2" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#prediction-error-measures-for-binary-classification"><i class="fa fa-check"></i><b>2.6.2</b> Prediction error measures for Binary classification</a></li>
<li class="chapter" data-level="2.6.3" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#classification-error-estimation"><i class="fa fa-check"></i><b>2.6.3</b> Classification error estimation</a></li>
<li class="chapter" data-level="2.6.4" data-path="assessing-the-validity-of-a-model.html"><a href="assessing-the-validity-of-a-model.html#the-roc-curve"><i class="fa fa-check"></i><b>2.6.4</b> The ROC curve</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html"><i class="fa fa-check"></i><b>3</b> Ordering the variables</a><ul>
<li class="chapter" data-level="3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-3"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#stepwise-forward-regression"><i class="fa fa-check"></i><b>3.2</b> Stepwise forward regression</a><ul>
<li class="chapter" data-level="3.2.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#partial-correlations"><i class="fa fa-check"></i><b>3.2.1</b> Partial correlations</a></li>
<li class="chapter" data-level="3.2.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#selection-by-hypothesis-testing"><i class="fa fa-check"></i><b>3.2.2</b> Selection by hypothesis testing</a></li>
<li class="chapter" data-level="3.2.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#orthogonal-matching-pursuit"><i class="fa fa-check"></i><b>3.2.3</b> Orthogonal matching pursuit</a></li>
<li class="chapter" data-level="3.2.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#forward-stagewise-regression"><i class="fa fa-check"></i><b>3.2.4</b> Forward stagewise regression</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#streamwise-regression"><i class="fa fa-check"></i><b>3.3</b> Streamwise regression</a><ul>
<li class="chapter" data-level="3.3.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#introduction-4"><i class="fa fa-check"></i><b>3.3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.3.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#sure-independence-screening"><i class="fa fa-check"></i><b>3.3.2</b> Sure Independence Screening</a></li>
<li class="chapter" data-level="3.3.3" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#pc-simple-algorithm"><i class="fa fa-check"></i><b>3.3.3</b> PC-simple algorithm</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-and-regression-tree-cart"><i class="fa fa-check"></i><b>3.4</b> Classification And Regression Tree (CART)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#regression-tree"><i class="fa fa-check"></i><b>3.4.1</b> Regression tree</a></li>
<li class="chapter" data-level="3.4.2" data-path="ordering-the-variables.html"><a href="ordering-the-variables.html#classification-trees"><i class="fa fa-check"></i><b>3.4.2</b> Classification Trees</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html"><i class="fa fa-check"></i><b>4</b> Shrinkage Methods</a><ul>
<li class="chapter" data-level="4.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#introduction-5"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#ridge-regression"><i class="fa fa-check"></i><b>4.2</b> Ridge regression</a></li>
<li class="chapter" data-level="4.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#the-lasso-estimator"><i class="fa fa-check"></i><b>4.3</b> The lasso estimator</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="solutions.html"><a href="solutions.html"><i class="fa fa-check"></i><b>5</b> Solutions</a><ul>
<li class="chapter" data-level="5.1" data-path="solutions.html"><a href="solutions.html#chapter-1"><i class="fa fa-check"></i><b>5.1</b> Chapter 1</a><ul>
<li class="chapter" data-level="5.1.1" data-path="solutions.html"><a href="solutions.html#zam"><i class="fa fa-check"></i><b>5.1.1</b> Data on Malnutrition in Zambia</a></li>
<li class="chapter" data-level="5.1.2" data-path="solutions.html"><a href="solutions.html#leuk"><i class="fa fa-check"></i><b>5.1.2</b> Prognostic Factors in Childhood Leukemia</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="solutions.html"><a href="solutions.html#chapter-2"><i class="fa fa-check"></i><b>5.2</b> Chapter 2</a><ul>
<li class="chapter" data-level="5.2.1" data-path="solutions.html"><a href="solutions.html#cv"><i class="fa fa-check"></i><b>5.2.1</b> Cross-validation</a></li>
<li class="chapter" data-level="5.2.2" data-path="solutions.html"><a href="solutions.html#aic"><i class="fa fa-check"></i><b>5.2.2</b> Akaike Information Criterion</a></li>
<li class="chapter" data-level="5.2.3" data-path="solutions.html"><a href="solutions.html#roc"><i class="fa fa-check"></i><b>5.2.3</b> ROC curves</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="solutions.html"><a href="solutions.html#chapter-3"><i class="fa fa-check"></i><b>5.3</b> Chapter 3</a><ul>
<li class="chapter" data-level="5.3.1" data-path="solutions.html"><a href="solutions.html#HT"><i class="fa fa-check"></i><b>5.3.1</b> Selection by Hypothesis Testing</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Model Selection in High Dimensions</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="shrinkage-methods" class="section level1">
<h1><span class="header-section-number">4</span> Shrinkage Methods</h1>
<div id="introduction-5" class="section level2">
<h2><span class="header-section-number">4.1</span> Introduction</h2>
Shrinkage methods denote estimation methods under constraints that <em>shrink</em> estimators toward a given value. The origin goes back to the James–Stein estimator <span class="citation">(James and Stein <a href="#ref-JaSt:61">1961</a>)</span>. The rationale is that one can sacrifice <em>bias</em> for <em>variance</em>, i.e. optimize in some sense the Mean Squared Error (MSE), which, for an estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> is
<span class="math display">\[\begin{equation}
\mbox{MSE}\left(\hat{\boldsymbol{\beta}}\right)=\mathbb{E}\left[\left\Vert\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right\Vert^2\right]=\mathbb{E}\left[\left(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right)^T\left(\hat{\boldsymbol{\beta}}-\boldsymbol{\beta}\right)\right]
\end{equation}\]</span>
<p>(see <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span>, Section 7.1, for details on the James–Stein estimator)</p>
<p>For the least squares (LS) estimator of the linear model <span class="math inline">\(\mathbf{y}=\mathbf{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon}\)</span>, <span class="math inline">\(\boldsymbol{\varepsilon}\sim N\left(\mathbf{0},\sigma^2\mathbf{I}\right)\)</span>, and supposing without loss of generality that <span class="math inline">\(\mathbf{y}\)</span> is centered and the covariates (columns in <span class="math inline">\(\mathbf{X}\)</span>) are standardized, we have <span class="math inline">\(\mbox{MSE}\left(\hat{\boldsymbol{\beta}}_{LS}\right)=\sigma^2\mbox{tr}\left[\left(\mathbf{X}^T\mathbf{X}\right)^{-1}\right]\)</span>, which obviously grows with the dimension <span class="math inline">\(p\)</span>.</p>
In terms of out-of-sample prediction error (PE), we have the following result
<span class="math display">\[\begin{eqnarray}
\mbox{PE}\left(\mathbf{x}_0,\hat{\boldsymbol{\beta}}\right)
&amp;=&amp;\sigma^2+\mathbb{E}_{Y\vert\mathbf{X}=\mathbf{x}_0}\left[\left(Y-\mathbf{x}_0\hat{\boldsymbol{\beta}}\right)\right]^2
+\mathbb{E}_{Y\vert \mathbf{X}=\mathbf{x}_0}\left[\left(Y-\mathbf{x}_0\hat{\boldsymbol{\beta}}\right)^2\right] \\
&amp;=&amp; \sigma^2+\mbox{Bias}^2\left(\mathbf{x}_0,\hat{\boldsymbol{\beta}}\right)+
\mbox{var}\left(\mathbf{x}_0,\hat{\boldsymbol{\beta}}\right)
\end{eqnarray}\]</span>
<p>As model becomes more complex (more terms included), on one hand, local structure/curvature can be picked up, but on the other hand, coefficient estimates suffer from high variance as more terms are included in the model (see overfitting phenomenon).</p>
<blockquote>
<p>Reported from <a href="http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf" class="uri">http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf</a> <img src="Figures/BiasVarTradeOff.png" alt="Top" /></p>
</blockquote>
<p>Hence, introducing a little bias in the estimators might lead to a substantial decrease in variance, and hence to a substantial decrease in PE. The idea is then not to let the <span class="math inline">\(\hat{\beta}_j\)</span> take too large values in order to avoid too large variances. To control the variance, the slope coefficients are <em>regularized</em> in that a constraint is added to the optimization problem (e.g. LS) that controls how large the coefficients can be.</p>
</div>
<div id="ridge-regression" class="section level2">
<h2><span class="header-section-number">4.2</span> Ridge regression</h2>
The first proposed regularization method is the <em>ridge</em> constraint (or Tikhonov regularization <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization" class="uri">https://en.wikipedia.org/wiki/Tikhonov_regularization</a>), which amounts to set the constraint <span class="math inline">\(\sum_{i=1}^p\beta_j^2&lt;t_{\lambda}\)</span>. In terms of optimization, for a given value of <span class="math inline">\(\lambda\)</span>, the ridge estimator <span class="math inline">\(\hat{\boldsymbol{\beta}}(\lambda)\)</span> is given by
<span class="math display">\[\begin{equation}
\hat{\boldsymbol{\beta}}(\lambda)=\mbox{argmin}_{\boldsymbol{\beta}}\left\Vert\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\right\Vert^2-\lambda\Vert\boldsymbol{\beta}\Vert^2
\end{equation}\]</span>
The solution is
<span class="math display">\[\begin{eqnarray}
\hat{\boldsymbol{\beta}}(\lambda)&amp;=&amp;\left(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I}\right)^{-1}\mathbf{X}^T\mathbf{y}\\
&amp;=&amp; \left(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I}\right)^{-1}\left(\mathbf{X}^T\mathbf{X}\right)\hat{\boldsymbol{\beta}}_{LS}
\end{eqnarray}\]</span>
<p>Hence, the variance of <span class="math inline">\(\hat{\boldsymbol{\beta}}_{LS}\)</span> is <em>shrunk</em> by an amount of <span class="math inline">\(\left(\mathbf{X}^T\mathbf{X}+\lambda\mathbf{I}\right)^{-1}\left(\mathbf{X}^T\mathbf{X}\right)\)</span> which depends on <span class="math inline">\(\lambda\)</span>. With <span class="math inline">\(\lambda=0\)</span>, there is no shrinkage and <span class="math inline">\(\hat{\boldsymbol{\beta}}(0)=\hat{\boldsymbol{\beta}}_{LS}\)</span>, while when <span class="math inline">\(\lambda\rightarrow\infty\)</span>, <span class="math inline">\(\hat{\boldsymbol{\beta}}(\lambda)\rightarrow\mbox{0}\)</span>, the maximal shinkage.</p>
<blockquote>
<p>Reported from <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span>, Section 7.2: Ridge regression applied to the diabetes data <img src="Figures/RidgeEstimExample.png" alt="Top" /></p>
</blockquote>
<p>Ridge estimators have lower variance the the LS, but this does not guarantee that the corresponding predictions <span class="math inline">\(\hat{\boldsymbol{\mu}}\left(\lambda\right)=\mathbf{X}\hat{\boldsymbol{\beta}}(\lambda)\)</span> are more accurate than the ones obtained by the LS <span class="math inline">\(\hat{\boldsymbol{\mu}}(0)=\mathbf{X}\hat{\boldsymbol{\beta}}(0)\)</span>, since <span class="math inline">\(\hat{\boldsymbol{\beta}}(\lambda)\)</span> is biased towards <span class="math inline">\(\mathbf{0}\)</span>. However, in high dimensions, the prior belief is that most of the <span class="math inline">\(\beta_j\)</span> lie near zero, and biasing the LS estimator toward zero then becomes a necessity.</p>
<p>Another notable advantage of the ridge estimator is that the inclusion of <span class="math inline">\(\lambda\)</span> makes the optimization problem solvable even if <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is not invertible (e.g. <span class="math inline">\(\mathbf{X}\)</span> is not of full rank when <span class="math inline">\(p&gt;n\)</span>). This was the original motivation for ridge regression as proposed by <span class="citation">Hoerl and Kennard (<a href="#ref-HoKe:70">1970</a>)</span> who also recommended to select <span class="math inline">\(\lambda\)</span> <em>graphically</em>. Standard practice now is to use (10-fold) cross-validation.</p>
</div>
<div id="the-lasso-estimator" class="section level2">
<h2><span class="header-section-number">4.3</span> The lasso estimator</h2>
<p>Instead of regularizing the estimator using <span class="math inline">\(\sum_{i=1}^p\beta_j^2=\Vert\boldsymbol{\beta}\Vert^2&lt;t_{\lambda}\)</span>, one could use other <span class="math inline">\(l_q\)</span>-norms, i.e. <span class="math inline">\(\Vert\boldsymbol{\beta}\Vert_{q}=\sum_{i=1}^p\vert\beta_j\vert^{q}&lt;t_{\lambda}\)</span>. In particular, when <span class="math inline">\(q=0\)</span>, we have <span class="math inline">\(\Vert\boldsymbol{\beta}\Vert_{0}=\sum_{i=1}^p\vert\beta_j\vert^{0}&lt;t_{\lambda}\)</span> with, by definition, <span class="math inline">\(\sum_{i=1}^p\vert\beta_j\vert^{0}=\sum_{i=1}^pI\left(\beta_j\neq 0\right)\)</span>, the number of slope coefficients different from <span class="math inline">\(0\)</span>, and hence corresponds to subset selection.</p>
<span class="citation">Tibshirani (<a href="#ref-Tibs:96">1996</a>)</span> proposed to use <span class="math inline">\(q=1\)</span> leading to the very famous lasso estimator. The great advantage of the lasso, on top of the fact that the resulting optimization problem
<span class="math display">\[\begin{equation}
\hat{\boldsymbol{\beta}}(\lambda)=\mbox{argmin}_{\boldsymbol{\beta}}\left\Vert\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\right\Vert^2-\lambda\Vert\boldsymbol{\beta}\Vert_1
\end{equation}\]</span>
<p>is convex (since both the loss <span class="math inline">\(\left\Vert\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\right\Vert^2\)</span> and the constraint <span class="math inline">\(\Vert\boldsymbol{\beta}\Vert_1=\sum_{j=1}^p\vert\beta_j\vert\)</span> are convex in <span class="math inline">\(\boldsymbol{\beta}\)</span>), is that the shrinking forces a set of <span class="math inline">\(\beta_j\)</span> to exactly <span class="math inline">\(0\)</span>. Hence, the lasso estimator provides simultaneously a regularized estimator and model (subset) selection.</p>
<blockquote>
<p>Reported from <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span>, Section 16.2 <img src="Figures/LassoRidgeIllustration.png" alt="Top" /></p>
</blockquote>
<p>As for the ridge estimator, <span class="math inline">\(\lambda\)</span> needs to be chosen and, for model selection purposes (or out-of-sample prediction error optimization), a criteria that assesses the model validity can be used. The most comon choices are the <span class="math inline">\(C_p\)</span> and 10-fold CV. Practically, for <em>well</em> chosen values of <span class="math inline">\(\lambda\)</span>, <span class="math inline">\(\hat{\boldsymbol{\beta}}(\lambda)\)</span> and the corresponding model validity criterion are computed. <span class="math inline">\(\lambda\)</span> is chosen as to optimize (minimize) the later.</p>
<p>To compute the entire lasso path (i.e. for all values of <span class="math inline">\(\lambda\)</span> that <em>add</em> one <span class="math inline">\(\beta_j\neq 0\)</span> at each step), <span class="citation">Efron et al. (<a href="#ref-EfHaJoTi:04">2004</a>)</span> propose the Least Angle Regression algorithm (LARS), which is based on correlations between the covariates to enter and the residuals from the previous fit. LARS is introduced as a more <em>democratic</em> version of forward stepwise regression. Indeed, while forward stepwise regression builds a model sequentially, adding one variable at a time and updating the LS estimator to include all the active variables, the LARS only enters <em>as much</em> of a covariate as it <em>deserves</em>. At the first step it identifies the variable most correlated with the response, and moves the coefficient of this variable continuously toward (not fully) its LS estimate. The residuals are updated and this causes the correlations with other variables to evolve. As soon as another variable reaches the same correlation with the residual as the ones in the active set, this variable then joins the active set.</p>
<blockquote>
<p>Reported from <span class="citation">Hastie, Tibshirani, and Friedman (<a href="#ref-HaTiFr:09">2009</a>)</span>, Section 3.4. Note that <span class="math inline">\(\langle \mbox{x}_j,\mathbf{r}\rangle= \mbox{x}_j^T\mathbf{r}\)</span>. <img src="Figures/LARS.png" alt="Top" /></p>
</blockquote>
<p>In step 3 and 4, the values of the slopes in the active set, say <span class="math inline">\(\mathcal{A}\)</span>, are augmented as <span class="math inline">\(\boldsymbol{\beta}_{\mathcal{A}}(\epsilon)=\boldsymbol{\beta}_{\mathcal{A}}+\epsilon\cdot\delta_{\mathcal{A}}\)</span> for small values of <span class="math inline">\(\epsilon\)</span>, with <span class="math inline">\(\delta_{\mathcal{A}}=\left(\mathbf{X}^T_{\mathcal{A}}\mathbf{X}_{\mathcal{A}}\right)^{-1}\mathbf{X}^T_{\mathcal{A}}\mathbf{r}_{\mathcal{A}}\)</span>, <span class="math inline">\(\mathbf{r}_{\mathcal{A}}=\mathbf{y}-\mathbf{X}_{\mathcal{A}}\boldsymbol{\beta}_{\mathcal{A}}\)</span>.</p>
<p>The LARS bears very close similarities with forward stagewise regression (see <span class="citation">Hastie et al. (<a href="#ref-HaTaTiWa:07">2007</a>)</span>, <span class="citation">Tibshirani (<a href="#ref-TibR:15">2015</a>)</span> and <span class="citation">Efron and Hastie (<a href="#ref-EfHa:16">2016</a>)</span>, Section 16.4). Hence, if the covariates are correlated, the lasso path might not produce the suitable model selection path (risk of overfitting).</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-JaSt:61">
<p>James, W., and C. Stein. 1961. “Estimation with Quadratic Loss.” In <em>Proc. 4th Berkeley Symposium on Mathematical Statistics and Probability, Vol. I</em>, 361–79. University of California Press.</p>
</div>
<div id="ref-EfHa:16">
<p>Efron, B., and T. Hastie. 2016. <em>Computer Age Statistical Inference: Algorithms, Evidence, and Data Science</em>. Cambridge University Press.</p>
</div>
<div id="ref-HoKe:70">
<p>Hoerl, A. E., and R. Kennard. 1970. “Ridge Regression: Biased Estimation for Nonorthogonal Problems.” <em>Technometrics</em> 12: 55–67.</p>
</div>
<div id="ref-Tibs:96">
<p>Tibshirani, R. 1996. “Regression Shrinkage and Selection via the Lasso.” <em>Jrssb</em> 58: 267–88.</p>
</div>
<div id="ref-EfHaJoTi:04">
<p>Efron, B., T. Hastie, I. Johnstone, and R. Tibshirani. 2004. “Least Angle Regression (with Discussion).” <em>Annals of Statistics</em> 32: 407–99.</p>
</div>
<div id="ref-HaTiFr:09">
<p>Hastie, Trevor, Tibshirani Robert, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. Second Edition. Springer Series in Statistics. Springer.</p>
</div>
<div id="ref-HaTaTiWa:07">
<p>Hastie, T., J. Taylor, R. Tibshirani, and G. Walther. 2007. “Forward Stagewise Regression and the Monotone Lasso.” <em>Electronic Journal of Statistics</em> 1: 1–29.</p>
</div>
<div id="ref-TibR:15">
<p>Tibshirani, R. J. 2015. “A General Framework for Fast Stagewise Algorithms.” <em>Journal of Machine Learning Research</em> 16: 2543–88.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ordering-the-variables.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="solutions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
